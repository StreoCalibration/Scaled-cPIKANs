# Scaled-cPIKAN ì•Œê³ ë¦¬ì¦˜ ì„±ëŠ¥ ê°œì„  ë°©ë²•ë¡  ì—°êµ¬ ì¡°ì‚¬ ë³´ê³ ì„œ

**ì‘ì„±ì¼**: 2025ë…„ 10ì›” 25ì¼  
**ì¡°ì‚¬ ë°©ë²•**: AI Research Assistant (Semantic Scholar API), arXiv ê²€ìƒ‰  
**ì¡°ì‚¬ ë²”ìœ„**: 2023-2025ë…„ ìµœì‹  ì—°êµ¬ ë…¼ë¬¸ ë° ë°©ë²•ë¡ 

---

## ëª©ì°¨

1. [ê°œìš”](#1-ê°œìš”)
2. [ì •í™•ë„ ê°œì„  ê¸°ë²•](#2-ì •í™•ë„-ê°œì„ -ê¸°ë²•)
3. [ì†ë„ ê°œì„  ê¸°ë²•](#3-ì†ë„-ê°œì„ -ê¸°ë²•)
4. [í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼ë²•](#4-í•˜ì´ë¸Œë¦¬ë“œ-ì ‘ê·¼ë²•)
5. [Scaled-cPIKAN í”„ë¡œì íŠ¸ ì ìš© ë¡œë“œë§µ](#5-scaled-cpikan-í”„ë¡œì íŠ¸-ì ìš©-ë¡œë“œë§µ)
6. [ì°¸ê³ ë¬¸í—Œ](#6-ì°¸ê³ ë¬¸í—Œ)

---

## 1. ê°œìš”

### 1.1 ì¡°ì‚¬ ë°°ê²½

Scaled-cPIKANì€ Chebyshev ê¸°ë°˜ Kolmogorov-Arnold Networks (KAN)ë¥¼ Physics-Informed Neural Networks (PINN) í”„ë ˆì„ì›Œí¬ì™€ ê²°í•©í•œ ìµœì‹  PDE ì†”ë²„ì…ë‹ˆë‹¤. ë³¸ ë³´ê³ ì„œëŠ” ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µí•˜ê¸° ìœ„í•´ ìµœì‹  ì—°êµ¬ë¥¼ ì¡°ì‚¬í–ˆìŠµë‹ˆë‹¤:

- **ì •í™•ë„**: ìƒëŒ€ L2 ì˜¤ì°¨ë¥¼ ì–´ë–»ê²Œ ë” ë‚®ì¶œ ìˆ˜ ìˆëŠ”ê°€?
- **ì†ë„**: í›ˆë ¨ ì‹œê°„ê³¼ ìˆ˜ë ´ ì†ë„ë¥¼ ì–´ë–»ê²Œ ê°œì„ í•  ìˆ˜ ìˆëŠ”ê°€?
- **ì•ˆì •ì„±**: í›ˆë ¨ ì‹¤íŒ¨(NaN/Inf, ë°œì‚°)ë¥¼ ì–´ë–»ê²Œ ë°©ì§€í•  ìˆ˜ ìˆëŠ”ê°€?
- **í™•ì¥ì„±**: ë” í° ë„ë©”ì¸, ë” ë³µì¡í•œ PDEë¡œ í™•ì¥í•˜ëŠ” ë°©ë²•ì€?

### 1.2 í˜„ì¬ Scaled-cPIKANì˜ í•µì‹¬ êµ¬ì„± ìš”ì†Œ

1. **ChebyKAN ë ˆì´ì–´**: Chebyshev ë‹¤í•­ì‹ ê¸°ë°˜ í•™ìŠµ ê°€ëŠ¥ í™œì„±í™” í•¨ìˆ˜
2. **ì•„í•€ ë„ë©”ì¸ ìŠ¤ì¼€ì¼ë§**: ë¬¼ë¦¬ì  ë„ë©”ì¸ â†’ [-1, 1] ë³€í™˜
3. **ë¬¼ë¦¬ ì •ë³´ ì†ì‹¤**: PDE ì”ì°¨ + ê²½ê³„ì¡°ê±´ + ì´ˆê¸°ì¡°ê±´ + ë°ì´í„° ì†ì‹¤
4. **2ë‹¨ê³„ ìµœì í™”**: Adam (ì‚¬ì „í•™ìŠµ) â†’ L-BFGS (ë¯¸ì„¸ì¡°ì •)

### 1.3 ì¡°ì‚¬ ë°©ë²•ë¡ 

- **ë…¼ë¬¸ ê²€ìƒ‰**: Semantic Scholar, arXivë¥¼ í†µí•´ 2023-2025ë…„ ë°œí‘œëœ ë…¼ë¬¸ ê²€ìƒ‰
- **í‚¤ì›Œë“œ**: PINN optimization, adaptive sampling, loss weighting, KAN acceleration, Chebyshev neural networks, transfer learning, domain decomposition
- **ì„ ì • ê¸°ì¤€**: ì¸ìš©ìˆ˜, ìµœì‹ ì„±, êµ¬í˜„ ê°€ëŠ¥ì„±, Scaled-cPIKANê³¼ì˜ í˜¸í™˜ì„±

---

## 2. ì •í™•ë„ ê°œì„  ê¸°ë²•

### 2.1 ì ì‘í˜• ì”ì°¨ ìƒ˜í”Œë§ (Adaptive Residual Sampling)

#### 2.1.1 VW-PINNs: Volume Weighting Method

**ë…¼ë¬¸**: Song et al., "VW-PINNs: A volume weighting method for PDE residuals in physics-informed neural networks" (2024)  
**ì¸ìš©ìˆ˜**: 35íšŒ  
**DOI**: 10.48550/arXiv.2401.06196

**í•µì‹¬ ì•„ì´ë””ì–´**:
- ë¹„ê· ì¼ ì½œë¡œì¼€ì´ì…˜ í¬ì¸íŠ¸ ì‚¬ìš© ì‹œ, ê° í¬ì¸íŠ¸ê°€ ì°¨ì§€í•˜ëŠ” "ë¶€í”¼(volume)"ë¡œ ê°€ì¤‘ì¹˜ ë¶€ì—¬
- ë°€ì§‘ëœ ì˜ì—­ì—ì„œëŠ” ë‚®ì€ ê°€ì¤‘ì¹˜, í¬ì†Œí•œ ì˜ì—­ì—ì„œëŠ” ë†’ì€ ê°€ì¤‘ì¹˜
- ì»¤ë„ ë°€ë„ ì¶”ì •(KDE)ì„ ì‚¬ìš©í•˜ì—¬ ë©”ì‹œ ì—†ì´ ë¶€í”¼ ê·¼ì‚¬

**ìˆ˜í•™ì  ì •ì˜**:
```
ë¶€í”¼ ê°€ì¤‘ ì”ì°¨ = (1/V_i) * |PDE_residual(x_i)|^2
ì—¬ê¸°ì„œ V_iëŠ” í¬ì¸íŠ¸ x_iê°€ ì ìœ í•˜ëŠ” ë¶€í”¼
```

**ì„±ëŠ¥ í–¥ìƒ**:
- ê¸°ì¡´ ì ì‘í˜• ìƒ˜í”Œë§ ëŒ€ë¹„ 3ë°° íš¨ìœ¨ ê°œì„ 
- ì—­ë¬¸ì œì—ì„œ ìƒëŒ€ ì˜¤ì°¨ 10ë°° ì´ìƒ ê°ì†Œ
- ìœ ë™ ì‹œë®¬ë ˆì´ì…˜(ì›í˜• ì‹¤ë¦°ë”, NACA0012)ì—ì„œ ìˆ˜ë ´ ì‹¤íŒ¨ ë¬¸ì œ í•´ê²°

**Scaled-cPIKAN ì ìš© ë°©ì•ˆ**:
```python
# src/data.pyì— VolumeWeightedSampler ì¶”ê°€
class VolumeWeightedSampler:
    def __init__(self, points, bandwidth='scott'):
        self.points = points
        self.kde = KernelDensity(bandwidth=bandwidth)
        self.kde.fit(points)
    
    def get_volume_weights(self):
        log_density = self.kde.score_samples(self.points)
        density = np.exp(log_density)
        volume_weights = 1.0 / (density + 1e-8)
        return volume_weights / volume_weights.sum()

# src/loss.pyì— ì ìš©
def pde_loss_with_volume_weights(residuals, volume_weights):
    return (volume_weights * residuals**2).mean()
```

#### 2.1.2 TCAS-PINN: Temporal Causality-Based Adaptive Sampling

**ë…¼ë¬¸**: Guo et al., "TCAS-PINN: Physics-informed neural networks with a novel temporal causality-based adaptive sampling method" (2024)  
**ì¸ìš©ìˆ˜**: 8íšŒ  
**DOI**: 10.1088/1674-1056/ad21f3

**í•µì‹¬ ì•„ì´ë””ì–´**:
- ì‹œê°„ ì¢…ì† PDEì—ì„œ ì¸ê³¼ì„±(causality)ì„ ê³ ë ¤í•œ ìƒ˜í”Œë§
- ì´ˆê¸° ì‹œê°„ ì˜ì—­ì—ì„œ ë¨¼ì € í•™ìŠµ í›„, ì ì§„ì ìœ¼ë¡œ ë‚˜ì¤‘ ì‹œê°„ìœ¼ë¡œ í™•ì¥
- ì”ì°¨ê°€ í° ì‹œê³µê°„ ì˜ì—­ì— ìƒ˜í”Œ ì¶”ê°€

**ì ìš© ì‹œë‚˜ë¦¬ì˜¤**:
- Allen-Cahn ë°©ì •ì‹ (ì‹œê°„ ì¢…ì†)
- Burgers ë°©ì •ì‹
- Reaction-Diffusion ë°©ì •ì‹

**Scaled-cPIKAN ì ìš©**:
- `examples/solve_*` ìŠ¤í¬ë¦½íŠ¸ì—ì„œ ì‹œê°„ ì¢…ì† ë¬¸ì œ ì‹œ ì ìš©
- ì´ˆê¸° ì‹œê°„ [0, T/4]ì—ì„œ í›ˆë ¨ í›„ ì ì§„ì ìœ¼ë¡œ [0, T]ë¡œ í™•ì¥

### 2.2 ì†ì‹¤ í•¨ìˆ˜ ê· í˜• ì¡°ì • (Loss Balancing)

#### 2.2.1 ConFIG: Conflict-Free Gradient Method

**ë…¼ë¬¸**: Liu et al., "ConFIG: Towards Conflict-free Training of Physics Informed Neural Networks" (2024)  
**ì¸ìš©ìˆ˜**: 20íšŒ  
**DOI**: 10.48550/arXiv.2408.11104

**í•µì‹¬ ì•„ì´ë””ì–´**:
- ë‹¤ì¤‘ ì†ì‹¤ í•­(PDE, BC, IC, Data)ì˜ ê·¸ë˜ë””ì–¸íŠ¸ê°€ ì¶©ëŒí•  ë•Œ ì—…ë°ì´íŠ¸ ë°©í–¥ ì¡°ì •
- ìµœì¢… ì—…ë°ì´íŠ¸ ë°©í–¥ì´ ëª¨ë“  ì†ì‹¤ ê·¸ë˜ë””ì–¸íŠ¸ì™€ ì–‘ì˜ ë‚´ì ì„ ê°–ë„ë¡ ë³´ì¥
- Dual Cone ì˜ì—­ ë‚´ì—ì„œ ì—…ë°ì´íŠ¸ ë°©í–¥ ì„ íƒ

**ìˆ˜í•™ì  ì •ì˜**:
```
ìµœì í™” ë¬¸ì œ: 
min_{d} ||d||^2
subject to: <d, âˆ‡L_pde> â‰¥ 0, <d, âˆ‡L_bc> â‰¥ 0, ...
```

**ì„±ëŠ¥ í–¥ìƒ**:
- ê¸°ì¡´ PINN ëŒ€ë¹„ ìš°ìˆ˜í•œ ì•ˆì •ì„±
- ê³ ë‚œì´ë„ PDEì—ì„œ í›ˆë ¨ ì‹¤íŒ¨ìœ¨ ê°ì†Œ
- ë‹¤ì¤‘ ì‘ì—… ë²¤ì¹˜ë§ˆí¬ì—ì„œë„ íš¨ê³¼ì 

**Scaled-cPIKAN ì ìš©**:
```python
# src/train.pyì˜ Trainer í´ë˜ìŠ¤ì— ì¶”ê°€
class ConFIGOptimizer:
    def __init__(self, optimizer):
        self.optimizer = optimizer
    
    def step(self, loss_dict):
        gradients = {}
        for name, loss in loss_dict.items():
            grad = torch.autograd.grad(loss, model.parameters(), 
                                       retain_graph=True, create_graph=False)
            gradients[name] = grad
        
        # Dual cone projection
        adjusted_grad = self._project_to_dual_cone(gradients)
        
        # Apply adjusted gradient
        for param, adj_g in zip(model.parameters(), adjusted_grad):
            param.grad = adj_g
        
        self.optimizer.step()
```

#### 2.2.2 Dual Cone Gradient Descent (DCGD)

**ë…¼ë¬¸**: Hwang & Lim, "Dual Cone Gradient Descent for Training Physics-Informed Neural Networks" (2024)  
**ì¸ìš©ìˆ˜**: 8íšŒ  
**DOI**: 10.48550/arXiv.2409.18426

**í•µì‹¬ ì•„ì´ë””ì–´**:
- ê·¸ë˜ë””ì–¸íŠ¸ í¬ê¸° ë¶ˆê· í˜•ê³¼ ìŒì˜ ë‚´ì  ë¬¸ì œ ë™ì‹œ í•´ê²°
- NTK (Neural Tangent Kernel) ë¶„ì„ ê¸°ë°˜ ì´ë¡ ì  ìˆ˜ë ´ ë³´ì¥
- í•™ìŠµë¥  ì–´ë‹ë§ ë° NTKì™€ ê²°í•© ê°€ëŠ¥

**íŠ¹ì§•**:
- ë¹„ë³¼ë¡ ì„¤ì •ì—ì„œ ìˆ˜ë ´ ì†ì„± ì´ë¡ ì  ì¦ëª…
- ë³µì¡í•œ PDEì—ì„œ ì˜ˆì¸¡ ì •í™•ë„ í–¥ìƒ
- ê¸°ì¡´ PINN ì‹¤íŒ¨ ëª¨ë“œ ì•ˆì •í™”

### 2.3 Fourier Featuresë¥¼ í†µí•œ ìŠ¤í™íŠ¸ëŸ´ í¸í–¥ ê·¹ë³µ

#### 2.3.1 ê³ ì£¼íŒŒ ë¬¸ì œë¥¼ ìœ„í•œ Fourier Features

**ë…¼ë¬¸**: Chai et al., "Overcoming the Spectral Bias Problem of Physics-Informed Neural Networks in Solving the Frequency-Domain Acoustic Wave Equation" (2024)  
**ì¸ìš©ìˆ˜**: 11íšŒ  
**DOI**: 10.1109/TGRS.2024.3440471

**í•µì‹¬ ì•„ì´ë””ì–´**:
- ì…ë ¥ ì¢Œí‘œì— Random Fourier Features (RFF) ì ìš©
- ê³ ì£¼íŒŒ ì„±ë¶„ì„ ì €ì°¨ì› íŠ¹ì§• ê³µê°„ìœ¼ë¡œ ë§¤í•‘

**ìˆ˜í•™ì  ì •ì˜**:
```
Ï†(x) = [cos(2Ï€B^T x), sin(2Ï€B^T x)]
ì—¬ê¸°ì„œ B ~ N(0, Ïƒ^2)ëŠ” ë¬´ì‘ìœ„ ì£¼íŒŒìˆ˜ í–‰ë ¬
```

**Scaled-cPIKANê³¼ì˜ ê´€ê³„**:
- **Chebyshev vs Fourier**: ë‘ ë°©ë²• ëª¨ë‘ ìŠ¤í™íŠ¸ëŸ´ í¸í–¥ ê·¹ë³µ
- ChebyshevëŠ” êµ¬ê°„ [-1,1]ì—ì„œ ì§êµ ë‹¤í•­ì‹, FourierëŠ” ì£¼ê¸° í•¨ìˆ˜
- **ê²°í•© ê°€ëŠ¥ì„±**: Chebyshev ë ˆì´ì–´ ì „ì— Fourier ì¸ì½”ë”© ì¶”ê°€

**ì ìš© ë°©ì•ˆ**:
```python
# src/models.pyì— ì¶”ê°€
class FourierFeatureLayer(nn.Module):
    def __init__(self, input_dim, num_features, sigma=10.0):
        super().__init__()
        self.B = nn.Parameter(torch.randn(input_dim, num_features) * sigma, 
                              requires_grad=False)
    
    def forward(self, x):
        x_proj = 2 * np.pi * x @ self.B
        return torch.cat([torch.cos(x_proj), torch.sin(x_proj)], dim=-1)

# Scaled_cPIKAN ìˆ˜ì •
class Scaled_cPIKAN_FF(nn.Module):
    def __init__(self, layers_dims, cheby_order, domain_min, domain_max,
                 use_fourier=False, num_fourier_features=256):
        super().__init__()
        if use_fourier:
            self.fourier_layer = FourierFeatureLayer(
                layers_dims[0], num_fourier_features
            )
            layers_dims[0] = 2 * num_fourier_features
        # ... ë‚˜ë¨¸ì§€ ChebyKAN ë ˆì´ì–´ êµ¬ì„±
```

#### 2.3.2 Wavelet-Based KAN (Wav-KAN)

**ë…¼ë¬¸**: Meshir et al., "On the study of frequency control and spectral bias in Wavelet-Based Kolmogorov Arnold networks" (2025)  
**ì¸ìš©ìˆ˜**: 4íšŒ  
**DOI**: 10.48550/arXiv.2502.00280

**í•µì‹¬ ì•„ì´ë””ì–´**:
- Chebyshev ëŒ€ì‹  Waveletì„ ê¸°ì € í•¨ìˆ˜ë¡œ ì‚¬ìš©
- Mother waveletì˜ ì£¼íŒŒìˆ˜ë¥¼ ì œì–´í•˜ì—¬ ì €ì£¼íŒŒ/ê³ ì£¼íŒŒ ê· í˜• ì¡°ì •
- NTK ê³ ìœ ê°’ ë¶„ì„ìœ¼ë¡œ ê³ ì£¼íŒŒ ìˆ˜ë ´ ëŠ¥ë ¥ í–¥ìƒ

**Scaled-cPIKAN ì ìš© ê°€ëŠ¥ì„±**:
- **í˜„ì¬**: Chebyshev ë‹¤í•­ì‹ (`ChebyKANLayer`)
- **í™•ì¥**: `WaveletKANLayer` ì¶”ê°€ êµ¬í˜„
- **ì„ íƒ ê¸°ì¤€**: 
  - Chebyshev: ì •ìƒ ìƒíƒœ ë¬¸ì œ, ë§¤ë„ëŸ¬ìš´ í•´
  - Wavelet: êµ­ì†Œ ê¸‰ê²© ë³€í™”, ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ë¬¸ì œ

### 2.4 ì „ì´ í•™ìŠµ (Transfer Learning)

**ë…¼ë¬¸**: Mustajab et al., "Physics-Informed Neural Networks for High-Frequency and Multi-Scale Problems using Transfer Learning" (2024)  
**ì¸ìš©ìˆ˜**: 23íšŒ  
**DOI**: 10.48550/arXiv.2401.02810

**í•µì‹¬ ì•„ì´ë””ì–´**:
- ì €ì£¼íŒŒ ë¬¸ì œì—ì„œ ë¨¼ì € í›ˆë ¨ (ì˜ˆ: k=1 Helmholtz)
- í•™ìŠµëœ ê°€ì¤‘ì¹˜ë¥¼ ì´ˆê¸°ê°’ìœ¼ë¡œ ê³ ì£¼íŒŒ ë¬¸ì œ ë¯¸ì„¸ì¡°ì • (ì˜ˆ: k=10)
- ë„¤íŠ¸ì›Œí¬ íŒŒë¼ë¯¸í„° ì¦ê°€ ì—†ì´ ë³µì¡ë„ í™•ì¥

**í›ˆë ¨ ì „ëµ**:
1. **Phase 1**: ë‹¨ìˆœí•œ ë¬¸ì œ (ë‚®ì€ k, ì‘ì€ ë„ë©”ì¸)
2. **Phase 2**: ì¤‘ê°„ ë‚œì´ë„ (ì¤‘ê°„ k)
3. **Phase 3**: ëª©í‘œ ë¬¸ì œ (ë†’ì€ k, í° ë„ë©”ì¸)

**ì„±ëŠ¥ í–¥ìƒ**:
- í›ˆë ¨ ì‹œê°„ ë‹¨ì¶•
- í•„ìš” ë°ì´í„° í¬ì¸íŠ¸ ê°ì†Œ
- L2 ìƒëŒ€ ì˜¤ì°¨ ê°œì„ 

**Scaled-cPIKAN ì ìš©**:
```python
# examples/solve_helmholtz_transfer_learning.py
def train_with_transfer_learning():
    # Phase 1: k=Ï€
    model_phase1 = Scaled_cPIKAN([1, 32, 32, 32, 1], cheby_order=3, ...)
    train(model_phase1, k=np.pi, epochs=10000)
    
    # Phase 2: k=2Ï€ (ì „ì´ í•™ìŠµ)
    model_phase2 = Scaled_cPIKAN([1, 32, 32, 32, 1], cheby_order=3, ...)
    model_phase2.load_state_dict(model_phase1.state_dict())
    train(model_phase2, k=2*np.pi, epochs=5000)
    
    # Phase 3: k=4Ï€ (ëª©í‘œ)
    model_phase3 = Scaled_cPIKAN([1, 32, 32, 32, 1], cheby_order=3, ...)
    model_phase3.load_state_dict(model_phase2.state_dict())
    train(model_phase3, k=4*np.pi, epochs=5000)
```

---

### 2.5 ê²½ê³„/ì œì•½ ê°•ì œ ê°•í™”: Augmented Lagrangian & Hard Constraints

**ë™ê¸°**: BC/IC ìœ„ë°˜ìœ¼ë¡œ í•™ìŠµì´ í”ë“¤ë¦¬ë©´ PDE ì”ì°¨ê°€ ì‘ì•„ì ¸ë„ ì‹¤ì œ í•´ê°€ í‹€ì–´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¨ìˆœ ê°€ì¤‘ í•©ì´ ì•„ë‹Œ ë¼ê·¸ë‘ì§€ ìŠ¹ìˆ˜ë¥¼ ì‚¬ìš©í•œ ë²Œì ì€ ì•ˆì •ì ì¸ ìˆ˜ë ´ì„ ë•ìŠµë‹ˆë‹¤.

**í•µì‹¬ ë°©ë²•**:
- Augmented Lagrangian(ì¦ê°• ë¼ê·¸ë‘ì§€ì•ˆ): BC/IC ì œì•½ g(u)=0ì— ëŒ€í•´ ë¼ê·¸ë‘ì§€ ìŠ¹ìˆ˜ Î»ë¥¼ í•™ìŠµì ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•˜ë©°, ë²Œì  í•­ì„ í•¨ê»˜ ì‚¬ìš©í•´ ì œì•½ì„ ê°•í•˜ê²Œ ë§Œì¡±.
- Hard constraints: ê±°ë¦¬ í•¨ìˆ˜(distance function) ë“±ìœ¼ë¡œ BCë¥¼ êµ¬ì¡°ì ìœ¼ë¡œ ë§Œì¡±í•˜ë„ë¡ ì¶œë ¥ íŒŒë¼ë©”í„°ë¼ì´ì œì´ì…˜.

**ì°¸ê³  ë…¼ë¬¸**:
- Zhang et al., 2025, Scientific Reports: â€œPhysics-informed neural networks with hybrid Kolmogorovâ€“Arnold network and augmented Lagrangian function for PDEsâ€ â€” KANê³¼ AL ê²°í•©ìœ¼ë¡œ ì œì•½ ë§Œì¡±ë„ì™€ ì •í™•ë„ í–¥ìƒ ë³´ê³ .
- Son et al., 2022, arXiv: â€œAL-PINNs: Augmented Lagrangian relaxation method for PINNsâ€ â€” ì¼ë°˜ì  AL í”„ë ˆì„ì›Œí¬ ì œì‹œ.
- Zhou et al., 2024: ìœ¤í™œ í•´ì„ì—ì„œ AL-PINN+ì „ì´í•™ìŠµìœ¼ë¡œ ì•ˆì •ì„± í–¥ìƒ ì‚¬ë¡€.

**êµ¬í˜„ ìŠ¤ì¼€ì¹˜ (loss.py)**:
```python
class AugmentedLagrangianLoss:
    def __init__(self, mu_init=1.0, mu_max=1e6, rho=2.0):
        self.mu = mu_init  # ë²Œì  ê³„ìˆ˜
        self.mu_max = mu_max
        self.rho = rho     # ì¦ê°€ìœ¨
        self.lambdas = {}  # ì œì•½ë³„ ë¼ê·¸ë‘ì§€ ìŠ¹ìˆ˜

    def step(self, constraints):
        # constraints: dict[name -> g(u)_batch]
        loss_al = 0.0
        for name, g in constraints.items():
            lam = self.lambdas.setdefault(name, 0.0)
            loss_al = loss_al + lam * g.mean() + 0.5 * self.mu * (g**2).mean()
            # ìŠ¹ìˆ˜ ì—…ë°ì´íŠ¸(ì™¸ë£¨í”„/ì—í­ ë‹¨ìœ„)
            self.lambdas[name] = lam + self.mu * g.detach().mean().item()
        return loss_al

    def update_penalty(self):
        self.mu = min(self.mu * self.rho, self.mu_max)
```

**í†µí•© íŒ**:
- ì´ˆê¸°ì— ì‘ì€ muë¡œ ì‹œì‘í•´ ìˆ˜ ì—í­ë§ˆë‹¤ update_penalty()ë¥¼ í˜¸ì¶œ.
- L-BFGS ë‹¨ê³„ ì „ ALë¡œ ì œì•½ì„ ìˆ˜ë ´ì‹œí‚¨ ë’¤ ë¯¸ì„¸ì¡°ì •.
- Hard constraintsëŠ” ë„ë©”ì¸ë³„(ë””ë¦¬í´ë ˆ/ë…¸ì´ë§Œ)ë¡œ ì„ íƒì ìœ¼ë¡œ ì ìš©.

---

### 2.6 ë³€ë¶„/ì—ë„ˆì§€ ê¸°ë°˜ í•™ìŠµ: Deep Ritz & Sobolev í›ˆë ¨

**ì•„ì´ë””ì–´**: íŠ¹ì • PDE(íŠ¹íˆ íƒ€ì›í˜•)ëŠ” ì—ë„ˆì§€ í•¨ìˆ˜ E(u)ì˜ ìµœì†Œí™”ë¡œ ë™ì¹˜. ì”ì°¨ ìµœì†Œí™” ëŒ€ì‹  ì—ë„ˆì§€ ìµœì†Œí™”ë¥¼ í•™ìŠµ ëª©í‘œë¡œ ì‚¼ìœ¼ë©´ ìˆ˜ë ´ì„±ì´ ì¢‹ì•„ì§€ëŠ” ì‚¬ë¡€ ë‹¤ìˆ˜.

ì˜ˆ: í¬ì•„ì†¡ ë¬¸ì œ
\[ E(u) = \int_\Omega \Big(\tfrac{1}{2} \|\nabla u\|^2 - f u\Big)\,dx, \quad u|_{\partial\Omega}=g \]

**ì¥ì **:
- ì”ì°¨ ë…¸ì´ì¦ˆì— ëœ ë¯¼ê°, ê²½ê³„ ì¡°ê±´ì„ ë³€ë¶„ì  ì œì•½ìœ¼ë¡œ í†µí•© ìš©ì´
- Sobolev í›ˆë ¨(ë„í•¨ìˆ˜ ì •í•©)ì„ ê²°í•©í•˜ë©´ ê³ ì£¼íŒŒ ì„±ë¶„ í‘œí˜„ë ¥ ê°œì„ 

**ì°¸ê³  ë¬¸í—Œ**:
- Xu & Huang, 2024, IJCAI: â€œA Priori Estimation of the Approximation, Optimization and Generalization Error of Random Neural Networks for PDEsâ€ â€” ë³€ë¶„/ì†Œë³¼ë ˆí”„ ê´€ì ì˜ ì´ë¡  ë¶„ì„.

**ì ìš© ìŠ¤ì¼€ì¹˜**:
- `loss.py`ì— Deep-Ritz ì—ë„ˆì§€ í•¨ìˆ˜ ì¶”ê°€, ê²½ê³„ëŠ” AL ë˜ëŠ” í•˜ë“œ ì œì•½ìœ¼ë¡œ ì²˜ë¦¬.
- Helmholtz ë“± ë¹„ìëª… ì—ë„ˆì§€ëŠ” ë³€ë¶„í˜• ìœ ë„ í›„ ì ìš©(ê°€ëŠ¥ ì‹œ).

## 3. ì†ë„ ê°œì„  ê¸°ë²•

### 3.1 ë„ë©”ì¸ ë¶„í•´ ë° ë³‘ë ¬ í›ˆë ¨ (Domain Decomposition)

#### 3.1.1 Schwarz Domain Decomposition for PINNs

**ë…¼ë¬¸**: KopanicÃ¡kovÃ¡ et al., "Enhancing training of physics-informed neural networks using domain-decomposition based preconditioning strategies" (2023)  
**ì¸ìš©ìˆ˜**: 24íšŒ  
**DOI**: 10.1137/23M1583375

**í•µì‹¬ ì•„ì´ë””ì–´**:
- ê³„ì‚° ë„ë©”ì¸ì„ ì—¬ëŸ¬ í•˜ìœ„ ë„ë©”ì¸ìœ¼ë¡œ ë¶„í• 
- ê° í•˜ìœ„ ë„ë©”ì¸ì— ëŒ€í•´ ê°œë³„ ì„œë¸Œ ë„¤íŠ¸ì›Œí¬ í›ˆë ¨
- ê²½ê³„ì—ì„œ ì¸í„°í˜ì´ìŠ¤ ì¡°ê±´ìœ¼ë¡œ ê²°í•©
- L-BFGSì— ëŒ€í•œ ê°€ë²•/ìŠ¹ë²• ì „ì²˜ë¦¬ ì „ëµ

**ë³‘ë ¬í™” ì´ì **:
- **ê°€ë²• ì „ì²˜ë¦¬**: ë³¸ì§ˆì ìœ¼ë¡œ ë³‘ë ¬ ê°€ëŠ¥ (ê° í•˜ìœ„ ë„ë©”ì¸ ë…ë¦½ í›ˆë ¨)
- **ëª¨ë¸ ë³‘ë ¬ì„±**: ì—¬ëŸ¬ GPUì— ë¶„ì‚° ê°€ëŠ¥
- **ìˆ˜ë ´ ì†ë„**: L-BFGS ìˆ˜ë ´ í¬ê²Œ ê°œì„ 

**Scaled-cPIKAN ì ìš©**:
```python
# src/models.pyì— ì¶”ê°€
class DomainDecomposedcPIKAN(nn.Module):
    def __init__(self, domain_splits, layers_dims, cheby_order):
        super().__init__()
        self.domain_splits = domain_splits
        self.submodels = nn.ModuleList([
            Scaled_cPIKAN(layers_dims, cheby_order, 
                         domain_min=split['min'], 
                         domain_max=split['max'])
            for split in domain_splits
        ])
    
    def forward(self, x):
        # xì˜ ìœ„ì¹˜ì— ë”°ë¼ í•´ë‹¹ ì„œë¸Œëª¨ë¸ ì„ íƒ
        outputs = []
        for i, split in enumerate(self.domain_splits):
            mask = (x >= split['min']).all(dim=1) & (x <= split['max']).all(dim=1)
            if mask.any():
                outputs.append(self.submodels[i](x[mask]))
        return torch.cat(outputs)
```

**í›ˆë ¨ ì „ëµ**:
1. ê° ì„œë¸Œëª¨ë¸ì„ ë³‘ë ¬ë¡œ í›ˆë ¨
2. ì¸í„°í˜ì´ìŠ¤ ê²½ê³„ì—ì„œ ì—°ì†ì„± ì†ì‹¤ ì¶”ê°€
3. ì „ì—­ ì†ì‹¤ë¡œ ë¯¸ì„¸ì¡°ì •

#### 3.1.2 Distributed Training on Multiple GPUs

**ë…¼ë¬¸**: Feeney et al., "Breaking Boundaries: Distributed Domain Decomposition with Scalable Physics-Informed Neural PDE Solvers" (2023)  
**ì¸ìš©ìˆ˜**: 4íšŒ  
**DOI**: 10.1145/3581784.3613217

**í•µì‹¬ ì•„ì´ë””ì–´**:
- MPI/NCCLì„ ì‚¬ìš©í•œ ë¶„ì‚° í›ˆë ¨
- ê° GPUê°€ í•˜ìœ„ ë„ë©”ì¸ ë‹´ë‹¹
- ê²½ê³„ ì •ë³´ êµí™˜ìœ¼ë¡œ ê¸€ë¡œë²Œ ì¼ê´€ì„± ìœ ì§€

**êµ¬í˜„ ë°©ë²•** (PyTorch DDP):
```python
# examples/train_distributed.py
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP

def setup_distributed(rank, world_size):
    dist.init_process_group("nccl", rank=rank, world_size=world_size)

def train_distributed(rank, world_size):
    setup_distributed(rank, world_size)
    
    model = Scaled_cPIKAN(...).to(rank)
    model = DDP(model, device_ids=[rank])
    
    # ê° rankê°€ ë‹¤ë¥¸ í•˜ìœ„ ë„ë©”ì¸ ìƒ˜í”Œë§
    local_sampler = get_domain_sampler(rank, world_size)
    
    # í›ˆë ¨ ë£¨í”„
    for epoch in range(epochs):
        loss = compute_loss(model, local_sampler)
        loss.backward()
        optimizer.step()
```

### 3.2 Mixed Precision Training (ì´ë¯¸ v2 ê°€ì´ë“œì— í¬í•¨)

**í˜„ì¬ ìƒíƒœ**: `doc/v2/ì„±ëŠ¥_í–¥ìƒ_ì¦‰ì‹œ_ì ìš©_ê°€ì´ë“œ.md`ì— í¬í•¨ë¨  
**ì¶”ê°€ ìµœì í™”**:
- **Gradient Scaling**: `torch.cuda.amp.GradScaler` ì‚¬ìš©
- **AMP ìµœì í™”**: `autocast` ì»¨í…ìŠ¤íŠ¸ ë²”ìœ„ ìµœì†Œí™”
- **ë©”ëª¨ë¦¬ ì ˆê°**: 30-50% ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ
- **ì†ë„ í–¥ìƒ**: 1.5-2ë°° ì†ë„ ê°œì„ 

### 3.3 íš¨ìœ¨ì ì¸ ìë™ ë¯¸ë¶„ ê³„ì‚°

#### 3.3.1 2ì°¨ ë„í•¨ìˆ˜ ë²¡í„°í™”

**í˜„ì¬ ë¬¸ì œ**:
```python
# ë¹„íš¨ìœ¨ì : ê° ì¢Œí‘œ ì„±ë¶„ë§ˆë‹¤ ë³„ë„ autograd í˜¸ì¶œ
u_xx = torch.autograd.grad(u_x.sum(), x, create_graph=True)[0][:, 0]
u_yy = torch.autograd.grad(u_y.sum(), x, create_graph=True)[0][:, 1]
```

**ê°œì„  ë°©ì•ˆ**:
```python
# íš¨ìœ¨ì : Jacobianì„ í•œ ë²ˆì— ê³„ì‚° í›„ ëŒ€ê° ì¶”ì¶œ
def laplacian_efficient(u, x):
    # 1ì°¨ ë„í•¨ìˆ˜ (Jacobian)
    u_x = torch.autograd.grad(u.sum(), x, create_graph=True)[0]
    
    # 2ì°¨ ë„í•¨ìˆ˜ (Hessian ëŒ€ê°)
    # PyTorch 2.x: torch.func.jacfwd ì‚¬ìš© ê°€ëŠ¥
    laplacian = 0
    for i in range(x.shape[1]):
        u_xi_xi = torch.autograd.grad(
            u_x[:, i].sum(), x, create_graph=True
        )[0][:, i]
        laplacian += u_xi_xi
    
    return laplacian
```

**ì„±ëŠ¥ í–¥ìƒ**: 20-40% ê³„ì‚° ì‹œê°„ ì ˆê° (PDE ë³µì¡ë„ì— ë”°ë¼)

#### 3.3.2 ì²´í¬í¬ì¸íŒ…ì„ í†µí•œ ë©”ëª¨ë¦¬ ì ˆì•½

**ì•„ì´ë””ì–´**: ì „ë°© íŒ¨ìŠ¤ ì¤‘ê°„ ê²°ê³¼ë¥¼ ì €ì¥í•˜ì§€ ì•Šê³  ì¬ê³„ì‚°
```python
from torch.utils.checkpoint import checkpoint

class Scaled_cPIKAN_Checkpoint(nn.Module):
    def forward(self, x):
        x_scaled = self._affine_scale(x)
        
        # ì²´í¬í¬ì¸íŒ…ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½
        for layer in self.network:
            x_scaled = checkpoint(layer, x_scaled)
        
        return x_scaled
```

**íŠ¸ë ˆì´ë“œì˜¤í”„**:
- ë©”ëª¨ë¦¬: 50-70% ê°ì†Œ
- ì†ë„: 10-20% ëŠë ¤ì§
- **ì ìš© ì‹œë‚˜ë¦¬ì˜¤**: í° ë„¤íŠ¸ì›Œí¬, ë©”ëª¨ë¦¬ ì œì•½ í™˜ê²½

---

### 3.4 ë”¥ ë„ë©”ì¸ ë¶„í•´(Deep Domain Decomposition)ì™€ ë©€í‹°í”¼ë¸ë¦¬í‹°

**í•µì‹¬**: ì‹œê°„ ì˜ì¡´/ëŒ€ê·œëª¨ ë¬¸ì œì—ì„œ ë„ë©”ì¸ì„ ë¶„í•´í•˜ê³ , ê° ì„œë¸Œë„ë©”ì¸ì— ì„œë¡œ ë‹¤ë¥¸ í”¼ë¸ë¦¬í‹°/í•´ìƒë„ ë° ê°œë³„ ë„¤íŠ¸ì›Œí¬ë¥¼ ë°°ì¹˜í•´ ë³‘ë ¬ì„±Â·ìˆ˜ë ´ì„±ì„ ë™ì‹œì— í™•ë³´.

**ì°¸ê³  ë…¼ë¬¸**:
- Heinlein et al., 2024, arXiv: â€œMultifidelity domain decomposition-based PINNs for time-dependent problemsâ€ â€” ì‹œê°„ ì˜ì¡´ ë¬¸ì œì—ì„œ ë©€í‹°í”¼ë¸ë¦¬í‹° DDMìœ¼ë¡œ íš¨ìœ¨ ê°œì„ .
- Jagtap & Karniadakis, 2020, XPINNs â€” ê³µê°„Â·ì‹œê°„ ë¶„í•´ì˜ ê¸°ë°˜ í”„ë ˆì„ì›Œí¬(ê³ ì „ì ì´ì§€ë§Œ ì—¬ì „íˆ ì‹¤ìš©ì  ê¸°ì¤€ì„ ).

**ì ìš© ê°€ì´ë“œ**:
- 1ì°¨ ë‹¨ê³„: XPINNs ìŠ¤íƒ€ì¼ë¡œ ê²½ê³„ ì—°ì†ì„± ì†ì‹¤ë§Œ ì¶”ê°€í•˜ì—¬ ë¶„í•´ íš¨ê³¼ í™•ì¸
- 2ì°¨ ë‹¨ê³„: ê° ì„œë¸Œë„ë©”ì¸ì— ìƒ˜í”Œë§/ê°€ì¤‘/í•´ìƒë„ ì°¨ë“±í™”(ë©€í‹°í”¼ë¸ë¦¬í‹°)
- 3ì°¨ ë‹¨ê³„: DDPì™€ ê²°í•©í•´ GPUë‹¹ ì„œë¸Œë„ë©”ì¸ í• ë‹¹(ëŒ€ê·œëª¨ 3D ê°€ì†)

---

### 3.5 torch.func ê¸°ë°˜ 2ê³„ ë¯¸ë¶„ ë²¡í„°í™”(ì‹¤ë¬´ ìµœì í™”)

**ì•„ì´ë””ì–´**: PyTorch 2.xì˜ `torch.func.jacfwd/jacrev`ì™€ `vmap`ì„ ì¡°í•©í•´ ë°°ì¹˜ í—¤ì‹œì•ˆ ëŒ€ê°ì„ íš¨ìœ¨ì ìœ¼ë¡œ êµ¬í•¨.

```python
import torch
from torch import func as F

def laplacian_func(u_fn, x):
    # u_fn: x -> u(x)
    def grad_u(x):
        return F.jacfwd(u_fn)(x)  # (B, D)
    def hess_diag(x):
        J = grad_u(x)             # (B, D)
        # ê° ì¢Œí‘œ ì„±ë¶„ë³„ 2ê³„ ë¯¸ë¶„ ëŒ€ê° ì¶”ì¶œ
        diag_terms = []
        for i in range(x.shape[-1]):
            def comp_fn(x):
                return grad_u(x)[..., i]
            # d/dx_i of grad_u_i
            dxi = F.jacfwd(comp_fn)(x)[..., i]
            diag_terms.append(dxi)
        return sum(diag_terms)
    return hess_diag(x)
```

**íš¨ê³¼**: autograd í˜¸ì¶œ ìˆ˜ ê°ì†Œë¡œ 2ì°¨ ë„í•¨ìˆ˜ ê¸°ë°˜ ì”ì°¨(ë¼í”Œë¼ì‹œì•ˆ ë“±) ê³„ì‚°ì„ 20-40% ì ˆê°(ë¬¸ì œ ê·œëª¨ì— ë”°ë¼).

## 4. í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼ë²•

### 4.1 ë©€í‹° í”¼ë¸ë¦¬í‹° ë°©ë²• (Multi-Fidelity)

**ì•„ì´ë””ì–´**:
- ì €í•´ìƒë„ ê·¸ë¦¬ë“œì—ì„œ ë¹ ë¥´ê²Œ ì‚¬ì „í•™ìŠµ
- ê³ í•´ìƒë„ ê·¸ë¦¬ë“œë¡œ ì ì§„ì  ë¯¸ì„¸ì¡°ì •

**êµ¬í˜„**:
```python
def multi_fidelity_training():
    # Level 1: 100 í¬ì¸íŠ¸
    train(model, n_points=100, epochs=5000)
    
    # Level 2: 500 í¬ì¸íŠ¸
    train(model, n_points=500, epochs=3000)
    
    # Level 3: 2000 í¬ì¸íŠ¸
    train(model, n_points=2000, epochs=2000)
```

### 4.2 ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ (Curriculum Learning)

**ë…¼ë¬¸**: Li et al., "Causality-enhanced Discreted Physics-informed Neural Networks for Predicting Evolutionary Equations" (2024)  
**ì¸ìš©ìˆ˜**: 3íšŒ

**ì „ëµ**:
1. **ë‚œì´ë„ ì¦ê°€**: ë‹¨ìˆœ â†’ ë³µì¡
   - ë‚®ì€ Reynolds ìˆ˜ â†’ ë†’ì€ Reynolds ìˆ˜
   - ì‘ì€ ë„ë©”ì¸ â†’ í° ë„ë©”ì¸
   - ì§§ì€ ì‹œê°„ â†’ ê¸´ ì‹œê°„
2. **ì†ì‹¤ ê°€ì¤‘ì¹˜ ì¡°ì •**: ì´ˆê¸°ì—ëŠ” BC/IC ì¤‘ì‹œ â†’ í›„ê¸°ì—ëŠ” PDE ì¤‘ì‹œ

### 4.3 ì•™ìƒë¸” ë°©ë²•

**ì•„ì´ë””ì–´**: ì—¬ëŸ¬ ëª¨ë¸ì˜ ì˜ˆì¸¡ì„ ê²°í•©í•˜ì—¬ ë¶ˆí™•ì‹¤ì„± ì •ëŸ‰í™” ë° ì •í™•ë„ í–¥ìƒ

```python
class EnsemblecPIKAN(nn.Module):
    def __init__(self, num_models, layers_dims, cheby_order, ...):
        super().__init__()
        self.models = nn.ModuleList([
            Scaled_cPIKAN(layers_dims, cheby_order, ...)
            for _ in range(num_models)
        ])
    
    def forward(self, x):
        predictions = [model(x) for model in self.models]
        mean = torch.stack(predictions).mean(dim=0)
        std = torch.stack(predictions).std(dim=0)
        return mean, std  # ë¶ˆí™•ì‹¤ì„± ì¶”ì • í¬í•¨
```

---

## 5. Scaled-cPIKAN í”„ë¡œì íŠ¸ ì ìš© ë¡œë“œë§µ

### 5.1 ì¦‰ì‹œ ì ìš© ê°€ëŠ¥ (1-2ì£¼, Priority: Critical)

ì´ë¯¸ `doc/v2/ì„±ëŠ¥_í–¥ìƒ_ì¦‰ì‹œ_ì ìš©_ê°€ì´ë“œ.md`ì— í¬í•¨ëœ í•­ëª©ë“¤:

âœ… **ì™„ë£Œëœ í•­ëª©**:
- [x] ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ (1ë‹¨ê³„)
- [x] Mixed Precision Training (1ë‹¨ê³„)
- [x] Early Stopping (1ë‹¨ê³„)

ğŸ”„ **ì§„í–‰ ì¤‘**:
- [ ] ë™ì  ì†ì‹¤ ê°€ì¤‘ì¹˜ (`DynamicWeightedLoss`) ê¸°ë³¸ í™œì„±í™” (2ë‹¨ê³„)
- [ ] ì ì‘í˜• ì½œë¡œì¼€ì´ì…˜ ìƒ˜í”Œë§ (`AdaptiveResidualSampler`) í†µí•© (2ë‹¨ê³„)

### 5.2 ë‹¨ê¸° ì ìš© (2-4ì£¼, Priority: High)

#### 5.2.1 ì†ì‹¤ í•¨ìˆ˜ ê°œì„ 

- [ ] **ConFIG ë°©ë²• êµ¬í˜„** (src/train.py)
  - ì˜ˆìƒ ì‹œê°„: 3-5ì¼
  - ì˜ˆìƒ íš¨ê³¼: í›ˆë ¨ ì•ˆì •ì„± 20-30% í–¥ìƒ
  - íŒŒì¼: `src/train.py` â†’ `ConFIGOptimizer` í´ë˜ìŠ¤ ì¶”ê°€

- [ ] **ë¶€í”¼ ê°€ì¤‘ ìƒ˜í”Œë§** (VW-PINNs)
  - ì˜ˆìƒ ì‹œê°„: 2-3ì¼
  - ì˜ˆìƒ íš¨ê³¼: ë¹„ê· ì¼ ìƒ˜í”Œë§ ì‹œ ì˜¤ì°¨ 10ë°° ê°ì†Œ
  - íŒŒì¼: `src/data.py` â†’ `VolumeWeightedSampler` í´ë˜ìŠ¤ ì¶”ê°€

#### 5.2.2 ìŠ¤í™íŠ¸ëŸ´ í¸í–¥ ê·¹ë³µ

- [ ] **Fourier Features ë ˆì´ì–´ ì¶”ê°€**
  - ì˜ˆìƒ ì‹œê°„: 2ì¼
  - ì˜ˆìƒ íš¨ê³¼: ê³ ì£¼íŒŒ ë¬¸ì œ ì •í™•ë„ í–¥ìƒ
  - íŒŒì¼: `src/models.py` â†’ `FourierFeatureLayer` ë° `Scaled_cPIKAN_FF`

#### 5.2.3 ìë™ ë¯¸ë¶„ íš¨ìœ¨í™”

- [ ] **2ì°¨ ë„í•¨ìˆ˜ ë²¡í„°í™”**
  - ì˜ˆìƒ ì‹œê°„: 2ì¼
  - ì˜ˆìƒ íš¨ê³¼: ê³„ì‚° ì‹œê°„ 20-40% ì ˆê°
  - íŒŒì¼: `src/loss.py` â†’ `laplacian_efficient()` í•¨ìˆ˜

### 5.3 ì¤‘ê¸° ì ìš© (1-2ê°œì›”, Priority: Medium)

#### 5.3.1 ì „ì´ í•™ìŠµ í”„ë ˆì„ì›Œí¬

- [ ] **ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ íŒŒì´í”„ë¼ì¸**
  - íŒŒì¼: `examples/train_curriculum.py` ì‹ ê·œ ì‘ì„±
  - ì €ì£¼íŒŒ â†’ ê³ ì£¼íŒŒ ë‹¨ê³„ë³„ í›ˆë ¨

- [ ] **ë©€í‹° í”¼ë¸ë¦¬í‹° í›ˆë ¨**
  - íŒŒì¼: `src/train.py` â†’ `MultiResolutionTrainer` í´ë˜ìŠ¤

#### 5.3.2 ë„ë©”ì¸ ë¶„í•´

- [ ] **í•˜ìœ„ ë„ë©”ì¸ ë¶„í•  ë° ë³‘ë ¬ í›ˆë ¨**
  - íŒŒì¼: `src/models.py` â†’ `DomainDecomposedcPIKAN`
  - ì˜ˆìƒ íš¨ê³¼: í° ë„ë©”ì¸ì—ì„œ 2-4ë°° ì†ë„ í–¥ìƒ (GPU ìˆ˜ì— ë¹„ë¡€)

#### 5.3.3 Wavelet-KAN ë³€í˜•

- [ ] **WaveletKANLayer êµ¬í˜„**
  - íŒŒì¼: `src/models.py` â†’ `WaveletKANLayer` í´ë˜ìŠ¤
  - ìš©ë„: êµ­ì†Œ ê¸‰ê²© ë³€í™” ë¬¸ì œ (ì¶©ê²©íŒŒ, ë¶ˆì—°ì†ë©´)

### 5.4 ì¥ê¸° ì—°êµ¬ (3-6ê°œì›”, Priority: Low)

#### 5.4.1 ì‹ ê²½ ì—°ì‚°ì í†µí•© (Neural Operators)

- Fourier Neural Operator (FNO)ì™€ì˜ í•˜ì´ë¸Œë¦¬ë“œ
- íŒŒë¼ë©”íŠ¸ë¦­ PDE ì†”ë²„ (ë‹¤ì–‘í•œ ê²½ê³„ ì¡°ê±´ ë™ì‹œ í•™ìŠµ)

#### 5.4.2 ë¶ˆí™•ì‹¤ì„± ì •ëŸ‰í™” (Uncertainty Quantification)

- ë² ì´ì§€ì•ˆ PINN (Bayesian-cPIKAN)
- ì•™ìƒë¸” ë°©ë²•
- Conformal Prediction ì ìš©

#### 5.4.3 ìë™ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹

- Neural Architecture Search (NAS) for optimal layer dims
- Automated loss weight tuning (Meta-learning)

---

## 5.x ì¶”ê°€ ê¶Œì¥ ë¡œë“œë§µ (ì •í™•ë„ ì¤‘ì‹¬)

### ì¦‰ì‹œ/ë‹¨ê¸° (1â€“4ì£¼)
- [ ] Augmented Lagrangian ê¸°ë°˜ BC/IC ê°•í™”(`AugmentedLagrangianLoss`) ì‹œë²” ì ìš© â€” Poisson/Helmholtzì— A/B í…ŒìŠ¤íŠ¸ë¡œ ìƒëŒ€ L2 ë³€í™” ì¸¡ì •
- [ ] Deep Ritz ë³€ë¶„ ì†ì‹¤ ì˜µì…˜ ì¶”ê°€(`--loss-mode=ritz|residual`) â€” Poisson/ì •ìƒíƒ€ì… PDE ìš°ì„  ì ìš©

### ì¤‘ê¸° (1â€“2ê°œì›”)
- [ ] ë”¥ ë„ë©”ì¸ ë¶„í•´(ë©€í‹°í”¼ë¸ë¦¬í‹°) ì‹¤í—˜ â€” 1D/2D ì‹œê°„ ì˜ì¡´ ë¬¸ì œë¡œ ì‹œì‘, DDP ê²°í•© ë²¤ì¹˜ë§ˆí¬
- [ ] NTK/ê·¸ë˜ë””ì–¸íŠ¸ ìŠ¤ì¼€ì¼ ê¸°ë°˜ ë™ì  ê°€ì¤‘(ConFIG/DCGDì™€ ë¹„êµ) â€” ì•ˆì •ì„±/ìˆ˜ë ´ìœ¨ í‘œì¤€í™” í‰ê°€ ì§€í‘œ í™•ë¦½

### ì¥ê¸° (3â€“6ê°œì›”)
- [ ] í•˜ë“œ ì œì•½ íŒŒë¼ë©”í„°ë¼ì´ì œì´ì…˜(ê±°ë¦¬ í•¨ìˆ˜) ë¼ì´ë¸ŒëŸ¬ë¦¬í™” â€” ë‹¤ì–‘í•œ ê²½ê³„ ìœ í˜• ì§€ì› í…œí”Œë¦¿ ì œê³µ
- [ ] ë³€ë¶„/ì œì•½ í•˜ì´ë¸Œë¦¬ë“œ(Deep Ritz + AL) ì¡°í•© í‰ê°€ â€” ê³ ë‚œì´ë„ BCì—ì„œì˜ ì •í™•ë„ ìƒí•œ íƒìƒ‰

## 6. ì°¸ê³ ë¬¸í—Œ

### 6.1 í•µì‹¬ ë…¼ë¬¸ (High Impact)

#### ì†ì‹¤ í•¨ìˆ˜ ë° ìµœì í™”

1. **VW-PINNs** (35 citations, 2024)  
   Song et al., "VW-PINNs: A volume weighting method for PDE residuals in physics-informed neural networks"  
   DOI: 10.48550/arXiv.2401.06196  
   **í•µì‹¬**: ë¶€í”¼ ê°€ì¤‘ ì”ì°¨ ìƒ˜í”Œë§

2. **ConFIG** (20 citations, 2024)  
   Liu et al., "ConFIG: Towards Conflict-free Training of Physics Informed Neural Networks"  
   DOI: 10.48550/arXiv.2408.11104  
   **í•µì‹¬**: ì¶©ëŒ ì—†ëŠ” ê·¸ë˜ë””ì–¸íŠ¸ ì—…ë°ì´íŠ¸

3. **DCGD** (8 citations, 2024)  
   Hwang & Lim, "Dual Cone Gradient Descent for Training Physics-Informed Neural Networks"  
   DOI: 10.48550/arXiv.2409.18426  
   **í•µì‹¬**: ì´ì¤‘ ì›ë¿” ê·¸ë˜ë””ì–¸íŠ¸ í•˜ê°•

#### ì ì‘í˜• ìƒ˜í”Œë§

4. **TCAS-PINN** (8 citations, 2024)  
   Guo et al., "TCAS-PINN: Physics-informed neural networks with a novel temporal causality-based adaptive sampling method"  
   DOI: 10.1088/1674-1056/ad21f3  
   **í•µì‹¬**: ì‹œê°„ ì¸ê³¼ì„± ê¸°ë°˜ ìƒ˜í”Œë§

5. **Advancing Fluid Dynamics** (21 citations, 2024)  
   Zhou et al., "Advancing fluid dynamics simulations: A comprehensive approach to optimizing physics-informed neural networks"  
   DOI: 10.1063/5.0180770  
   **í•µì‹¬**: ì”ì°¨ ê¸°ë°˜ ì ì‘í˜• ìƒ˜í”Œë§ + ì ì‘í˜• ì†ì‹¤ ê°€ì¤‘ì¹˜ + ì°¨ë¶„ ì§„í™” ì•Œê³ ë¦¬ì¦˜

#### ìŠ¤í™íŠ¸ëŸ´ í¸í–¥ ê·¹ë³µ

6. **Fourier Features for PINNs** (11 citations, 2024)  
   Chai et al., "Overcoming the Spectral Bias Problem of Physics-Informed Neural Networks"  
   DOI: 10.1109/TGRS.2024.3440471  
   **í•µì‹¬**: Random Fourier Featuresë¡œ ê³ ì£¼íŒŒ í‘œí˜„

7. **Wavelet-KAN** (4 citations, 2025)  
   Meshir et al., "On the study of frequency control and spectral bias in Wavelet-Based Kolmogorov Arnold networks"  
   DOI: 10.48550/arXiv.2502.00280  
   **í•µì‹¬**: Wavelet ê¸°ë°˜ KAN, NTK ë¶„ì„

#### ì „ì´ í•™ìŠµ

8. **Transfer Learning for PINNs** (23 citations, 2024)  
   Mustajab et al., "Physics-Informed Neural Networks for High-Frequency and Multi-Scale Problems using Transfer Learning"  
   DOI: 10.48550/arXiv.2401.02810  
   **í•µì‹¬**: ì €ì£¼íŒŒ â†’ ê³ ì£¼íŒŒ ë‹¨ê³„ì  í•™ìŠµ

#### ë„ë©”ì¸ ë¶„í•´

9. **Schwarz Preconditioning** (24 citations, 2023)  
   KopanicÃ¡kovÃ¡ et al., "Enhancing training of physics-informed neural networks using domain-decomposition based preconditioning strategies"  
   DOI: 10.1137/23M1583375  
   **í•µì‹¬**: ë„ë©”ì¸ ë¶„í•´ ê¸°ë°˜ ì „ì²˜ë¦¬, ë³‘ë ¬ í›ˆë ¨

10. **Distributed Domain Decomposition** (4 citations, 2023)  
    Feeney et al., "Breaking Boundaries: Distributed Domain Decomposition with Scalable Physics-Informed Neural PDE Solvers"  
    DOI: 10.1145/3581784.3613217  
    **í•µì‹¬**: ë‹¤ì¤‘ GPU ë¶„ì‚° í›ˆë ¨

11. **Hybrid KAN + Augmented Lagrangian** (2025)  
    Zhang et al., "Physics-informed neural networks with hybrid Kolmogorovâ€“Arnold network and augmented Lagrangian function for solving partial differential equations"  
    DOI: 10.1038/s41598-025-92900-1  
    **í•µì‹¬**: KANê³¼ AL ê²°í•©ìœ¼ë¡œ ì œì•½ ë§Œì¡±/ì •í™•ë„ í–¥ìƒ

12. **AL-PINNs** (2022)  
    Son et al., "AL-PINNs: Augmented Lagrangian relaxation method for Physics-Informed Neural Networks"  
    DOI: 10.48550/arXiv.2205.01059  
    **í•µì‹¬**: ì œì•½ ë§Œì¡±ì„ ìœ„í•œ AL ì¼ë°˜ í”„ë ˆì„ì›Œí¬

13. **Multifidelity Domain Decomposition PINNs** (2024)  
    Heinlein et al., "Multifidelity domain decomposition-based physics-informed neural networks for time-dependent problems"  
    DOI: 10.48550/arXiv.2401.07888  
    **í•µì‹¬**: ì‹œê°„ ì˜ì¡´ ë¬¸ì œì—ì„œ ë©€í‹°í”¼ë¸ë¦¬í‹° DDM ì ìš©

### 6.2 Scaled-cPIKAN ì›ë³¸ ë…¼ë¬¸

14. **Scaled-cPIKANs** (3 citations, 2025)  
    Mostajeran & Faroughi, "Scaled-cPIKANs: Domain Scaling in Chebyshev-based Physics-informed Kolmogorov-Arnold Networks"  
    DOI: 10.48550/arXiv.2501.02762  
    Journal: Journal of Computational Physics (2025)  
    **í•µì‹¬**: Chebyshev ê¸°ë°˜ KAN + ë„ë©”ì¸ ìŠ¤ì¼€ì¼ë§ + PINN

### 6.3 ê´€ë ¨ KAN ë…¼ë¬¸

15. **Physics-informed KAN with Chebyshev** (8 citations, 2024)  
    Guo et al., "Physics-informed Kolmogorovâ€“Arnold network with Chebyshev polynomials for fluid mechanics"  
    DOI: 10.1063/5.0284999

16. **EPi-cKANs** (9 citations, 2024)  
    Mostajeran & Faroughi, "EPi-cKANs: Elasto-Plasticity Informed Kolmogorov-Arnold Networks Using Chebyshev Polynomials"  
    DOI: 10.48550/arXiv.2410.10897

### 6.4 ë¦¬ë·° ë…¼ë¬¸

17. **Review of PINNs** (2025)  
    Plankovskyy et al., "Review of Physics-Informed Neural Networks: Challenges in Loss Function Design and Geometric Integration"  
    DOI: 10.3390/math13203289

---

## 7. êµ¬í˜„ ìš°ì„ ìˆœìœ„ ìš”ì•½í‘œ

| ê¸°ë²• | ë‚œì´ë„ | ì˜ˆìƒ ì‹œê°„ | ì˜ˆìƒ íš¨ê³¼ | ìš°ì„ ìˆœìœ„ |
|------|--------|----------|----------|---------|
| ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ | â­ | 1ì¼ | ì•ˆì •ì„±â†‘ | âœ… ì™„ë£Œ |
| Mixed Precision (AMP) | â­ | 1ì¼ | ì†ë„ 1.5-2x | âœ… ì™„ë£Œ |
| Early Stopping | â­ | 1ì¼ | ì‹œê°„ ì ˆì•½ | âœ… ì™„ë£Œ |
| ë™ì  ì†ì‹¤ ê°€ì¤‘ì¹˜ | â­â­ | 2ì¼ | ìˆ˜ë ´ ì•ˆì • | ğŸ”´ High |
| ì ì‘í˜• ìƒ˜í”Œë§ | â­â­ | 3ì¼ | ì˜¤ì°¨ ê°ì†Œ | ğŸ”´ High |
| ConFIG ìµœì í™” | â­â­â­ | 5ì¼ | ì•ˆì •ì„± 20-30%â†‘ | ğŸ”´ High |
| ë¶€í”¼ ê°€ì¤‘ ìƒ˜í”Œë§ | â­â­ | 3ì¼ | ë¹„ê· ì¼ ìƒ˜í”Œ 10x ê°œì„  | ğŸŸ¡ Medium |
| Fourier Features | â­â­ | 2ì¼ | ê³ ì£¼íŒŒ ì •í™•ë„â†‘ | ğŸŸ¡ Medium |
| 2ì°¨ ë„í•¨ìˆ˜ ë²¡í„°í™” | â­â­ | 2ì¼ | ì†ë„ 20-40%â†‘ | ğŸŸ¡ Medium |
| ì „ì´ í•™ìŠµ | â­â­â­ | 1ì£¼ | ê³ ì£¼íŒŒ ë¬¸ì œ ê°œì„  | ğŸŸ¡ Medium |
| ë„ë©”ì¸ ë¶„í•´ | â­â­â­â­ | 2ì£¼ | ëŒ€ê·œëª¨ 2-4x ì†ë„ | ğŸŸ¢ Low |
| Wavelet-KAN | â­â­â­â­ | 2ì£¼ | ë¶ˆì—°ì† ë¬¸ì œ ê°œì„  | ğŸŸ¢ Low |

**ë²”ë¡€**:
- â­: ë§¤ìš° ì‰¬ì›€
- â­â­: ë³´í†µ
- â­â­â­: ì–´ë ¤ì›€
- â­â­â­â­: ë§¤ìš° ì–´ë ¤ì›€
- ğŸ”´ High: 1ê°œì›” ì´ë‚´ ì ìš©
- ğŸŸ¡ Medium: 1-2ê°œì›” ì ìš©
- ğŸŸ¢ Low: 2-6ê°œì›” ì—°êµ¬

---

## 8. ê²°ë¡  ë° ê¶Œì¥ì‚¬í•­

### 8.1 í•µì‹¬ ë°œê²¬ì‚¬í•­

1. **ì†ì‹¤ í•¨ìˆ˜ ê· í˜•**ì´ PINN ì„±ê³µì˜ ê°€ì¥ ì¤‘ìš”í•œ ìš”ì†Œ
   - ConFIG, DCGD ê°™ì€ ì¶©ëŒ ì—†ëŠ” ê·¸ë˜ë””ì–¸íŠ¸ ë°©ë²•ì´ íš¨ê³¼ì 
   - ë™ì  ê°€ì¤‘ì¹˜ ì¡°ì •ìœ¼ë¡œ ìˆ˜ë™ íŠœë‹ ë¶€ë‹´ ê°ì†Œ

2. **ì ì‘í˜• ìƒ˜í”Œë§**ì´ ë°ì´í„° íš¨ìœ¨ì„±ì˜ í•µì‹¬
   - ì”ì°¨ ê¸°ë°˜ ìƒ˜í”Œë§ìœ¼ë¡œ ë™ì¼ í¬ì¸íŠ¸ë¡œ ë” ë‚˜ì€ ì •í™•ë„
   - ë¶€í”¼ ê°€ì¤‘ ë°©ë²•ìœ¼ë¡œ ë¹„ê· ì¼ ìƒ˜í”Œë§ ë¬¸ì œ í•´ê²°

3. **ìŠ¤í™íŠ¸ëŸ´ í¸í–¥**ì€ ì—¬ì „íˆ ì£¼ìš” ë„ì „ ê³¼ì œ
   - Fourier Features, Wavelet-KAN ë“± ë‹¤ì–‘í•œ í•´ê²°ì±… ì¡´ì¬
   - **Scaled-cPIKANì˜ Chebyshev ê¸°ë°˜ ì ‘ê·¼ì´ ì´ë¯¸ ê°•ë ¥í•œ ê¸°ë°˜**

4. **ì „ì´ í•™ìŠµê³¼ ì»¤ë¦¬í˜ëŸ¼**ì´ ê³ ë‚œì´ë„ ë¬¸ì œì˜ ëŒíŒŒêµ¬
   - ì €ì£¼íŒŒ â†’ ê³ ì£¼íŒŒ ë‹¨ê³„ì  ì ‘ê·¼
   - ì‘ì€ ë„ë©”ì¸ â†’ í° ë„ë©”ì¸ í™•ì¥

5. **ë³‘ë ¬í™”**ê°€ ëŒ€ê·œëª¨ í™•ì¥ì˜ ì—´ì‡ 
   - ë„ë©”ì¸ ë¶„í•´ë¡œ í° ë¬¸ì œë¥¼ ì‘ì€ ë¬¸ì œë¡œ
   - ë‹¤ì¤‘ GPUë¡œ ì„ í˜• ì†ë„ í–¥ìƒ

### 8.2 Scaled-cPIKANì— ëŒ€í•œ ê¶Œì¥ì‚¬í•­

#### ì¦‰ì‹œ ì ìš© (ì´ë²ˆ ì£¼ ë‚´)

1. âœ… **ê¸°ë³¸ ì•ˆì •í™” ì™„ë£Œ í™•ì¸**
   - ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
   - Mixed Precision
   - Early Stopping

2. ğŸ”´ **ë™ì  ì†ì‹¤ ê°€ì¤‘ì¹˜ í™œì„±í™”**
   - `examples/run_pipeline.py`ì—ì„œ `DynamicWeightedLoss` ê¸°ë³¸ ì‚¬ìš©
   - ëª¨ë“  PINN ì˜ˆì œì— ì ìš©

3. ğŸ”´ **ì ì‘í˜• ìƒ˜í”Œë§ í†µí•©**
   - `AdaptiveResidualSampler`ë¥¼ í›ˆë ¨ ë£¨í”„ì— í†µí•©
   - 2-3íšŒ ì •ì œ ë°˜ë³µ ê¸°ë³¸ ì„¤ì •

#### 1ê°œì›” ë‚´ ì ìš©

4. **ConFIG ìµœì í™” êµ¬í˜„**
   - ìƒˆë¡œìš´ `ConFIGOptimizer` í´ë˜ìŠ¤
   - ê¸°ì¡´ Adam/L-BFGSì™€ ë³‘í–‰ ê°€ëŠ¥í•˜ë„ë¡ ì„¤ê³„

5. **Fourier Features ì˜µì…˜ ì¶”ê°€**
   - ê³ ì£¼íŒŒ ë¬¸ì œìš© `Scaled_cPIKAN_FF` ë³€í˜•
   - í•˜ì´í¼íŒŒë¼ë¯¸í„°: `use_fourier=True/False`

6. **2ì°¨ ë„í•¨ìˆ˜ íš¨ìœ¨í™”**
   - `src/loss.py`ì˜ ëª¨ë“  Laplacian ê³„ì‚° ìµœì í™”
   - ë²¤ì¹˜ë§ˆí¬ë¡œ ì„±ëŠ¥ ì¸¡ì •

#### 2-3ê°œì›” ë‚´ ì—°êµ¬

7. **ì „ì´ í•™ìŠµ í”„ë ˆì„ì›Œí¬**
   - ìë™ ì»¤ë¦¬í˜ëŸ¼ ìƒì„±
   - ì²´í¬í¬ì¸íŠ¸ ê¸°ë°˜ ì ì§„ì  í•™ìŠµ

8. **ë„ë©”ì¸ ë¶„í•´ ë³‘ë ¬í™”**
   - í° 3D ë¬¸ì œë¥¼ ìœ„í•œ í•„ìˆ˜ ê¸°ëŠ¥
   - ë‹¤ì¤‘ GPU í™˜ê²½ì—ì„œ ë²¤ì¹˜ë§ˆí¬

#### ì¥ê¸° ë¹„ì „ (6ê°œì›”+)

9. **íŒŒë¼ë©”íŠ¸ë¦­ PDE ì†”ë²„**
   - ë‹¤ì–‘í•œ ê²½ê³„ ì¡°ê±´ì„ í•œ ë²ˆì— í•™ìŠµ
   - Neural Operator í†µí•©

10. **ë¶ˆí™•ì‹¤ì„± ì •ëŸ‰í™”**
    - ì•™ìƒë¸” ë°©ë²•
    - ë² ì´ì§€ì•ˆ ì ‘ê·¼

### 8.3 ì„±ê³µ ì§€í‘œ (KPIs)

êµ¬í˜„ í›„ ë‹¤ìŒ ì§€í‘œë¡œ ì„±ëŠ¥ í–¥ìƒ ì¸¡ì •:

| ì§€í‘œ | í˜„ì¬ (baseline) | ëª©í‘œ (1ê°œì›” í›„) | ëª©í‘œ (3ê°œì›” í›„) |
|-----|----------------|----------------|----------------|
| Helmholtz 1D ìƒëŒ€ L2 ì˜¤ì°¨ | ~1e-4 | < 5e-5 | < 1e-5 |
| 3D Poisson ìˆ˜ë ´ ì‹œê°„ | ì‹¤íŒ¨/ëŠë¦¼ | 30ë¶„ | 15ë¶„ |
| í›ˆë ¨ ì•ˆì •ì„± (ì„±ê³µë¥ ) | 70% | 90% | 95% |
| í•„ìš” ì½œë¡œì¼€ì´ì…˜ í¬ì¸íŠ¸ | 10,000 | 5,000 | 3,000 |
| ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ | 100% | 50-60% (AMP) | 40-50% |

---

**ë¬¸ì„œ ë**  
**ì‘ì„±**: AI Research Assistant ì¡°ì‚¬ ê²°ê³¼ ê¸°ë°˜  
**ê²€í†  í•„ìš”**: ê° ê¸°ë²•ì˜ êµ¬í˜„ ì„¸ë¶€ì‚¬í•­ ë° í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹  
**ì—…ë°ì´íŠ¸ ì£¼ê¸°**: ë¶„ê¸°ë³„ (ìƒˆë¡œìš´ ë…¼ë¬¸ ë° ê¸°ë²• ì¶”ê°€)
