# 5단계: 훈련 과정 🟡

> **난이도:** 중급  
> **소요 시간:** 1시간 (훈련 시간 포함)  
> **사전 지식:** 최적화 알고리즘 기초

---

## 🎯 학습 목표

이 단계를 마치면 다음을 할 수 있습니다:

- ✅ PINN 2단계 훈련 전략 이해하기
- ✅ Adam 최적화기 사용하기
- ✅ L-BFGS 최적화기로 미세조정하기
- ✅ 학습률 스케줄링 적용하기
- ✅ 훈련 과정 모니터링 및 로깅하기
- ✅ 조기 종료 (Early Stopping) 구현하기

---

## 🔄 2단계 최적화 전략

### 왜 2단계인가요?

PINN 훈련은 어렵습니다! 단일 최적화기로는 충분하지 않을 수 있습니다.

**전략:**
```
[1단계] Adam으로 빠른 사전학습
   ↓
[2단계] L-BFGS로 정밀한 미세조정
```

### 각 단계의 역할

#### 1단계: Adam (Stochastic Optimizer)

**특징:**
- ✅ 빠른 초기 수렴
- ✅ 노이즈에 강건
- ✅ 메모리 효율적
- ❌ 최종 정확도 제한

**목표:** 좋은 초기 지점 찾기

#### 2단계: L-BFGS (Quasi-Newton Method)

**특징:**
- ✅ 매우 정밀한 수렴
- ✅ 함수 최솟값에 가까이 접근
- ❌ 초기 지점에 민감
- ❌ 메모리 많이 사용

**목표:** 정밀한 해 획득

### 비유

```
Adam:     "빠른 자동차로 대충 목적지 근처까지"
L-BFGS:   "도보로 정확한 주소 찾기"
```

---

## 🚀 1단계: Adam 사전학습

### Adam 최적화기 생성

```python
import torch
from torch.optim import Adam
from src.models import Scaled_cPIKAN

# 모델 생성
model = Scaled_cPIKAN(
    layers_hidden=[1, 20, 20, 1],
    degree=3,
    domain_min=torch.tensor([0.0]),
    domain_max=torch.tensor([1.0])
)

# Adam 최적화기
optimizer = Adam(
    model.parameters(),
    lr=1e-3,              # 학습률
    betas=(0.9, 0.999),   # 모멘텀 파라미터
    eps=1e-8              # 수치 안정성
)

print(f"최적화기: {optimizer}")
```

### 훈련 루프

```python
from tqdm import tqdm

# 하이퍼파라미터
num_epochs = 1000
log_interval = 100

# 훈련 히스토리
history = {'loss': [], 'loss_pde': [], 'loss_bc': []}

# 훈련 시작
for epoch in tqdm(range(num_epochs), desc="Adam 사전학습"):
    # Forward pass
    loss, metrics = loss_function(model, x_collocation, x_bc, u_bc)
    
    # Backward pass
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    # 로깅
    history['loss'].append(metrics['loss_total'])
    history['loss_pde'].append(metrics['loss_pde'])
    history['loss_bc'].append(metrics['loss_bc'])
    
    # 출력
    if (epoch + 1) % log_interval == 0:
        print(f"Epoch {epoch+1}/{num_epochs}")
        print(f"  Loss: {metrics['loss_total']:.6f}")
        print(f"  PDE: {metrics['loss_pde']:.6f}, BC: {metrics['loss_bc']:.6f}")

print("✅ Adam 사전학습 완료!")
```

### 학습률 스케줄링

학습률을 점진적으로 감소시켜 안정적인 수렴을 유도합니다.

```python
from torch.optim.lr_scheduler import ExponentialLR

# 스케줄러 생성
scheduler = ExponentialLR(
    optimizer,
    gamma=0.995  # 매 에포크마다 학습률 × 0.995
)

# 훈련 루프에서
for epoch in range(num_epochs):
    # ... 훈련 코드 ...
    
    # 학습률 업데이트
    scheduler.step()
    
    if (epoch + 1) % log_interval == 0:
        current_lr = optimizer.param_groups[0]['lr']
        print(f"  학습률: {current_lr:.2e}")
```

---

## 🎯 2단계: L-BFGS 미세조정

### L-BFGS 최적화기 생성

```python
from torch.optim import LBFGS

# L-BFGS 최적화기
optimizer_lbfgs = LBFGS(
    model.parameters(),
    lr=1.0,                  # L-BFGS는 보통 1.0 사용
    max_iter=20,             # 라인 서치 최대 반복
    history_size=50,         # 히스토리 저장 크기
    line_search_fn='strong_wolfe'  # 라인 서치 방법
)

print(f"L-BFGS 최적화기 준비 완료")
```

### L-BFGS 훈련 루프

L-BFGS는 **closure 함수**를 요구합니다!

```python
def closure():
    """L-BFGS를 위한 closure 함수"""
    optimizer_lbfgs.zero_grad()
    loss, metrics = loss_function(model, x_collocation, x_bc, u_bc)
    loss.backward()
    return loss

# L-BFGS 미세조정
num_lbfgs_steps = 100

for step in tqdm(range(num_lbfgs_steps), desc="L-BFGS 미세조정"):
    loss = optimizer_lbfgs.step(closure)
    
    if (step + 1) % 10 == 0:
        with torch.no_grad():
            _, metrics = loss_function(model, x_collocation, x_bc, u_bc)
        print(f"Step {step+1}/{num_lbfgs_steps}: Loss = {metrics['loss_total']:.8f}")

print("✅ L-BFGS 미세조정 완료!")
```

---

## 📊 Trainer 클래스 사용하기

프로젝트에는 `src/train.py`에 `Trainer` 클래스가 구현되어 있습니다!

### 기본 사용법

```python
from src.train import Trainer
from src.models import Scaled_cPIKAN
from src.loss import PhysicsInformedLoss
import torch

# 1. 모델 생성
model = Scaled_cPIKAN(
    layers_hidden=[1, 30, 30, 1],
    degree=4,
    domain_min=torch.tensor([0.0]),
    domain_max=torch.tensor([1.0])
)

# 2. 손실 함수 생성
loss_fn = PhysicsInformedLoss(
    pde_fn=my_pde_residual,
    boundary_weight=10.0
)

# 3. 데이터 준비
train_data = {
    'x_pde': x_collocation,
    'x_bc': x_boundary,
    'u_bc': u_boundary
}

# 4. Trainer 생성
trainer = Trainer(
    model=model,
    loss_fn=loss_fn,
    device='cuda' if torch.cuda.is_available() else 'cpu'
)

# 5. 훈련 실행
history = trainer.train(
    train_data=train_data,
    num_epochs_adam=1000,      # Adam 에포크
    num_steps_lbfgs=100,       # L-BFGS 스텝
    lr_adam=1e-3,              # Adam 학습률
    lr_lbfgs=1.0,              # L-BFGS 학습률
    log_interval=100           # 로그 출력 간격
)

print("✅ 훈련 완료!")
```

### 훈련 히스토리 확인

```python
import matplotlib.pyplot as plt

# 손실 곡선 시각화
plt.figure(figsize=(12, 4))

# 총 손실
plt.subplot(1, 3, 1)
plt.plot(history['loss'])
plt.xlabel('Epoch')
plt.ylabel('Total Loss')
plt.title('훈련 손실')
plt.yscale('log')
plt.grid(True, alpha=0.3)

# PDE 손실
plt.subplot(1, 3, 2)
plt.plot(history['loss_pde'])
plt.xlabel('Epoch')
plt.ylabel('PDE Loss')
plt.title('PDE 잔차 손실')
plt.yscale('log')
plt.grid(True, alpha=0.3)

# 경계 조건 손실
plt.subplot(1, 3, 3)
plt.plot(history['loss_bc'])
plt.xlabel('Epoch')
plt.ylabel('BC Loss')
plt.title('경계 조건 손실')
plt.yscale('log')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

---

## ⏹️ 조기 종료 (Early Stopping)

### 개념

손실이 더 이상 개선되지 않으면 훈련을 중단합니다.

**이유:**
- 시간 절약
- 과적합 방지

### 구현

```python
class EarlyStopping:
    """조기 종료 헬퍼 클래스"""
    
    def __init__(self, patience=50, min_delta=1e-6):
        """
        Args:
            patience: 개선 없이 기다릴 에포크 수
            min_delta: 개선으로 인정할 최소 변화량
        """
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.best_loss = float('inf')
        self.early_stop = False
    
    def __call__(self, loss):
        if loss < self.best_loss - self.min_delta:
            # 개선됨
            self.best_loss = loss
            self.counter = 0
        else:
            # 개선 안 됨
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True
        
        return self.early_stop

# 사용 예제
early_stopping = EarlyStopping(patience=50, min_delta=1e-6)

for epoch in range(num_epochs):
    # 훈련...
    loss, metrics = loss_function(model, x_collocation, x_bc, u_bc)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    # 조기 종료 체크
    if early_stopping(metrics['loss_total']):
        print(f"조기 종료: Epoch {epoch+1}")
        break
```

---

## 🖥️ GPU 가속

### GPU로 모델 및 데이터 이동

```python
# 디바이스 설정
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"사용 디바이스: {device}")

# 모델을 GPU로
model = model.to(device)

# 데이터를 GPU로
x_collocation = x_collocation.to(device)
x_bc = x_bc.to(device)
u_bc = u_bc.to(device)

print("✅ GPU 설정 완료")
```

### 메모리 관리

```python
# GPU 메모리 확인
if torch.cuda.is_available():
    print(f"GPU 메모리 사용량: {torch.cuda.memory_allocated() / 1e9:.2f} GB")
    print(f"GPU 메모리 예약량: {torch.cuda.memory_reserved() / 1e9:.2f} GB")

# 메모리 정리
torch.cuda.empty_cache()
```

---

## 📈 훈련 모니터링

### TensorBoard 사용 (선택사항)

```python
from torch.utils.tensorboard import SummaryWriter

# Writer 생성
writer = SummaryWriter('runs/pinn_experiment')

# 훈련 루프에서
for epoch in range(num_epochs):
    loss, metrics = loss_function(model, x_collocation, x_bc, u_bc)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    # TensorBoard 로깅
    writer.add_scalar('Loss/total', metrics['loss_total'], epoch)
    writer.add_scalar('Loss/pde', metrics['loss_pde'], epoch)
    writer.add_scalar('Loss/bc', metrics['loss_bc'], epoch)

writer.close()

# TensorBoard 실행: tensorboard --logdir=runs
```

### 실시간 플롯 (tqdm + matplotlib)

```python
from tqdm import tqdm
import matplotlib.pyplot as plt
from IPython.display import clear_output

# 실시간 플롯 함수
def plot_realtime(history):
    clear_output(wait=True)
    plt.figure(figsize=(10, 4))
    
    plt.subplot(1, 2, 1)
    plt.plot(history['loss'])
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Total Loss')
    plt.yscale('log')
    plt.grid(True)
    
    plt.subplot(1, 2, 2)
    plt.plot(history['loss_pde'], label='PDE')
    plt.plot(history['loss_bc'], label='BC')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Component Losses')
    plt.yscale('log')
    plt.legend()
    plt.grid(True)
    
    plt.tight_layout()
    plt.show()

# 훈련 루프에서 (Jupyter Notebook)
for epoch in tqdm(range(num_epochs)):
    # 훈련...
    
    if (epoch + 1) % 50 == 0:
        plot_realtime(history)
```

---

## 🧪 실습: 완전한 훈련 파이프라인

### 목표

1D Helmholtz 방정식을 처음부터 끝까지 훈련하기

### 전체 코드

```python
import torch
from torch.optim import Adam, LBFGS
from torch.optim.lr_scheduler import ExponentialLR
from src.models import Scaled_cPIKAN
from tqdm import tqdm
import matplotlib.pyplot as plt

# ============================================
# 1. 데이터 준비
# ============================================
print("1. 데이터 준비 중...")
x_collocation = torch.linspace(0.01, 0.99, 200).unsqueeze(1).requires_grad_(True)
x_bc = torch.tensor([[0.0], [1.0]])
u_bc = torch.zeros(2, 1)

# ============================================
# 2. 모델 생성
# ============================================
print("2. 모델 생성 중...")
model = Scaled_cPIKAN(
    layers_hidden=[1, 30, 30, 1],
    degree=4,
    domain_min=torch.tensor([0.0]),
    domain_max=torch.tensor([1.0])
)

# GPU 사용 (가능한 경우)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)
x_collocation = x_collocation.to(device)
x_bc = x_bc.to(device)
u_bc = u_bc.to(device)

print(f"디바이스: {device}")
print(f"파라미터 개수: {sum(p.numel() for p in model.parameters())}")

# ============================================
# 3. 손실 함수 정의
# ============================================
def helmholtz_loss(model, x_col, x_bc, u_bc, k=1.0, lambda_bc=10.0):
    # PDE 잔차
    x_col_grad = x_col.requires_grad_(True)
    u = model(x_col_grad)
    
    du_dx = torch.autograd.grad(u, x_col_grad, torch.ones_like(u), create_graph=True)[0]
    d2u_dx2 = torch.autograd.grad(du_dx, x_col_grad, torch.ones_like(du_dx), create_graph=True)[0]
    
    residual = d2u_dx2 + k**2 * u
    loss_pde = torch.mean(residual ** 2)
    
    # 경계 조건
    u_bc_pred = model(x_bc)
    loss_bc = torch.mean((u_bc_pred - u_bc) ** 2)
    
    # 총 손실
    loss_total = loss_pde + lambda_bc * loss_bc
    
    return loss_total, {'loss_total': loss_total.item(), 'loss_pde': loss_pde.item(), 'loss_bc': loss_bc.item()}

# ============================================
# 4. Adam 사전학습
# ============================================
print("\n3. Adam 사전학습 시작...")
optimizer_adam = Adam(model.parameters(), lr=1e-3)
scheduler = ExponentialLR(optimizer_adam, gamma=0.995)

history = {'loss': [], 'loss_pde': [], 'loss_bc': []}
num_epochs_adam = 2000

for epoch in tqdm(range(num_epochs_adam), desc="Adam"):
    loss, metrics = helmholtz_loss(model, x_collocation, x_bc, u_bc)
    
    optimizer_adam.zero_grad()
    loss.backward()
    optimizer_adam.step()
    scheduler.step()
    
    history['loss'].append(metrics['loss_total'])
    history['loss_pde'].append(metrics['loss_pde'])
    history['loss_bc'].append(metrics['loss_bc'])
    
    if (epoch + 1) % 500 == 0:
        print(f"\nEpoch {epoch+1}: Loss = {metrics['loss_total']:.6f}")

print("✅ Adam 사전학습 완료!")

# ============================================
# 5. L-BFGS 미세조정
# ============================================
print("\n4. L-BFGS 미세조정 시작...")
optimizer_lbfgs = LBFGS(model.parameters(), lr=1.0, max_iter=20)

def closure():
    optimizer_lbfgs.zero_grad()
    loss, _ = helmholtz_loss(model, x_collocation, x_bc, u_bc)
    loss.backward()
    return loss

num_steps_lbfgs = 50
for step in tqdm(range(num_steps_lbfgs), desc="L-BFGS"):
    optimizer_lbfgs.step(closure)
    
    if (step + 1) % 10 == 0:
        with torch.no_grad():
            _, metrics = helmholtz_loss(model, x_collocation, x_bc, u_bc)
        print(f"\nStep {step+1}: Loss = {metrics['loss_total']:.8f}")
        history['loss'].append(metrics['loss_total'])

print("✅ L-BFGS 미세조정 완료!")

# ============================================
# 6. 결과 시각화
# ============================================
print("\n5. 결과 시각화 중...")
plt.figure(figsize=(12, 4))

# 손실 곡선
plt.subplot(1, 2, 1)
plt.plot(history['loss'])
plt.xlabel('Iteration')
plt.ylabel('Loss')
plt.title('Training Loss')
plt.yscale('log')
plt.grid(True, alpha=0.3)
plt.axvline(num_epochs_adam, color='r', linestyle='--', label='Adam → L-BFGS')
plt.legend()

# 예측 결과
plt.subplot(1, 2, 2)
x_test = torch.linspace(0, 1, 200).unsqueeze(1).to(device)
with torch.no_grad():
    u_pred = model(x_test).cpu().numpy()
    u_exact = torch.sin(torch.pi * x_test.cpu()).numpy()  # 정확한 해 (예시)

plt.plot(x_test.cpu().numpy(), u_pred, label='PINN 예측', linewidth=2)
plt.plot(x_test.cpu().numpy(), u_exact, '--', label='정확한 해', linewidth=2)
plt.xlabel('x')
plt.ylabel('u(x)')
plt.title('Solution')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('training_result.png', dpi=150)
plt.show()

print("✅ 전체 파이프라인 완료!")
```

---

## 🎯 체크포인트

다음 항목을 모두 완료했으면 다음 단계로 진행하세요!

- [ ] Adam 최적화기 사용법 이해
- [ ] L-BFGS 최적화기 사용법 이해
- [ ] 2단계 훈련 전략 실행 성공
- [ ] 학습률 스케줄링 적용
- [ ] 훈련 손실 시각화
- [ ] GPU 가속 활용 (가능한 경우)

---

## 🆘 자주 묻는 질문

### Q1: Adam과 L-BFGS 중 어떤 것이 더 좋나요?

**A:** 둘 다 사용하세요! Adam으로 빠르게 시작하고 L-BFGS로 마무리하는 것이 best practice입니다.

### Q2: 손실이 줄어들지 않아요

**A:** 다음을 시도하세요:
1. 학습률 감소 (예: 1e-3 → 1e-4)
2. 더 많은 에포크
3. 경계 조건 가중치 증가
4. 모델 크기 증가

### Q3: "CUDA out of memory" 에러가 나요

**A:** 해결 방법:
1. 배치 크기 감소 (콜로케이션 포인트 수)
2. 모델 크기 감소 (은닉층 뉴런 수)
3. `torch.cuda.empty_cache()` 호출

### Q4: 훈련이 너무 느려요

**A:** 최적화 방법:
1. GPU 사용
2. 콜로케이션 포인트 수 감소
3. Chebyshev 차수 감소
4. Mixed precision training 사용

---

## 📚 추가 자료

- **Adam 논문:** Kingma & Ba, "Adam: A Method for Stochastic Optimization" (2014)
- **L-BFGS 논문:** Liu & Nocedal, "On the limited memory BFGS method" (1989)
- **PyTorch 최적화기 문서:** [https://pytorch.org/docs/stable/optim.html](https://pytorch.org/docs/stable/optim.html)

---

## 🎉 축하합니다!

훈련 과정을 마스터했습니다! 이제 결과를 분석할 준비가 되었습니다.

**다음 단계:**

👉 [6단계: 결과 분석으로 이동하기](06_결과분석.md)

또는 [메인 가이드로 돌아가기](../사용자_가이드.md)

---

*마지막 업데이트: 2025년 1월*
