# 5ë‹¨ê³„: í›ˆë ¨ ê³¼ì • ğŸŸ¡

> **ë‚œì´ë„:** ì¤‘ê¸‰  
> **ì†Œìš” ì‹œê°„:** 1ì‹œê°„ (í›ˆë ¨ ì‹œê°„ í¬í•¨)  
> **ì‚¬ì „ ì§€ì‹:** ìµœì í™” ì•Œê³ ë¦¬ì¦˜ ê¸°ì´ˆ

---

## ğŸ¯ í•™ìŠµ ëª©í‘œ

ì´ ë‹¨ê³„ë¥¼ ë§ˆì¹˜ë©´ ë‹¤ìŒì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

- âœ… PINN 2ë‹¨ê³„ í›ˆë ¨ ì „ëµ ì´í•´í•˜ê¸°
- âœ… Adam ìµœì í™”ê¸° ì‚¬ìš©í•˜ê¸°
- âœ… L-BFGS ìµœì í™”ê¸°ë¡œ ë¯¸ì„¸ì¡°ì •í•˜ê¸°
- âœ… í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§ ì ìš©í•˜ê¸°
- âœ… í›ˆë ¨ ê³¼ì • ëª¨ë‹ˆí„°ë§ ë° ë¡œê¹…í•˜ê¸°
- âœ… ì¡°ê¸° ì¢…ë£Œ (Early Stopping) êµ¬í˜„í•˜ê¸°

---

## ğŸ”„ 2ë‹¨ê³„ ìµœì í™” ì „ëµ

### ì™œ 2ë‹¨ê³„ì¸ê°€ìš”?

PINN í›ˆë ¨ì€ ì–´ë µìŠµë‹ˆë‹¤! ë‹¨ì¼ ìµœì í™”ê¸°ë¡œëŠ” ì¶©ë¶„í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

**ì „ëµ:**
```
[1ë‹¨ê³„] Adamìœ¼ë¡œ ë¹ ë¥¸ ì‚¬ì „í•™ìŠµ
   â†“
[2ë‹¨ê³„] L-BFGSë¡œ ì •ë°€í•œ ë¯¸ì„¸ì¡°ì •
```

### ê° ë‹¨ê³„ì˜ ì—­í• 

#### 1ë‹¨ê³„: Adam (Stochastic Optimizer)

**íŠ¹ì§•:**
- âœ… ë¹ ë¥¸ ì´ˆê¸° ìˆ˜ë ´
- âœ… ë…¸ì´ì¦ˆì— ê°•ê±´
- âœ… ë©”ëª¨ë¦¬ íš¨ìœ¨ì 
- âŒ ìµœì¢… ì •í™•ë„ ì œí•œ

**ëª©í‘œ:** ì¢‹ì€ ì´ˆê¸° ì§€ì  ì°¾ê¸°

#### 2ë‹¨ê³„: L-BFGS (Quasi-Newton Method)

**íŠ¹ì§•:**
- âœ… ë§¤ìš° ì •ë°€í•œ ìˆ˜ë ´
- âœ… í•¨ìˆ˜ ìµœì†Ÿê°’ì— ê°€ê¹Œì´ ì ‘ê·¼
- âŒ ì´ˆê¸° ì§€ì ì— ë¯¼ê°
- âŒ ë©”ëª¨ë¦¬ ë§ì´ ì‚¬ìš©

**ëª©í‘œ:** ì •ë°€í•œ í•´ íšë“

### ë¹„ìœ 

```
Adam:     "ë¹ ë¥¸ ìë™ì°¨ë¡œ ëŒ€ì¶© ëª©ì ì§€ ê·¼ì²˜ê¹Œì§€"
L-BFGS:   "ë„ë³´ë¡œ ì •í™•í•œ ì£¼ì†Œ ì°¾ê¸°"
```

---

## ğŸš€ 1ë‹¨ê³„: Adam ì‚¬ì „í•™ìŠµ

### Adam ìµœì í™”ê¸° ìƒì„±

```python
import torch
from torch.optim import Adam
from src.models import Scaled_cPIKAN

# ëª¨ë¸ ìƒì„±
model = Scaled_cPIKAN(
    layers_hidden=[1, 20, 20, 1],
    degree=3,
    domain_min=torch.tensor([0.0]),
    domain_max=torch.tensor([1.0])
)

# Adam ìµœì í™”ê¸°
optimizer = Adam(
    model.parameters(),
    lr=1e-3,              # í•™ìŠµë¥ 
    betas=(0.9, 0.999),   # ëª¨ë©˜í…€ íŒŒë¼ë¯¸í„°
    eps=1e-8              # ìˆ˜ì¹˜ ì•ˆì •ì„±
)

print(f"ìµœì í™”ê¸°: {optimizer}")
```

### í›ˆë ¨ ë£¨í”„

```python
from tqdm import tqdm

# í•˜ì´í¼íŒŒë¼ë¯¸í„°
num_epochs = 1000
log_interval = 100

# í›ˆë ¨ íˆìŠ¤í† ë¦¬
history = {'loss': [], 'loss_pde': [], 'loss_bc': []}

# í›ˆë ¨ ì‹œì‘
for epoch in tqdm(range(num_epochs), desc="Adam ì‚¬ì „í•™ìŠµ"):
    # Forward pass
    loss, metrics = loss_function(model, x_collocation, x_bc, u_bc)
    
    # Backward pass
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    # ë¡œê¹…
    history['loss'].append(metrics['loss_total'])
    history['loss_pde'].append(metrics['loss_pde'])
    history['loss_bc'].append(metrics['loss_bc'])
    
    # ì¶œë ¥
    if (epoch + 1) % log_interval == 0:
        print(f"Epoch {epoch+1}/{num_epochs}")
        print(f"  Loss: {metrics['loss_total']:.6f}")
        print(f"  PDE: {metrics['loss_pde']:.6f}, BC: {metrics['loss_bc']:.6f}")

print("âœ… Adam ì‚¬ì „í•™ìŠµ ì™„ë£Œ!")
```

### í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§

í•™ìŠµë¥ ì„ ì ì§„ì ìœ¼ë¡œ ê°ì†Œì‹œì¼œ ì•ˆì •ì ì¸ ìˆ˜ë ´ì„ ìœ ë„í•©ë‹ˆë‹¤.

```python
from torch.optim.lr_scheduler import ExponentialLR

# ìŠ¤ì¼€ì¤„ëŸ¬ ìƒì„±
scheduler = ExponentialLR(
    optimizer,
    gamma=0.995  # ë§¤ ì—í¬í¬ë§ˆë‹¤ í•™ìŠµë¥  Ã— 0.995
)

# í›ˆë ¨ ë£¨í”„ì—ì„œ
for epoch in range(num_epochs):
    # ... í›ˆë ¨ ì½”ë“œ ...
    
    # í•™ìŠµë¥  ì—…ë°ì´íŠ¸
    scheduler.step()
    
    if (epoch + 1) % log_interval == 0:
        current_lr = optimizer.param_groups[0]['lr']
        print(f"  í•™ìŠµë¥ : {current_lr:.2e}")
```

---

## ğŸ¯ 2ë‹¨ê³„: L-BFGS ë¯¸ì„¸ì¡°ì •

### L-BFGS ìµœì í™”ê¸° ìƒì„±

```python
from torch.optim import LBFGS

# L-BFGS ìµœì í™”ê¸°
optimizer_lbfgs = LBFGS(
    model.parameters(),
    lr=1.0,                  # L-BFGSëŠ” ë³´í†µ 1.0 ì‚¬ìš©
    max_iter=20,             # ë¼ì¸ ì„œì¹˜ ìµœëŒ€ ë°˜ë³µ
    history_size=50,         # íˆìŠ¤í† ë¦¬ ì €ì¥ í¬ê¸°
    line_search_fn='strong_wolfe'  # ë¼ì¸ ì„œì¹˜ ë°©ë²•
)

print(f"L-BFGS ìµœì í™”ê¸° ì¤€ë¹„ ì™„ë£Œ")
```

### L-BFGS í›ˆë ¨ ë£¨í”„

L-BFGSëŠ” **closure í•¨ìˆ˜**ë¥¼ ìš”êµ¬í•©ë‹ˆë‹¤!

```python
def closure():
    """L-BFGSë¥¼ ìœ„í•œ closure í•¨ìˆ˜"""
    optimizer_lbfgs.zero_grad()
    loss, metrics = loss_function(model, x_collocation, x_bc, u_bc)
    loss.backward()
    return loss

# L-BFGS ë¯¸ì„¸ì¡°ì •
num_lbfgs_steps = 100

for step in tqdm(range(num_lbfgs_steps), desc="L-BFGS ë¯¸ì„¸ì¡°ì •"):
    loss = optimizer_lbfgs.step(closure)
    
    if (step + 1) % 10 == 0:
        with torch.no_grad():
            _, metrics = loss_function(model, x_collocation, x_bc, u_bc)
        print(f"Step {step+1}/{num_lbfgs_steps}: Loss = {metrics['loss_total']:.8f}")

print("âœ… L-BFGS ë¯¸ì„¸ì¡°ì • ì™„ë£Œ!")
```

---

## ğŸ“Š Trainer í´ë˜ìŠ¤ ì‚¬ìš©í•˜ê¸°

í”„ë¡œì íŠ¸ì—ëŠ” `src/train.py`ì— `Trainer` í´ë˜ìŠ¤ê°€ êµ¬í˜„ë˜ì–´ ìˆìŠµë‹ˆë‹¤!

### ê¸°ë³¸ ì‚¬ìš©ë²•

```python
from src.train import Trainer
from src.models import Scaled_cPIKAN
from src.loss import PhysicsInformedLoss
import torch

# 1. ëª¨ë¸ ìƒì„±
model = Scaled_cPIKAN(
    layers_hidden=[1, 30, 30, 1],
    degree=4,
    domain_min=torch.tensor([0.0]),
    domain_max=torch.tensor([1.0])
)

# 2. ì†ì‹¤ í•¨ìˆ˜ ìƒì„±
loss_fn = PhysicsInformedLoss(
    pde_fn=my_pde_residual,
    boundary_weight=10.0
)

# 3. ë°ì´í„° ì¤€ë¹„
train_data = {
    'x_pde': x_collocation,
    'x_bc': x_boundary,
    'u_bc': u_boundary
}

# 4. Trainer ìƒì„±
trainer = Trainer(
    model=model,
    loss_fn=loss_fn,
    device='cuda' if torch.cuda.is_available() else 'cpu'
)

# 5. í›ˆë ¨ ì‹¤í–‰
history = trainer.train(
    train_data=train_data,
    num_epochs_adam=1000,      # Adam ì—í¬í¬
    num_steps_lbfgs=100,       # L-BFGS ìŠ¤í…
    lr_adam=1e-3,              # Adam í•™ìŠµë¥ 
    lr_lbfgs=1.0,              # L-BFGS í•™ìŠµë¥ 
    log_interval=100           # ë¡œê·¸ ì¶œë ¥ ê°„ê²©
)

print("âœ… í›ˆë ¨ ì™„ë£Œ!")
```

### í›ˆë ¨ íˆìŠ¤í† ë¦¬ í™•ì¸

```python
import matplotlib.pyplot as plt

# ì†ì‹¤ ê³¡ì„  ì‹œê°í™”
plt.figure(figsize=(12, 4))

# ì´ ì†ì‹¤
plt.subplot(1, 3, 1)
plt.plot(history['loss'])
plt.xlabel('Epoch')
plt.ylabel('Total Loss')
plt.title('í›ˆë ¨ ì†ì‹¤')
plt.yscale('log')
plt.grid(True, alpha=0.3)

# PDE ì†ì‹¤
plt.subplot(1, 3, 2)
plt.plot(history['loss_pde'])
plt.xlabel('Epoch')
plt.ylabel('PDE Loss')
plt.title('PDE ì”ì°¨ ì†ì‹¤')
plt.yscale('log')
plt.grid(True, alpha=0.3)

# ê²½ê³„ ì¡°ê±´ ì†ì‹¤
plt.subplot(1, 3, 3)
plt.plot(history['loss_bc'])
plt.xlabel('Epoch')
plt.ylabel('BC Loss')
plt.title('ê²½ê³„ ì¡°ê±´ ì†ì‹¤')
plt.yscale('log')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

---

## â¹ï¸ ì¡°ê¸° ì¢…ë£Œ (Early Stopping)

### ê°œë…

ì†ì‹¤ì´ ë” ì´ìƒ ê°œì„ ë˜ì§€ ì•Šìœ¼ë©´ í›ˆë ¨ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.

**ì´ìœ :**
- ì‹œê°„ ì ˆì•½
- ê³¼ì í•© ë°©ì§€

### êµ¬í˜„

```python
class EarlyStopping:
    """ì¡°ê¸° ì¢…ë£Œ í—¬í¼ í´ë˜ìŠ¤"""
    
    def __init__(self, patience=50, min_delta=1e-6):
        """
        Args:
            patience: ê°œì„  ì—†ì´ ê¸°ë‹¤ë¦´ ì—í¬í¬ ìˆ˜
            min_delta: ê°œì„ ìœ¼ë¡œ ì¸ì •í•  ìµœì†Œ ë³€í™”ëŸ‰
        """
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.best_loss = float('inf')
        self.early_stop = False
    
    def __call__(self, loss):
        if loss < self.best_loss - self.min_delta:
            # ê°œì„ ë¨
            self.best_loss = loss
            self.counter = 0
        else:
            # ê°œì„  ì•ˆ ë¨
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True
        
        return self.early_stop

# ì‚¬ìš© ì˜ˆì œ
early_stopping = EarlyStopping(patience=50, min_delta=1e-6)

for epoch in range(num_epochs):
    # í›ˆë ¨...
    loss, metrics = loss_function(model, x_collocation, x_bc, u_bc)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    # ì¡°ê¸° ì¢…ë£Œ ì²´í¬
    if early_stopping(metrics['loss_total']):
        print(f"ì¡°ê¸° ì¢…ë£Œ: Epoch {epoch+1}")
        break
```

---

## ğŸ–¥ï¸ GPU ê°€ì†

### GPUë¡œ ëª¨ë¸ ë° ë°ì´í„° ì´ë™

```python
# ë””ë°”ì´ìŠ¤ ì„¤ì •
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")

# ëª¨ë¸ì„ GPUë¡œ
model = model.to(device)

# ë°ì´í„°ë¥¼ GPUë¡œ
x_collocation = x_collocation.to(device)
x_bc = x_bc.to(device)
u_bc = u_bc.to(device)

print("âœ… GPU ì„¤ì • ì™„ë£Œ")
```

### ë©”ëª¨ë¦¬ ê´€ë¦¬

```python
# GPU ë©”ëª¨ë¦¬ í™•ì¸
if torch.cuda.is_available():
    print(f"GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {torch.cuda.memory_allocated() / 1e9:.2f} GB")
    print(f"GPU ë©”ëª¨ë¦¬ ì˜ˆì•½ëŸ‰: {torch.cuda.memory_reserved() / 1e9:.2f} GB")

# ë©”ëª¨ë¦¬ ì •ë¦¬
torch.cuda.empty_cache()
```

---

## ğŸ“ˆ í›ˆë ¨ ëª¨ë‹ˆí„°ë§

### TensorBoard ì‚¬ìš© (ì„ íƒì‚¬í•­)

```python
from torch.utils.tensorboard import SummaryWriter

# Writer ìƒì„±
writer = SummaryWriter('runs/pinn_experiment')

# í›ˆë ¨ ë£¨í”„ì—ì„œ
for epoch in range(num_epochs):
    loss, metrics = loss_function(model, x_collocation, x_bc, u_bc)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    # TensorBoard ë¡œê¹…
    writer.add_scalar('Loss/total', metrics['loss_total'], epoch)
    writer.add_scalar('Loss/pde', metrics['loss_pde'], epoch)
    writer.add_scalar('Loss/bc', metrics['loss_bc'], epoch)

writer.close()

# TensorBoard ì‹¤í–‰: tensorboard --logdir=runs
```

### ì‹¤ì‹œê°„ í”Œë¡¯ (tqdm + matplotlib)

```python
from tqdm import tqdm
import matplotlib.pyplot as plt
from IPython.display import clear_output

# ì‹¤ì‹œê°„ í”Œë¡¯ í•¨ìˆ˜
def plot_realtime(history):
    clear_output(wait=True)
    plt.figure(figsize=(10, 4))
    
    plt.subplot(1, 2, 1)
    plt.plot(history['loss'])
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Total Loss')
    plt.yscale('log')
    plt.grid(True)
    
    plt.subplot(1, 2, 2)
    plt.plot(history['loss_pde'], label='PDE')
    plt.plot(history['loss_bc'], label='BC')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Component Losses')
    plt.yscale('log')
    plt.legend()
    plt.grid(True)
    
    plt.tight_layout()
    plt.show()

# í›ˆë ¨ ë£¨í”„ì—ì„œ (Jupyter Notebook)
for epoch in tqdm(range(num_epochs)):
    # í›ˆë ¨...
    
    if (epoch + 1) % 50 == 0:
        plot_realtime(history)
```

---

## ğŸ§ª ì‹¤ìŠµ: ì™„ì „í•œ í›ˆë ¨ íŒŒì´í”„ë¼ì¸

### ëª©í‘œ

1D Helmholtz ë°©ì •ì‹ì„ ì²˜ìŒë¶€í„° ëê¹Œì§€ í›ˆë ¨í•˜ê¸°

### ì „ì²´ ì½”ë“œ

```python
import torch
from torch.optim import Adam, LBFGS
from torch.optim.lr_scheduler import ExponentialLR
from src.models import Scaled_cPIKAN
from tqdm import tqdm
import matplotlib.pyplot as plt

# ============================================
# 1. ë°ì´í„° ì¤€ë¹„
# ============================================
print("1. ë°ì´í„° ì¤€ë¹„ ì¤‘...")
x_collocation = torch.linspace(0.01, 0.99, 200).unsqueeze(1).requires_grad_(True)
x_bc = torch.tensor([[0.0], [1.0]])
u_bc = torch.zeros(2, 1)

# ============================================
# 2. ëª¨ë¸ ìƒì„±
# ============================================
print("2. ëª¨ë¸ ìƒì„± ì¤‘...")
model = Scaled_cPIKAN(
    layers_hidden=[1, 30, 30, 1],
    degree=4,
    domain_min=torch.tensor([0.0]),
    domain_max=torch.tensor([1.0])
)

# GPU ì‚¬ìš© (ê°€ëŠ¥í•œ ê²½ìš°)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)
x_collocation = x_collocation.to(device)
x_bc = x_bc.to(device)
u_bc = u_bc.to(device)

print(f"ë””ë°”ì´ìŠ¤: {device}")
print(f"íŒŒë¼ë¯¸í„° ê°œìˆ˜: {sum(p.numel() for p in model.parameters())}")

# ============================================
# 3. ì†ì‹¤ í•¨ìˆ˜ ì •ì˜
# ============================================
def helmholtz_loss(model, x_col, x_bc, u_bc, k=1.0, lambda_bc=10.0):
    # PDE ì”ì°¨
    x_col_grad = x_col.requires_grad_(True)
    u = model(x_col_grad)
    
    du_dx = torch.autograd.grad(u, x_col_grad, torch.ones_like(u), create_graph=True)[0]
    d2u_dx2 = torch.autograd.grad(du_dx, x_col_grad, torch.ones_like(du_dx), create_graph=True)[0]
    
    residual = d2u_dx2 + k**2 * u
    loss_pde = torch.mean(residual ** 2)
    
    # ê²½ê³„ ì¡°ê±´
    u_bc_pred = model(x_bc)
    loss_bc = torch.mean((u_bc_pred - u_bc) ** 2)
    
    # ì´ ì†ì‹¤
    loss_total = loss_pde + lambda_bc * loss_bc
    
    return loss_total, {'loss_total': loss_total.item(), 'loss_pde': loss_pde.item(), 'loss_bc': loss_bc.item()}

# ============================================
# 4. Adam ì‚¬ì „í•™ìŠµ
# ============================================
print("\n3. Adam ì‚¬ì „í•™ìŠµ ì‹œì‘...")
optimizer_adam = Adam(model.parameters(), lr=1e-3)
scheduler = ExponentialLR(optimizer_adam, gamma=0.995)

history = {'loss': [], 'loss_pde': [], 'loss_bc': []}
num_epochs_adam = 2000

for epoch in tqdm(range(num_epochs_adam), desc="Adam"):
    loss, metrics = helmholtz_loss(model, x_collocation, x_bc, u_bc)
    
    optimizer_adam.zero_grad()
    loss.backward()
    optimizer_adam.step()
    scheduler.step()
    
    history['loss'].append(metrics['loss_total'])
    history['loss_pde'].append(metrics['loss_pde'])
    history['loss_bc'].append(metrics['loss_bc'])
    
    if (epoch + 1) % 500 == 0:
        print(f"\nEpoch {epoch+1}: Loss = {metrics['loss_total']:.6f}")

print("âœ… Adam ì‚¬ì „í•™ìŠµ ì™„ë£Œ!")

# ============================================
# 5. L-BFGS ë¯¸ì„¸ì¡°ì •
# ============================================
print("\n4. L-BFGS ë¯¸ì„¸ì¡°ì • ì‹œì‘...")
optimizer_lbfgs = LBFGS(model.parameters(), lr=1.0, max_iter=20)

def closure():
    optimizer_lbfgs.zero_grad()
    loss, _ = helmholtz_loss(model, x_collocation, x_bc, u_bc)
    loss.backward()
    return loss

num_steps_lbfgs = 50
for step in tqdm(range(num_steps_lbfgs), desc="L-BFGS"):
    optimizer_lbfgs.step(closure)
    
    if (step + 1) % 10 == 0:
        with torch.no_grad():
            _, metrics = helmholtz_loss(model, x_collocation, x_bc, u_bc)
        print(f"\nStep {step+1}: Loss = {metrics['loss_total']:.8f}")
        history['loss'].append(metrics['loss_total'])

print("âœ… L-BFGS ë¯¸ì„¸ì¡°ì • ì™„ë£Œ!")

# ============================================
# 6. ê²°ê³¼ ì‹œê°í™”
# ============================================
print("\n5. ê²°ê³¼ ì‹œê°í™” ì¤‘...")
plt.figure(figsize=(12, 4))

# ì†ì‹¤ ê³¡ì„ 
plt.subplot(1, 2, 1)
plt.plot(history['loss'])
plt.xlabel('Iteration')
plt.ylabel('Loss')
plt.title('Training Loss')
plt.yscale('log')
plt.grid(True, alpha=0.3)
plt.axvline(num_epochs_adam, color='r', linestyle='--', label='Adam â†’ L-BFGS')
plt.legend()

# ì˜ˆì¸¡ ê²°ê³¼
plt.subplot(1, 2, 2)
x_test = torch.linspace(0, 1, 200).unsqueeze(1).to(device)
with torch.no_grad():
    u_pred = model(x_test).cpu().numpy()
    u_exact = torch.sin(torch.pi * x_test.cpu()).numpy()  # ì •í™•í•œ í•´ (ì˜ˆì‹œ)

plt.plot(x_test.cpu().numpy(), u_pred, label='PINN ì˜ˆì¸¡', linewidth=2)
plt.plot(x_test.cpu().numpy(), u_exact, '--', label='ì •í™•í•œ í•´', linewidth=2)
plt.xlabel('x')
plt.ylabel('u(x)')
plt.title('Solution')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('training_result.png', dpi=150)
plt.show()

print("âœ… ì „ì²´ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
```

---

## ğŸ¯ ì²´í¬í¬ì¸íŠ¸

ë‹¤ìŒ í•­ëª©ì„ ëª¨ë‘ ì™„ë£Œí–ˆìœ¼ë©´ ë‹¤ìŒ ë‹¨ê³„ë¡œ ì§„í–‰í•˜ì„¸ìš”!

- [ ] Adam ìµœì í™”ê¸° ì‚¬ìš©ë²• ì´í•´
- [ ] L-BFGS ìµœì í™”ê¸° ì‚¬ìš©ë²• ì´í•´
- [ ] 2ë‹¨ê³„ í›ˆë ¨ ì „ëµ ì‹¤í–‰ ì„±ê³µ
- [ ] í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§ ì ìš©
- [ ] í›ˆë ¨ ì†ì‹¤ ì‹œê°í™”
- [ ] GPU ê°€ì† í™œìš© (ê°€ëŠ¥í•œ ê²½ìš°)

---

## ğŸ†˜ ìì£¼ ë¬»ëŠ” ì§ˆë¬¸

### Q1: Adamê³¼ L-BFGS ì¤‘ ì–´ë–¤ ê²ƒì´ ë” ì¢‹ë‚˜ìš”?

**A:** ë‘˜ ë‹¤ ì‚¬ìš©í•˜ì„¸ìš”! Adamìœ¼ë¡œ ë¹ ë¥´ê²Œ ì‹œì‘í•˜ê³  L-BFGSë¡œ ë§ˆë¬´ë¦¬í•˜ëŠ” ê²ƒì´ best practiceì…ë‹ˆë‹¤.

### Q2: ì†ì‹¤ì´ ì¤„ì–´ë“¤ì§€ ì•Šì•„ìš”

**A:** ë‹¤ìŒì„ ì‹œë„í•˜ì„¸ìš”:
1. í•™ìŠµë¥  ê°ì†Œ (ì˜ˆ: 1e-3 â†’ 1e-4)
2. ë” ë§ì€ ì—í¬í¬
3. ê²½ê³„ ì¡°ê±´ ê°€ì¤‘ì¹˜ ì¦ê°€
4. ëª¨ë¸ í¬ê¸° ì¦ê°€

### Q3: "CUDA out of memory" ì—ëŸ¬ê°€ ë‚˜ìš”

**A:** í•´ê²° ë°©ë²•:
1. ë°°ì¹˜ í¬ê¸° ê°ì†Œ (ì½œë¡œì¼€ì´ì…˜ í¬ì¸íŠ¸ ìˆ˜)
2. ëª¨ë¸ í¬ê¸° ê°ì†Œ (ì€ë‹‰ì¸µ ë‰´ëŸ° ìˆ˜)
3. `torch.cuda.empty_cache()` í˜¸ì¶œ

### Q4: í›ˆë ¨ì´ ë„ˆë¬´ ëŠë ¤ìš”

**A:** ìµœì í™” ë°©ë²•:
1. GPU ì‚¬ìš©
2. ì½œë¡œì¼€ì´ì…˜ í¬ì¸íŠ¸ ìˆ˜ ê°ì†Œ
3. Chebyshev ì°¨ìˆ˜ ê°ì†Œ
4. Mixed precision training ì‚¬ìš©

---

## ğŸ“š ì¶”ê°€ ìë£Œ

- **Adam ë…¼ë¬¸:** Kingma & Ba, "Adam: A Method for Stochastic Optimization" (2014)
- **L-BFGS ë…¼ë¬¸:** Liu & Nocedal, "On the limited memory BFGS method" (1989)
- **PyTorch ìµœì í™”ê¸° ë¬¸ì„œ:** [https://pytorch.org/docs/stable/optim.html](https://pytorch.org/docs/stable/optim.html)

---

## ğŸ‰ ì¶•í•˜í•©ë‹ˆë‹¤!

í›ˆë ¨ ê³¼ì •ì„ ë§ˆìŠ¤í„°í–ˆìŠµë‹ˆë‹¤! ì´ì œ ê²°ê³¼ë¥¼ ë¶„ì„í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤.

**ë‹¤ìŒ ë‹¨ê³„:**

ğŸ‘‰ [6ë‹¨ê³„: ê²°ê³¼ ë¶„ì„ìœ¼ë¡œ ì´ë™í•˜ê¸°](06_ê²°ê³¼ë¶„ì„.md)

ë˜ëŠ” [ë©”ì¸ ê°€ì´ë“œë¡œ ëŒì•„ê°€ê¸°](../ì‚¬ìš©ì_ê°€ì´ë“œ.md)

---

*ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸: 2025ë…„ 1ì›”*
