# PDE 잔차 손실 - 완전 초보자 가이드 📚

> **대상:** PDE, 잔차, 손실 등의 용어가 생소한 초보자  
> **소요 시간:** 30분  
> **목표:** 각 단어의 뜻부터 실제 코드까지 완전 이해

---

## 🎯 이 문서를 읽어야 하는 이유

"PDE 잔차 손실"이라는 용어를 처음 들으면 막막하죠? 걱정하지 마세요!

이 문서는:
- ✅ 각 단어를 하나씩 쉽게 풀어서 설명
- ✅ 실생활 비유로 직관적 이해
- ✅ 수학 공식을 천천히 설명
- ✅ 직접 실행 가능한 코드 제공

**이 문서를 다 읽으면:**  
PINN이 어떻게 작동하는지 핵심을 이해하게 됩니다!

---

## 📖 Part 1: 단어 하나하나 분해하기

### 🔤 1. PDE (Partial Differential Equation)

**한국어:** 편미분 방정식

#### 단어 분해

**방정식 (Equation):**
```
초등학교 수학 문제:
x + 2 = 5
"x는 뭘까?" → x = 3

방정식 = 답을 찾아야 하는 문제
```

**미분 (Differential):**
```
"변화율"을 의미합니다.

예시:
- 속도 = 거리의 변화율
- 온도 변화 = 시간에 따른 온도의 변화율

자동차가 10m/s로 달린다
→ 시간이 1초 지나면 거리가 10m 변한다
→ 거리를 시간으로 미분하면 속도!
```

**편미분 (Partial):**
```
변수가 여러 개일 때, 한 변수만 변화시켜 보는 것

예시: 방 안의 온도 T(x, y, t)
- x, y: 위치 좌표
- t: 시간

질문들:
∂T/∂t = "시간이 지나면 온도가 얼마나 변할까?" (위치 고정)
∂T/∂x = "오른쪽으로 가면 온도가 얼마나 변할까?" (시간 고정)
```

#### PDE 예시: 열 방정식

```
∂T/∂t = α (∂²T/∂x² + ∂²T/∂y²)

읽는 법:
"온도가 시간에 따라 변하는 속도는
 온도가 공간적으로 얼마나 고르지 않은지에 비례한다"

실생활 의미:
뜨거운 커피를 방치하면? → 주변과 온도가 같아진다
(온도 차이가 큰 곳에서 열이 빠르게 이동)
```

---

### 🎯 2. 잔차 (Residual)

**한국어:** 남은 것, 나머지, 오차

#### 쉬운 설명

**"정답과 내 답의 차이"**

#### 일상 비유

```
🎯 과녁 던지기:
정답(정중앙): (0, 0)
내가 던진 곳: (2, 3)

잔차 = 내 위치 - 정답 위치
     = (2, 3) - (0, 0)
     = (2, 3)

크기 = √(2² + 3²) = √13 ≈ 3.6

→ 정답에서 3.6만큼 떨어져 있음!
```

#### 수학 문제에서

```
문제: x² = 4를 만족하는 x를 찾아라

내 답: x = 1.9

검증:
왼쪽 = 1.9² = 3.61
오른쪽 = 4
잔차 = 3.61 - 4 = -0.39

→ 답이 완벽하지 않음! (0.39만큼 오차)
```

#### PDE에서 잔차

```
방정식: d²u/dx² = 0  (이렇게 되어야 함)

내 예측 함수: u(x) = 0.1x² + 2x

검증:
du/dx = 0.2x + 2
d²u/dx² = 0.2  (실제 계산 결과)

잔차 R = 0.2 - 0 = 0.2

해석:
→ 방정식을 0.2만큼 만족하지 못함
→ 완벽한 해가 아님!
```

---

### 📉 3. 손실 (Loss)

**한국어:** 손실 함수, 비용 함수

#### 쉬운 설명

**"얼마나 틀렸는지를 숫자 하나로 표현"**

#### 게임 비유

```
🎮 게임 점수:
완벽한 플레이: 손실 = 0점
실수 1번: 손실 = 10점
실수 3번: 손실 = 50점

목표: 손실을 최소화!
```

#### 왜 필요한가?

```
잔차만으로는 부족:
- 지점 A: 잔차 = 0.1
- 지점 B: 잔차 = -0.2
- 지점 C: 잔차 = 0.05

"전체적으로 얼마나 틀렸는가?"를 한 숫자로!

손실 = 0.1² + (-0.2)² + 0.05²
     = 0.01 + 0.04 + 0.0025
     = 0.0525
```

#### 수학적 정의

```
N개 지점에서 잔차를 측정했다면:

Loss = (1/N) Σ Rᵢ²

설명:
- Rᵢ: i번째 지점의 잔차
- ²: 제곱 (음수 → 양수, 큰 오차 강조)
- Σ: 모든 지점 합산
- 1/N: 평균 (점수 정규화)
```

---

## 🍰 Part 2: 전체 개념을 비유로 이해하기

### 케이크 레시피 비유

완벽한 케이크를 만들어봅시다!

#### 📝 레시피 (PDE)

```
완벽한 레시피:
밀가루 300g + 설탕 200g + 달걀 3개 = 맛있는 케이크
```

#### 👨‍🍳 첫 시도

```
내가 넣은 재료:
- 밀가루: 310g
- 설탕: 180g
- 달걀: 3개
```

#### 📊 잔차 계산 (Residual)

```
각 재료별 오차:
- 밀가루 잔차: 310 - 300 = +10g (너무 많음!)
- 설탕 잔차: 180 - 200 = -20g (부족!)
- 달걀 잔차: 3 - 3 = 0 (완벽!)
```

#### ❌ 손실 계산 (Loss)

```
총 손실 = 10² + (-20)² + 0²
        = 100 + 400 + 0
        = 500

→ 숫자가 클수록 레시피를 못 따른 것!
```

#### 🎯 훈련 과정

```
시도 1: 손실 = 500  (많이 틀림)
시도 2: 손실 = 150  (개선됨!)
시도 3: 손실 = 50   (더 좋아짐!)
시도 10: 손실 = 5   (거의 완벽!)

→ 점점 레시피에 가까워진다!
```

---

## 🧮 Part 3: 수학적으로 이해하기

### 예제: 1D 열 방정식

**문제 설정:**
```
PDE: d²u/dx² = 0

물리적 의미:
"온도가 직선으로 변한다"
(가속도 없음 = 등속 = 직선)
```

### 단계별 계산

#### 1단계: 신경망 예측

```
우리가 만든 함수:
u(x) = sin(πx)

이 함수가 방정식을 만족하는지 확인하자!
```

#### 2단계: 미분 계산

```
1차 미분:
du/dx = π·cos(πx)

2차 미분:
d²u/dx² = -π²·sin(πx)
```

#### 3단계: 잔차 계산

```
원래 방정식: d²u/dx² = 0  (이래야 함)
실제 계산: d²u/dx² = -π²·sin(πx)

잔차:
R(x) = -π²·sin(πx) - 0
     = -π²·sin(πx)

예시 지점:
x = 0.5일 때:
R(0.5) = -π²·sin(π·0.5)
       = -π²·1
       ≈ -9.87

→ 이 지점에서 방정식을 9.87만큼 만족 못함!
```

#### 4단계: 손실 계산

```
100개 지점에서 측정했다고 가정:
x₁ = 0.01, x₂ = 0.02, ..., x₁₀₀ = 1.0

각 지점의 잔차:
R(x₁), R(x₂), ..., R(x₁₀₀)

손실:
L = (1/100) Σᵢ R(xᵢ)²
  = (1/100) Σᵢ [-π²·sin(πxᵢ)]²

수치 계산:
L ≈ 48.7

→ 손실이 크다 = 이 함수는 답이 아니다!
```

---

## 💻 Part 4: 코드로 이해하기

### 완전 초보자용 예제

```python
import torch
import matplotlib.pyplot as plt

print("=" * 50)
print("PDE 잔차 손실 계산 예제")
print("=" * 50)

# ============================================
# 설정: PDE 정의
# ============================================
# 우리가 풀 방정식: d²u/dx² = 0
print("\n📝 PDE: d²u/dx² = 0")
print("   (온도가 직선으로 변해야 함)")

# ============================================
# 1. 신경망 예측 (가짜 신경망)
# ============================================
def my_network(x):
    """
    실제론 복잡한 신경망이지만,
    지금은 간단한 함수로 대체
    """
    return torch.sin(torch.pi * x)

print("\n🤖 신경망 예측: u(x) = sin(πx)")

# ============================================
# 2. 잔차 계산 함수
# ============================================
def calculate_residual(x):
    """
    PDE를 얼마나 만족하는지 계산
    
    Args:
        x: 확인할 위치들
    
    Returns:
        residual: 각 위치의 잔차
    """
    # 미분을 계산하려면 requires_grad 필요
    x = x.requires_grad_(True)
    
    # 신경망 예측
    u = my_network(x)
    
    print("\n📊 미분 계산 중...")
    
    # 1차 미분: du/dx
    du_dx = torch.autograd.grad(
        outputs=u,
        inputs=x,
        grad_outputs=torch.ones_like(u),
        create_graph=True
    )[0]
    print("   ✓ 1차 미분 완료")
    
    # 2차 미분: d²u/dx²
    d2u_dx2 = torch.autograd.grad(
        outputs=du_dx,
        inputs=x,
        grad_outputs=torch.ones_like(du_dx),
        create_graph=True
    )[0]
    print("   ✓ 2차 미분 완료")
    
    # 잔차 = 실제값 - 목표값
    # PDE가 "d²u/dx² = 0"이므로
    residual = d2u_dx2 - 0
    
    return residual

# ============================================
# 3. 손실 함수
# ============================================
def compute_loss(residuals):
    """
    잔차들을 손실 하나로 합침
    
    Args:
        residuals: 각 지점의 잔차들
    
    Returns:
        loss: 총 손실 (작을수록 좋음)
    """
    # 제곱 평균
    loss = torch.mean(residuals ** 2)
    return loss

# ============================================
# 4. 실행
# ============================================
print("\n🔬 테스트 시작!")

# 100개 지점에서 테스트
x_test = torch.linspace(0, 1, 100)
print(f"   테스트 포인트: {len(x_test)}개")

# 잔차 계산
residuals = calculate_residual(x_test)

# 손실 계산
loss = compute_loss(residuals)

# ============================================
# 5. 결과 출력
# ============================================
print("\n" + "=" * 50)
print("📈 결과 분석")
print("=" * 50)

print(f"\n잔차 통계:")
print(f"  평균: {residuals.mean().item():+.6f}")
print(f"  최댓값: {residuals.max().item():+.6f}")
print(f"  최솟값: {residuals.min().item():+.6f}")

print(f"\n총 손실: {loss.item():.6f}")

# 평가
if loss.item() < 0.01:
    grade = "✓ 우수! 거의 완벽합니다!"
elif loss.item() < 0.1:
    grade = "○ 양호! 개선 여지 있음"
else:
    grade = "✗ 불량! 더 훈련 필요"

print(f"평가: {grade}")

# ============================================
# 6. 시각화
# ============================================
print("\n📊 그래프 생성 중...")

fig, axes = plt.subplots(1, 3, figsize=(15, 4))

# 그래프 1: 예측 함수
x_plot = torch.linspace(0, 1, 200)
u_plot = my_network(x_plot)
axes[0].plot(x_plot.numpy(), u_plot.detach().numpy(), 'b-', linewidth=2)
axes[0].set_xlabel('x', fontsize=12)
axes[0].set_ylabel('u(x)', fontsize=12)
axes[0].set_title('신경망 예측', fontsize=14, fontweight='bold')
axes[0].grid(True, alpha=0.3)

# 그래프 2: 잔차 분포
axes[1].plot(x_test.numpy(), residuals.detach().numpy(), 'r-', linewidth=2)
axes[1].axhline(0, color='black', linestyle='--', alpha=0.5, label='목표 (0)')
axes[1].set_xlabel('x', fontsize=12)
axes[1].set_ylabel('Residual', fontsize=12)
axes[1].set_title('잔차 (0이 완벽)', fontsize=14, fontweight='bold')
axes[1].legend()
axes[1].grid(True, alpha=0.3)

# 그래프 3: 손실 요약
axes[2].text(0.5, 0.6, f'총 손실\n\n{loss.item():.6f}', 
             ha='center', va='center', fontsize=20, fontweight='bold',
             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
axes[2].text(0.5, 0.3, grade, ha='center', va='center', fontsize=14)
axes[2].set_xlim(0, 1)
axes[2].set_ylim(0, 1)
axes[2].axis('off')

plt.tight_layout()
plt.savefig('pde_residual_loss_example.png', dpi=150, bbox_inches='tight')
print("   ✓ 저장 완료: pde_residual_loss_example.png")
plt.show()

print("\n" + "=" * 50)
print("✅ 실습 완료!")
print("=" * 50)
```

### 실행 결과 예시

```
==================================================
PDE 잔차 손실 계산 예제
==================================================

📝 PDE: d²u/dx² = 0
   (온도가 직선으로 변해야 함)

🤖 신경망 예측: u(x) = sin(πx)

🔬 테스트 시작!
   테스트 포인트: 100개

📊 미분 계산 중...
   ✓ 1차 미분 완료
   ✓ 2차 미분 완료

==================================================
📈 결과 분석
==================================================

잔차 통계:
  평균: -0.000000
  최댓값: +0.097484
  최솟값: -9.869604

총 손실: 48.705921
평가: ✗ 불량! 더 훈련 필요

📊 그래프 생성 중...
   ✓ 저장 완료: pde_residual_loss_example.png

==================================================
✅ 실습 완료!
==================================================
```

---

## 📊 Part 5: 그림으로 이해하기

### 시각적 표현

```
문제: d²u/dx² = 0을 만족하는 u 찾기

┌─────────────────────────────────────┐
│ 정답 함수 (직선)                      │
│                                     │
│     u = 2x + 1                     │
│                                     │
│      |                              │
│      |        /                     │
│      |       /                      │
│      |      /                       │
│      |     /                        │
│      |    /                         │
│      |___/____________              │
│                                     │
│  → 2차 미분 = 0 ✓                   │
└─────────────────────────────────────┘

┌─────────────────────────────────────┐
│ 우리 예측 (곡선)                      │
│                                     │
│     u = sin(πx)                    │
│                                     │
│      |   /\                         │
│      |  /  \                        │
│      | /    \                       │
│      |/      \____                  │
│      |                              │
│      |____________                  │
│                                     │
│  → 2차 미분 ≠ 0 ✗                   │
└─────────────────────────────────────┘

┌─────────────────────────────────────┐
│ 잔차 분포                            │
│                                     │
│  각 점에서 얼마나 틀렸는지             │
│                                     │
│      |     ↑ 여기 큰 오차            │
│      |    ↑↓                        │
│      |   ↓  ↑                       │
│      |  ↓    ↑                      │
│      | ↓      ↓                     │
│      |____________                  │
│         x 좌표                       │
│                                     │
│  빨간색 = 큰 잔차 (문제!)             │
│  파란색 = 작은 잔차 (좋음!)           │
└─────────────────────────────────────┘

┌─────────────────────────────────────┐
│ 손실 (Loss)                         │
│                                     │
│  모든 잔차를 제곱해서 합침             │
│                                     │
│      ┌───────────────┐              │
│      │               │              │
│      │  Loss = 48.7  │              │
│      │               │              │
│      │   ✗ 크다!     │              │
│      │   더 훈련!    │              │
│      │               │              │
│      └───────────────┘              │
│                                     │
│  목표: 이 숫자를 0에 가깝게!          │
└─────────────────────────────────────┘
```

---

## 🎮 Part 6: 실습 문제

### 연습 1: 다른 함수 시도하기

```python
import torch

# 문제: d²u/dx² + u = 0을 만족하는 함수 찾기

def candidate_function(x, choice=1):
    """여러 후보 함수 중 선택"""
    if choice == 1:
        return torch.sin(x)      # sin(x)
    elif choice == 2:
        return torch.cos(x)      # cos(x)
    elif choice == 3:
        return torch.exp(x)      # eˣ
    elif choice == 4:
        return x**2              # x²

def check_pde(x, choice):
    """PDE를 만족하는지 확인"""
    x = x.requires_grad_(True)
    u = candidate_function(x, choice)
    
    # 1차 미분
    du = torch.autograd.grad(u, x, torch.ones_like(u), create_graph=True)[0]
    
    # 2차 미분
    d2u = torch.autograd.grad(du, x, torch.ones_like(du), create_graph=True)[0]
    
    # 잔차: d²u/dx² + u = 0이어야 함
    residual = d2u + u
    loss = torch.mean(residual ** 2)
    
    return loss.item()

# 테스트
x = torch.linspace(0, 6.28, 100)

print("PDE: d²u/dx² + u = 0")
print("-" * 40)
for i in range(1, 5):
    loss = check_pde(x, i)
    if i == 1:
        name = "sin(x)"
    elif i == 2:
        name = "cos(x)"
    elif i == 3:
        name = "exp(x)"
    else:
        name = "x²"
    
    status = "✓ 정답!" if loss < 0.01 else "✗ 오답"
    print(f"함수 {i} ({name:6s}): Loss = {loss:.6f}  {status}")

print("-" * 40)
print("\n힌트: sin(x)와 cos(x)는 이 PDE의 해입니다!")
```

### 연습 2: 손실 최소화 게임

```python
import torch
import matplotlib.pyplot as plt

# 목표: 계수 a, b를 조정해서 손실 최소화
# 함수: u(x) = a*x + b

def linear_function(x, a, b):
    return a * x + b

def compute_loss_for_params(a, b):
    """주어진 a, b에 대한 손실 계산"""
    x = torch.linspace(0, 1, 50, requires_grad=True)
    u = linear_function(x, a, b)
    
    # d²u/dx² = 0이어야 함 (직선이면 자동으로 만족!)
    du = torch.autograd.grad(u, x, torch.ones_like(u), create_graph=True)[0]
    d2u = torch.autograd.grad(du, x, torch.ones_like(du), create_graph=True)[0]
    
    residual = d2u
    pde_loss = torch.mean(residual ** 2)
    
    # 경계 조건: u(0) = 0, u(1) = 1
    bc_loss = (linear_function(torch.tensor(0.0), a, b) - 0.0)**2 + \
              (linear_function(torch.tensor(1.0), a, b) - 1.0)**2
    
    total_loss = pde_loss + bc_loss
    return total_loss.item()

# 여러 파라미터 시도
print("파라미터 최적화 게임!")
print("=" * 50)

params = [
    (0.5, 0.5, "시도 1"),
    (1.0, 0.0, "시도 2"),
    (1.5, -0.5, "시도 3"),
    (1.0, 0.0, "정답!"),
]

for a, b, name in params:
    loss = compute_loss_for_params(a, b)
    print(f"{name:8s} (a={a:.1f}, b={b:.1f}): Loss = {loss:.6f}")

print("=" * 50)
print("\n정답: a=1, b=0 → u(x) = x")
print("이 함수는 직선이면서 u(0)=0, u(1)=1을 만족!")
```

---

## ❓ Part 7: 자주 묻는 질문 (FAQ)

### Q1: 왜 잔차를 제곱(²)하나요?

**A:** 두 가지 이유가 있습니다!

```
이유 1: 음수를 양수로
────────────────────
잔차: +0.5, -0.3, +0.1
제곱: 0.25, 0.09, 0.01

음수든 양수든 오차는 오차!
제곱하면 모두 양수가 됩니다.


이유 2: 큰 오차를 강조
────────────────────
잔차: 0.1, 1.0, 10.0
제곱: 0.01, 1.0, 100.0

큰 오차가 훨씬 더 크게 벌어집니다!
→ 신경망이 큰 오차를 우선 줄이도록 유도
```

### Q2: 잔차가 0이면 뭐가 좋은 건가요?

**A:** 방정식을 **완벽히** 만족한다는 뜻입니다!

```
시험 점수로 비유:

잔차 = 0         → 100점 만점!
잔차 = 0.001     → 99.9점
잔차 = 0.01      → 99점
잔차 = 0.1       → 90점
잔차 = 1.0       → 0점

실제로는 0.001~0.01 정도면
충분히 좋은 결과입니다!
```

### Q3: 왜 여러 지점에서 확인하나요?

**A:** 한 지점만 맞아도 소용없기 때문입니다!

```
비유: 시험 문제 100개

방법 A: 문제 1번만 맞힘
→ 점수 1점 (나머지 99개 틀림)

방법 B: 모든 문제를 확인
→ 전체 점수 계산 가능

PDE도 마찬가지!
한 지점만 맞으면 소용없어요.
전체 도메인에서 확인해야 합니다.
```

### Q4: 손실이 0이 될 수 있나요?

**A:** 이론적으로는 가능하지만, 실제로는 **거의 불가능**합니다!

```
이론 vs 실제:
──────────────
이론: 무한히 복잡한 신경망
      + 무한히 긴 훈련
      = 손실 0 가능!

실제: 제한된 신경망
      + 제한된 시간
      = 손실 0.0001 정도면 대성공!

실용적 목표:
- 손실 < 0.001: 우수
- 손실 < 0.01: 양호
- 손실 < 0.1: 개선 필요
```

### Q5: PDE를 왜 풀어야 하나요?

**A:** 실세계의 거의 모든 물리 현상이 PDE로 표현됩니다!

```
응용 분야:

🌡️ 열 전달
   → 에어컨 설계, 반도체 냉각

💧 유체 역학
   → 비행기 날개, 날씨 예측

⚡ 전자기학
   → 안테나 설계, 무선 통신

🏗️ 구조 역학
   → 건물 설계, 다리 안전성

🧬 생물학
   → 약물 확산, 종양 성장 모델

PDE를 풀 수 있으면
이 모든 문제를 시뮬레이션할 수 있습니다!
```

---

## 🎯 Part 8: 핵심 정리

### 3줄 요약

```
1. PDE 잔차 = "방정식을 얼마나 못 만족하는가"
2. 손실 = "모든 지점의 잔차를 합친 점수"
3. 훈련 = "손실을 0에 가깝게 만드는 과정"
```

### 전체 흐름

```
┌──────────────────────────────────────────┐
│ 1. PDE 주어짐                             │
│    예) d²u/dx² = 0                       │
└──────────┬───────────────────────────────┘
           ↓
┌──────────────────────────────────────────┐
│ 2. 신경망이 함수 예측                      │
│    예) u(x) = sin(x)                     │
└──────────┬───────────────────────────────┘
           ↓
┌──────────────────────────────────────────┐
│ 3. 여러 지점에서 잔차 계산                 │
│    R(x) = d²u/dx² - 0                   │
└──────────┬───────────────────────────────┘
           ↓
┌──────────────────────────────────────────┐
│ 4. 손실 함수로 합침                        │
│    L = (1/N) Σ R(xᵢ)²                   │
└──────────┬───────────────────────────────┘
           ↓
┌──────────────────────────────────────────┐
│ 5. 역전파로 신경망 파라미터 업데이트        │
│    → 손실 감소                            │
└──────────┬───────────────────────────────┘
           ↓
┌──────────────────────────────────────────┐
│ 6. 반복 훈련                              │
│    손실 → 0                              │
└──────────────────────────────────────────┘
```

### 왜 PINN이 혁명적인가?

```
전통적 방법:
───────────────
PDE 풀기 위해 "정답 데이터" 필요
→ 실험, 측정 필요
→ 비용 큼, 시간 오래 걸림


PINN 방법:
───────────────
"방정식"만 알면 됨!
→ 데이터 불필요
→ 저렴하고 빠름

혁신:
물리 법칙을 직접 신경망에 가르침
```

---

## 📚 Part 9: 더 공부하기

### 다음 단계

이 개념을 이해했다면:

1. **[손실 함수 가이드](../04_손실함수.md)** - 실전 구현
2. **[훈련 과정 가이드](../05_훈련과정.md)** - 실제 훈련 방법
3. **[결과 분석 가이드](../06_결과분석.md)** - 결과 해석

### 추가 개념

다른 초보자 가이드:
- Autograd (자동 미분)
- 경계 조건의 이해
- 2D/3D PDE로 확장

### 연습 문제 답

**연습 1 정답:**
```
sin(x): Loss ≈ 0.00  ✓ 정답!
cos(x): Loss ≈ 0.00  ✓ 정답!
exp(x): Loss ≈ 큼     ✗ 오답
x²:     Loss ≈ 큼     ✗ 오답

이유: d²(sin)/dx² = -sin, -sin + sin = 0 ✓
```

**연습 2 정답:**
```
u(x) = x
→ u(0) = 0 ✓
→ u(1) = 1 ✓
→ d²u/dx² = 0 ✓

완벽!
```

---

## 💬 마무리

축하합니다! 🎉

이제 여러분은 **PDE 잔차 손실**의 핵심을 완전히 이해했습니다!

**배운 내용:**
- ✅ PDE, 잔차, 손실의 정확한 의미
- ✅ 실생활 비유로 직관 확보
- ✅ 수학적 계산 방법
- ✅ 실제 코드 구현
- ✅ PINN의 혁신성

**다음 학습:**

이 기초를 바탕으로 실전 구현으로 넘어가세요!

👉 [손실 함수 가이드로 돌아가기](../04_손실함수.md)

---

**질문이나 피드백이 있으신가요?**

이 문서가 도움이 되었다면, 프로젝트에 스타⭐를 부탁드립니다!

**행운을 빕니다! 🚀**

---

*마지막 업데이트: 2025년 1월*
