# 7ë‹¨ê³„: ê³ ê¸‰ ê¸°ëŠ¥ ğŸ”´

> **ë‚œì´ë„:** ê³ ê¸‰  
> **ì†Œìš” ì‹œê°„:** 1.5ì‹œê°„  
> **ì‚¬ì „ ì§€ì‹:** 1~6ë‹¨ê³„ ì™„ë£Œ, PINN ê²½í—˜

---

## ğŸ¯ í•™ìŠµ ëª©í‘œ

ì´ ë‹¨ê³„ë¥¼ ë§ˆì¹˜ë©´ ë‹¤ìŒì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

- âœ… ë™ì  ì†ì‹¤ ê°€ì¤‘ì¹˜ (GradNorm) ì‚¬ìš©í•˜ê¸°
- âœ… ì ì‘í˜• ì”ì°¨ ìƒ˜í”Œë§ êµ¬í˜„í•˜ê¸°
- âœ… 3D ë¬¸ì œ í•´ê²°í•˜ê¸°
- âœ… ì„±ëŠ¥ ìµœì í™” ê¸°ë²• ì ìš©í•˜ê¸°
- âœ… ë³µì¡í•œ ë¬¸ì œ ë””ë²„ê¹…í•˜ê¸°

---

## âš–ï¸ ë™ì  ì†ì‹¤ ê°€ì¤‘ì¹˜ (GradNorm)

### ë¬¸ì œ ìƒí™©

ë‹¤ì¤‘ ì†ì‹¤ í•­ëª©ì˜ **í›ˆë ¨ ì†ë„ê°€ ë‹¤ë¦…ë‹ˆë‹¤**!

**ì˜ˆì‹œ:**
```
Epoch 100:
  L_pde:  0.1 â†’ 0.01  (10ë°° ê°ì†Œ)
  L_bc:   0.001 â†’ 0.0005  (2ë°° ê°ì†Œ)
```

ê²½ê³„ ì¡°ê±´ì´ ëŠë¦¬ê²Œ í•™ìŠµë˜ê³  ìˆìŠµë‹ˆë‹¤!

### GradNorm ì•„ì´ë””ì–´

ê° ì†ì‹¤ í•­ëª©ì˜ **í•™ìŠµ ì†ë„ë¥¼ ê· í˜• ìˆê²Œ** ì¡°ì •í•©ë‹ˆë‹¤.

**í•µì‹¬:**
```
ë¹ ë¥´ê²Œ í•™ìŠµë˜ëŠ” í•­ëª© â†’ ê°€ì¤‘ì¹˜ ê°ì†Œ
ëŠë¦¬ê²Œ í•™ìŠµë˜ëŠ” í•­ëª© â†’ ê°€ì¤‘ì¹˜ ì¦ê°€
```

### ì‚¬ìš© ë°©ë²•

```python
from src.loss import DynamicWeightedLoss, PhysicsInformedLoss

# ê¸°ë³¸ ì†ì‹¤ í•¨ìˆ˜
base_loss = PhysicsInformedLoss(
    pde_fn=my_pde_residual,
    boundary_weight=1.0,  # ì´ˆê¸° ê°€ì¤‘ì¹˜
    initial_weight=1.0
)

# ë™ì  ê°€ì¤‘ì¹˜ ë˜í¼
dynamic_loss = DynamicWeightedLoss(
    base_loss=base_loss,
    alpha=1.5,           # ê· í˜• íŒŒë¼ë¯¸í„° (1.5 ê¶Œì¥)
    learning_rate=0.01   # ê°€ì¤‘ì¹˜ í•™ìŠµë¥ 
)

print("âœ… DynamicWeightedLoss ìƒì„± ì™„ë£Œ")
```

### í›ˆë ¨ ë£¨í”„ì—ì„œ ì‚¬ìš©

```python
import torch
from torch.optim import Adam

# ëª¨ë¸ ìµœì í™”ê¸°
optimizer = Adam(model.parameters(), lr=1e-3)

# í›ˆë ¨
for epoch in range(num_epochs):
    # Forward pass
    loss, metrics = dynamic_loss(
        model=model,
        x_pde=x_collocation,
        x_bc=x_boundary,
        u_bc=u_boundary
    )
    
    # Backward pass (ëª¨ë¸ë§Œ)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    # ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ (ë³„ë„)
    dynamic_loss.update_weights()
    
    # í˜„ì¬ ê°€ì¤‘ì¹˜ í™•ì¸
    if (epoch + 1) % 100 == 0:
        weights = dynamic_loss.get_weights()
        print(f"Epoch {epoch+1}:")
        print(f"  Î»_pde: {weights['pde']:.4f}")
        print(f"  Î»_bc:  {weights['bc']:.4f}")
```

### ê°€ì¤‘ì¹˜ ì§„í™” ì‹œê°í™”

```python
import matplotlib.pyplot as plt

# ê°€ì¤‘ì¹˜ íˆìŠ¤í† ë¦¬ ê¸°ë¡
weight_history = {'pde': [], 'bc': [], 'ic': []}

for epoch in range(num_epochs):
    # í›ˆë ¨...
    
    weights = dynamic_loss.get_weights()
    weight_history['pde'].append(weights['pde'])
    weight_history['bc'].append(weights['bc'])
    if 'ic' in weights:
        weight_history['ic'].append(weights['ic'])

# ì‹œê°í™”
plt.figure(figsize=(10, 5))
plt.plot(weight_history['pde'], label='Î»_pde')
plt.plot(weight_history['bc'], label='Î»_bc')
if weight_history['ic']:
    plt.plot(weight_history['ic'], label='Î»_ic')
plt.xlabel('Epoch')
plt.ylabel('Weight')
plt.title('Dynamic Weight Evolution')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
```

### ì–¸ì œ ì‚¬ìš©í•˜ë‚˜ìš”?

**ì‚¬ìš© ê¶Œì¥:**
- âœ… ì†ì‹¤ í•­ëª©ì´ 3ê°œ ì´ìƒ
- âœ… ê° í•­ëª©ì˜ ìŠ¤ì¼€ì¼ì´ í¬ê²Œ ë‹¤ë¦„
- âœ… ì¼ë¶€ ì¡°ê±´ì´ ì˜ ë§Œì¡±ë˜ì§€ ì•ŠìŒ

**ì‚¬ìš© ë¶ˆí•„ìš”:**
- âŒ ì†ì‹¤ í•­ëª©ì´ 2ê°œ ì´í•˜
- âŒ ì´ë¯¸ ì˜ í›ˆë ¨ë¨
- âŒ ê°„ë‹¨í•œ ë¬¸ì œ

---

## ğŸ¯ ì ì‘í˜• ì”ì°¨ ìƒ˜í”Œë§

### ë¬¸ì œ ìƒí™©

ì”ì°¨ê°€ **ë¶ˆê· ë“±í•˜ê²Œ ë¶„í¬**í•©ë‹ˆë‹¤!

```
ì˜ì—­ A: ì”ì°¨ = 0.001  (ì‘ìŒ, ì˜ ë§Œì¡±)
ì˜ì—­ B: ì”ì°¨ = 0.1    (í¼, ëª» ë§Œì¡±)
```

ì˜ì—­ Bì— ë” ë§ì€ í¬ì¸íŠ¸ê°€ í•„ìš”í•©ë‹ˆë‹¤!

### ì ì‘í˜• ìƒ˜í”Œë§ ì•„ì´ë””ì–´

**ì „ëµ:**
1. ì”ì°¨ê°€ í° ì˜ì—­ ì°¾ê¸°
2. í•´ë‹¹ ì˜ì—­ì— ìƒˆ í¬ì¸íŠ¸ ì¶”ê°€
3. ë°˜ë³µ

### ì‚¬ìš© ë°©ë²•

```python
from src.data import AdaptiveResidualSampler

# ì´ˆê¸° ìƒ˜í”ŒëŸ¬
initial_sampler = LatinHypercubeSampler(
    n_points=500,
    domain_min=torch.tensor([0.0, 0.0]),
    domain_max=torch.tensor([1.0, 1.0])
)

# ì ì‘í˜• ìƒ˜í”ŒëŸ¬
adaptive_sampler = AdaptiveResidualSampler(
    initial_sampler=initial_sampler,
    refinement_percentage=0.15,  # ìƒìœ„ 15% ì”ì°¨ ì˜ì—­ ì •ì œ
    max_points=2000              # ìµœëŒ€ í¬ì¸íŠ¸ ìˆ˜
)

print(f"ì´ˆê¸° í¬ì¸íŠ¸: {adaptive_sampler.get_num_points()}")
```

### í›ˆë ¨ ì¤‘ ì •ì œ

```python
# í›ˆë ¨ ë£¨í”„
num_refinements = 5  # ì •ì œ íšŸìˆ˜
refine_interval = 200  # 200 ì—í¬í¬ë§ˆë‹¤

for epoch in range(num_epochs):
    # í˜„ì¬ í¬ì¸íŠ¸ ê°€ì ¸ì˜¤ê¸°
    x_collocation = adaptive_sampler.get_points()
    
    # í›ˆë ¨ (ì¼ë°˜ì ì¸ ë°©ë²•)
    loss, metrics = loss_function(model, x_collocation, x_bc, u_bc)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    # ì •ì œ (ì£¼ê¸°ì )
    if (epoch + 1) % refine_interval == 0 and epoch < num_refinements * refine_interval:
        # ì”ì°¨ ê³„ì‚°
        with torch.no_grad():
            residuals = compute_residuals(model, x_collocation)
        
        # í¬ì¸íŠ¸ ì •ì œ
        adaptive_sampler.refine(residuals)
        
        print(f"Epoch {epoch+1}: í¬ì¸íŠ¸ ì •ì œ ì™„ë£Œ")
        print(f"  ìƒˆ í¬ì¸íŠ¸ ìˆ˜: {adaptive_sampler.get_num_points()}")
        print(f"  í‰ê·  ì”ì°¨: {residuals.abs().mean().item():.6e}")
```

### ì •ì œ ê³¼ì • ì‹œê°í™”

```python
import matplotlib.pyplot as plt

# ì •ì œ ì „í›„ ë¹„êµ
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# ì •ì œ ì „
points_before = adaptive_sampler.get_points()
axes[0].scatter(points_before[:, 0].numpy(), points_before[:, 1].numpy(), 
                s=5, alpha=0.5)
axes[0].set_xlabel('x')
axes[0].set_ylabel('y')
axes[0].set_title(f'Before Refinement ({len(points_before)} points)')
axes[0].axis('equal')
axes[0].grid(True, alpha=0.3)

# ì”ì°¨ ê³„ì‚° ë° ì •ì œ
with torch.no_grad():
    residuals = compute_residuals(model, points_before)
adaptive_sampler.refine(residuals)

# ì •ì œ í›„
points_after = adaptive_sampler.get_points()
axes[1].scatter(points_after[:, 0].numpy(), points_after[:, 1].numpy(), 
                s=5, alpha=0.5)
axes[1].set_xlabel('x')
axes[1].set_ylabel('y')
axes[1].set_title(f'After Refinement ({len(points_after)} points)')
axes[1].axis('equal')
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

### ì–¸ì œ ì‚¬ìš©í•˜ë‚˜ìš”?

**ì‚¬ìš© ê¶Œì¥:**
- âœ… ë³µì¡í•œ í•´ (ê¸‰ê²©í•œ ë³€í™”, íŠ¹ì´ì )
- âœ… í° ë„ë©”ì¸
- âœ… ì˜¤ì°¨ê°€ íŠ¹ì • ì˜ì—­ì— ì§‘ì¤‘

**ì‚¬ìš© ë¶ˆí•„ìš”:**
- âŒ ê°„ë‹¨í•˜ê³  ë¶€ë“œëŸ¬ìš´ í•´
- âŒ ì´ë¯¸ ì¶©ë¶„í•œ í¬ì¸íŠ¸
- âŒ ì‘ì€ 1D ë¬¸ì œ

---

## ğŸ§Š 3D ë¬¸ì œ í•´ê²°

### 3D Poisson ë°©ì •ì‹

**ë°©ì •ì‹:**
```
âˆ‡Â²u = âˆ‚Â²u/âˆ‚xÂ² + âˆ‚Â²u/âˆ‚yÂ² + âˆ‚Â²u/âˆ‚zÂ² = f(x,y,z)
```

### ëª¨ë¸ ì •ì˜

```python
from src.models import Scaled_cPIKAN
import torch

# 3D ëª¨ë¸
model_3d = Scaled_cPIKAN(
    layers_hidden=[3, 100, 100, 100, 1],  # ì…ë ¥ 3ì°¨ì› (x,y,z)
    degree=4,
    domain_min=torch.tensor([0.0, 0.0, 0.0]),
    domain_max=torch.tensor([1.0, 1.0, 1.0])
)

print(f"3D ëª¨ë¸ íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in model_3d.parameters())}")
```

### 3D ë°ì´í„° ì¤€ë¹„

```python
from src.data import LatinHypercubeSampler

# ì½œë¡œì¼€ì´ì…˜ í¬ì¸íŠ¸ (3D LHS)
sampler_3d = LatinHypercubeSampler(
    n_points=5000,  # 3DëŠ” ë” ë§ì€ í¬ì¸íŠ¸ í•„ìš”
    domain_min=torch.tensor([0.0, 0.0, 0.0]),
    domain_max=torch.tensor([1.0, 1.0, 1.0])
)
xyz_collocation = sampler_3d.sample()

# ê²½ê³„ ì¡°ê±´ (6ê°œ ë©´)
def create_3d_boundary(n_per_face=50):
    """3D íë¸Œì˜ 6ê°œ ë©´ ê²½ê³„ ë°ì´í„°"""
    boundaries = []
    
    # x=0 ë©´
    y = torch.linspace(0, 1, n_per_face)
    z = torch.linspace(0, 1, n_per_face)
    Y, Z = torch.meshgrid(y, z, indexing='ij')
    X = torch.zeros_like(Y)
    boundaries.append(torch.stack([X.flatten(), Y.flatten(), Z.flatten()], dim=1))
    
    # x=1 ë©´
    X = torch.ones_like(Y)
    boundaries.append(torch.stack([X.flatten(), Y.flatten(), Z.flatten()], dim=1))
    
    # y=0 ë©´
    x = torch.linspace(0, 1, n_per_face)
    z = torch.linspace(0, 1, n_per_face)
    X, Z = torch.meshgrid(x, z, indexing='ij')
    Y = torch.zeros_like(X)
    boundaries.append(torch.stack([X.flatten(), Y.flatten(), Z.flatten()], dim=1))
    
    # y=1 ë©´
    Y = torch.ones_like(X)
    boundaries.append(torch.stack([X.flatten(), Y.flatten(), Z.flatten()], dim=1))
    
    # z=0 ë©´
    x = torch.linspace(0, 1, n_per_face)
    y = torch.linspace(0, 1, n_per_face)
    X, Y = torch.meshgrid(x, y, indexing='ij')
    Z = torch.zeros_like(X)
    boundaries.append(torch.stack([X.flatten(), Y.flatten(), Z.flatten()], dim=1))
    
    # z=1 ë©´
    Z = torch.ones_like(X)
    boundaries.append(torch.stack([X.flatten(), Y.flatten(), Z.flatten()], dim=1))
    
    # ëª¨ë“  ë©´ í•©ì¹˜ê¸°
    xyz_bc = torch.cat(boundaries, dim=0)
    u_bc = torch.zeros(len(xyz_bc), 1)  # ì˜ˆ: u=0
    
    return xyz_bc, u_bc

xyz_bc, u_bc = create_3d_boundary(n_per_face=20)
print(f"3D ê²½ê³„ í¬ì¸íŠ¸: {xyz_bc.shape}")
```

### 3D ì†ì‹¤ í•¨ìˆ˜

```python
def poisson_3d_loss(model, xyz_col, xyz_bc, u_bc, lambda_bc=10.0):
    """3D Poisson ë°©ì •ì‹ ì†ì‹¤"""
    
    # PDE ì”ì°¨
    xyz_col = xyz_col.requires_grad_(True)
    u = model(xyz_col)
    
    # 1ì°¨ ë¯¸ë¶„
    grad_u = torch.autograd.grad(
        u, xyz_col, torch.ones_like(u), create_graph=True
    )[0]
    
    # 2ì°¨ ë¯¸ë¶„ (Laplacian)
    laplacian = 0
    for i in range(3):  # x, y, z
        grad_u_i = grad_u[:, i:i+1]
        grad2_u_i = torch.autograd.grad(
            grad_u_i, xyz_col, torch.ones_like(grad_u_i), create_graph=True
        )[0][:, i:i+1]
        laplacian += grad2_u_i
    
    # ì†ŒìŠ¤ í•­
    f = -3 * torch.pi**2 * torch.sin(torch.pi * xyz_col[:, 0:1]) * \
        torch.sin(torch.pi * xyz_col[:, 1:2]) * torch.sin(torch.pi * xyz_col[:, 2:3])
    
    residual = laplacian - f
    loss_pde = torch.mean(residual ** 2)
    
    # ê²½ê³„ ì¡°ê±´
    u_bc_pred = model(xyz_bc)
    loss_bc = torch.mean((u_bc_pred - u_bc) ** 2)
    
    # ì´ ì†ì‹¤
    loss_total = loss_pde + lambda_bc * loss_bc
    
    return loss_total, {'loss_pde': loss_pde.item(), 'loss_bc': loss_bc.item()}
```

### 3D ê²°ê³¼ ì‹œê°í™”

```python
def visualize_3d_slices(model, z_values=[0.25, 0.5, 0.75]):
    """3D ê²°ê³¼ë¥¼ 2D ìŠ¬ë¼ì´ìŠ¤ë¡œ ì‹œê°í™”"""
    
    fig, axes = plt.subplots(1, len(z_values), figsize=(15, 4))
    
    for ax, z_val in zip(axes, z_values):
        # ê·¸ë¦¬ë“œ ìƒì„±
        x = torch.linspace(0, 1, 50)
        y = torch.linspace(0, 1, 50)
        X, Y = torch.meshgrid(x, y, indexing='ij')
        Z = torch.ones_like(X) * z_val
        
        xyz = torch.stack([X.flatten(), Y.flatten(), Z.flatten()], dim=1)
        
        # ì˜ˆì¸¡
        with torch.no_grad():
            U = model(xyz).reshape(50, 50)
        
        # í”Œë¡¯
        contour = ax.contourf(X.numpy(), Y.numpy(), U.numpy(), levels=15, cmap='viridis')
        ax.set_xlabel('x')
        ax.set_ylabel('y')
        ax.set_title(f'z = {z_val:.2f}')
        ax.axis('equal')
        fig.colorbar(contour, ax=ax)
    
    plt.tight_layout()
    plt.show()

# ì‚¬ìš©
visualize_3d_slices(model_3d, z_values=[0.25, 0.5, 0.75])
```

### 3D ì˜ˆì œ ì‹¤í–‰

í”„ë¡œì íŠ¸ì— ì™„ì „í•œ ì˜ˆì œê°€ ìˆìŠµë‹ˆë‹¤:

```bash
python examples/solve_3d_poisson.py --help
```

**ì£¼ìš” í”Œë˜ê·¸:**
- `--n-collocation`: ì½œë¡œì¼€ì´ì…˜ í¬ì¸íŠ¸ ìˆ˜ (ê¸°ë³¸ 5000)
- `--epochs-adam`: Adam ì—í¬í¬ (ê¸°ë³¸ 2000)
- `--steps-lbfgs`: L-BFGS ìŠ¤í… (ê¸°ë³¸ 100)

---

## âš¡ ì„±ëŠ¥ ìµœì í™”

### 1. Mixed Precision Training

**ì´ì :** ë©”ëª¨ë¦¬ ì ˆì•½ + ì†ë„ í–¥ìƒ (2~3ë°°)

```python
from torch.cuda.amp import autocast, GradScaler

# Scaler ìƒì„±
scaler = GradScaler()

# í›ˆë ¨ ë£¨í”„
for epoch in range(num_epochs):
    optimizer.zero_grad()
    
    # Mixed precision forward
    with autocast():
        loss, metrics = loss_function(model, x_collocation, x_bc, u_bc)
    
    # Scaled backward
    scaler.scale(loss).backward()
    scaler.step(optimizer)
    scaler.update()
```

### 2. ë°°ì¹˜ ì²˜ë¦¬

í° ë°ì´í„°ì…‹ì„ ì‘ì€ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ê¸°:

```python
def split_into_batches(x, batch_size):
    """ë°ì´í„°ë¥¼ ë°°ì¹˜ë¡œ ë¶„í• """
    n = len(x)
    indices = torch.randperm(n)
    batches = []
    for i in range(0, n, batch_size):
        batch_indices = indices[i:i+batch_size]
        batches.append(x[batch_indices])
    return batches

# ì‚¬ìš©
batch_size = 500
batches = split_into_batches(x_collocation, batch_size)

for epoch in range(num_epochs):
    for batch in batches:
        loss, _ = loss_function(model, batch, x_bc, u_bc)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
```

### 3. ì²´í¬í¬ì¸íŠ¸ ì €ì¥

í›ˆë ¨ ì¤‘ê°„ ìƒíƒœ ì €ì¥:

```python
def save_checkpoint(model, optimizer, epoch, loss, path='checkpoint.pth'):
    """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
    torch.save({
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'loss': loss
    }, path)
    print(f"âœ… ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {path}")

def load_checkpoint(model, optimizer, path='checkpoint.pth'):
    """ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
    checkpoint = torch.load(path)
    model.load_state_dict(checkpoint['model_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    epoch = checkpoint['epoch']
    loss = checkpoint['loss']
    print(f"âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: Epoch {epoch}, Loss {loss:.6f}")
    return epoch, loss

# ì‚¬ìš©
if (epoch + 1) % 500 == 0:
    save_checkpoint(model, optimizer, epoch, loss.item())
```

---

## ğŸ› ë¬¸ì œ í•´ê²° (Troubleshooting)

### ë¬¸ì œ 1: NaN ì†ì‹¤

**ì¦ìƒ:**
```
Epoch 150: Loss = nan
```

**ì›ì¸ ë° í•´ê²°:**

1. **í•™ìŠµë¥ ì´ ë„ˆë¬´ ë†’ìŒ**
   ```python
   optimizer = Adam(model.parameters(), lr=1e-4)  # 1e-3 â†’ 1e-4
   ```

2. **ê·¸ë˜ë””ì–¸íŠ¸ í­ë°œ**
   ```python
   torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
   ```

3. **ìˆ˜ì¹˜ì  ë¶ˆì•ˆì •ì„±**
   ```python
   # ì†ì‹¤ í•¨ìˆ˜ì—ì„œ
   loss = torch.mean(residual ** 2 + 1e-10)  # epsilon ì¶”ê°€
   ```

### ë¬¸ì œ 2: í›ˆë ¨ì´ ë©ˆì¶¤ (Plateau)

**ì¦ìƒ:**
```
Epoch 500: Loss = 0.05
Epoch 1000: Loss = 0.05  (ë³€í™” ì—†ìŒ)
```

**í•´ê²°:**

1. **í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì‚¬ìš©**
   ```python
   from torch.optim.lr_scheduler import ReduceLROnPlateau
   scheduler = ReduceLROnPlateau(optimizer, patience=50, factor=0.5)
   ```

2. **L-BFGSë¡œ ì „í™˜**
   ```python
   # Adamì´ plateauì— ë„ë‹¬í•˜ë©´ L-BFGSë¡œ
   optimizer = LBFGS(model.parameters(), lr=1.0)
   ```

3. **ëª¨ë¸ í¬ê¸° ì¦ê°€**
   ```python
   layers_hidden=[2, 50, 50, 50, 1]  # ì€ë‹‰ì¸µ ì¶”ê°€
   ```

### ë¬¸ì œ 3: ê²½ê³„ ì¡°ê±´ì´ ë§Œì¡± ì•ˆ ë¨

**ì¦ìƒ:**
```
Loss_pde: 0.001 (ì¢‹ìŒ)
Loss_bc: 0.5 (ë‚˜ì¨)
```

**í•´ê²°:**

1. **ê²½ê³„ ê°€ì¤‘ì¹˜ ì¦ê°€**
   ```python
   lambda_bc = 100.0  # 10.0 â†’ 100.0
   ```

2. **ê²½ê³„ í¬ì¸íŠ¸ ì¦ê°€**
   ```python
   x_bc = create_boundary_data(n_boundary=200)  # 50 â†’ 200
   ```

3. **ë™ì  ê°€ì¤‘ì¹˜ ì‚¬ìš©**
   ```python
   dynamic_loss = DynamicWeightedLoss(base_loss, alpha=1.5)
   ```

### ë¬¸ì œ 4: ë©”ëª¨ë¦¬ ë¶€ì¡±

**ì¦ìƒ:**
```
RuntimeError: CUDA out of memory
```

**í•´ê²°:**

1. **ë°°ì¹˜ í¬ê¸° ê°ì†Œ**
   ```python
   n_collocation = 500  # 1000 â†’ 500
   ```

2. **ëª¨ë¸ í¬ê¸° ê°ì†Œ**
   ```python
   layers_hidden=[2, 30, 30, 1]  # 50 â†’ 30
   ```

3. **ì²´í¬í¬ì¸íŒ… ì‚¬ìš©**
   ```python
   from torch.utils.checkpoint import checkpoint
   # (ê³ ê¸‰ ê¸°ë²•, ë¬¸ì„œ ì°¸ì¡°)
   ```

---

## ğŸ¯ ì²´í¬í¬ì¸íŠ¸

ë‹¤ìŒ í•­ëª©ì„ ëª¨ë‘ ì´í•´í–ˆìœ¼ë©´ ê°€ì´ë“œë¥¼ ì™„ë£Œí•œ ê²ƒì…ë‹ˆë‹¤!

- [ ] ë™ì  ì†ì‹¤ ê°€ì¤‘ì¹˜ ì‚¬ìš©ë²• ì´í•´
- [ ] ì ì‘í˜• ìƒ˜í”Œë§ êµ¬í˜„ ì„±ê³µ
- [ ] 3D ë¬¸ì œ í•´ê²° ê²½í—˜
- [ ] ì„±ëŠ¥ ìµœì í™” ê¸°ë²• í•™ìŠµ
- [ ] ì¼ë°˜ì ì¸ ë¬¸ì œ ë””ë²„ê¹… ê°€ëŠ¥

---

## ğŸ‰ ì¶•í•˜í•©ë‹ˆë‹¤!

ëª¨ë“  ê³ ê¸‰ ê¸°ëŠ¥ì„ ë§ˆìŠ¤í„°í–ˆìŠµë‹ˆë‹¤! ì´ì œ ì—¬ëŸ¬ë¶„ì€ Scaled-cPIKAN ì „ë¬¸ê°€ì…ë‹ˆë‹¤!

**ë‹¤ìŒ ë‹¨ê³„:**

- ğŸ”¬ ìì‹ ë§Œì˜ PDE ë¬¸ì œ ì •ì˜ ë° í•´ê²°
- ğŸ“ ì—°êµ¬ ë…¼ë¬¸ ì‘ì„±
- ğŸŒ ì»¤ë®¤ë‹ˆí‹°ì— ê¸°ì—¬
- ğŸ“š ìµœì‹  PINN ì—°êµ¬ ë™í–¥ íŒ”ë¡œìš°

**ì¶”ê°€ ìë£Œ:**

- PINN ìµœì‹  ë…¼ë¬¸: [arXiv.org](https://arxiv.org)
- ì»¤ë®¤ë‹ˆí‹°: [GitHub Discussions](https://github.com)
- íŠœí† ë¦¬ì–¼: [DeepXDE](https://deepxde.readthedocs.io/)

---

## ğŸ“š ì „ì²´ ì°¸ê³  ìë£Œ

### ë…¼ë¬¸

1. **PINN ì›ë…¼ë¬¸:** Raissi et al., "Physics-informed neural networks" (2019)
2. **KAN:** Liu et al., "KAN: Kolmogorov-Arnold Networks" (2024)
3. **GradNorm:** Chen et al., "GradNorm: Gradient Normalization for Adaptive Loss Balancing" (2018)
4. **Adaptive Sampling:** Wu et al., "A comprehensive study of non-adaptive and residual-based adaptive sampling" (2023)

### ì›¹ì‚¬ì´íŠ¸

- **PyTorch ê³µì‹ ë¬¸ì„œ:** [https://pytorch.org/docs/](https://pytorch.org/docs/)
- **DeepXDE:** [https://deepxde.readthedocs.io/](https://deepxde.readthedocs.io/)
- **SciML (Julia):** [https://sciml.ai/](https://sciml.ai/)

---

## ğŸ‘‹ ë§ˆë¬´ë¦¬

**ì§ˆë¬¸ì´ë‚˜ í”¼ë“œë°±ì´ ìˆìœ¼ì‹ ê°€ìš”?**

í”„ë¡œì íŠ¸ ì €ì¥ì†Œì˜ Issuesë‚˜ Discussionsì— ë‚¨ê²¨ì£¼ì„¸ìš”!

**í–‰ë³µí•œ PINN ì—¬í–‰ ë˜ì„¸ìš”! ğŸš€**

---

ğŸ‘‰ [ë©”ì¸ ê°€ì´ë“œë¡œ ëŒì•„ê°€ê¸°](../ì‚¬ìš©ì_ê°€ì´ë“œ.md)

---

*ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸: 2025ë…„ 1ì›”*
