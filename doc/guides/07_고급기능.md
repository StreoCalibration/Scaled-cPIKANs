# 7단계: 고급 기능 🔴

> **난이도:** 고급  
> **소요 시간:** 1.5시간  
> **사전 지식:** 1~6단계 완료, PINN 경험

---

## 🎯 학습 목표

이 단계를 마치면 다음을 할 수 있습니다:

- ✅ 동적 손실 가중치 (GradNorm) 사용하기
- ✅ 적응형 잔차 샘플링 구현하기
- ✅ 3D 문제 해결하기
- ✅ 성능 최적화 기법 적용하기
- ✅ 복잡한 문제 디버깅하기

---

## ⚖️ 동적 손실 가중치 (GradNorm)

### 문제 상황

다중 손실 항목의 **훈련 속도가 다릅니다**!

**예시:**
```
Epoch 100:
  L_pde:  0.1 → 0.01  (10배 감소)
  L_bc:   0.001 → 0.0005  (2배 감소)
```

경계 조건이 느리게 학습되고 있습니다!

### GradNorm 아이디어

각 손실 항목의 **학습 속도를 균형 있게** 조정합니다.

**핵심:**
```
빠르게 학습되는 항목 → 가중치 감소
느리게 학습되는 항목 → 가중치 증가
```

### 사용 방법

```python
from src.loss import DynamicWeightedLoss, PhysicsInformedLoss

# 기본 손실 함수
base_loss = PhysicsInformedLoss(
    pde_fn=my_pde_residual,
    boundary_weight=1.0,  # 초기 가중치
    initial_weight=1.0
)

# 동적 가중치 래퍼
dynamic_loss = DynamicWeightedLoss(
    base_loss=base_loss,
    alpha=1.5,           # 균형 파라미터 (1.5 권장)
    learning_rate=0.01   # 가중치 학습률
)

print("✅ DynamicWeightedLoss 생성 완료")
```

### 훈련 루프에서 사용

```python
import torch
from torch.optim import Adam

# 모델 최적화기
optimizer = Adam(model.parameters(), lr=1e-3)

# 훈련
for epoch in range(num_epochs):
    # Forward pass
    loss, metrics = dynamic_loss(
        model=model,
        x_pde=x_collocation,
        x_bc=x_boundary,
        u_bc=u_boundary
    )
    
    # Backward pass (모델만)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    # 가중치 업데이트 (별도)
    dynamic_loss.update_weights()
    
    # 현재 가중치 확인
    if (epoch + 1) % 100 == 0:
        weights = dynamic_loss.get_weights()
        print(f"Epoch {epoch+1}:")
        print(f"  λ_pde: {weights['pde']:.4f}")
        print(f"  λ_bc:  {weights['bc']:.4f}")
```

### 가중치 진화 시각화

```python
import matplotlib.pyplot as plt

# 가중치 히스토리 기록
weight_history = {'pde': [], 'bc': [], 'ic': []}

for epoch in range(num_epochs):
    # 훈련...
    
    weights = dynamic_loss.get_weights()
    weight_history['pde'].append(weights['pde'])
    weight_history['bc'].append(weights['bc'])
    if 'ic' in weights:
        weight_history['ic'].append(weights['ic'])

# 시각화
plt.figure(figsize=(10, 5))
plt.plot(weight_history['pde'], label='λ_pde')
plt.plot(weight_history['bc'], label='λ_bc')
if weight_history['ic']:
    plt.plot(weight_history['ic'], label='λ_ic')
plt.xlabel('Epoch')
plt.ylabel('Weight')
plt.title('Dynamic Weight Evolution')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
```

### 언제 사용하나요?

**사용 권장:**
- ✅ 손실 항목이 3개 이상
- ✅ 각 항목의 스케일이 크게 다름
- ✅ 일부 조건이 잘 만족되지 않음

**사용 불필요:**
- ❌ 손실 항목이 2개 이하
- ❌ 이미 잘 훈련됨
- ❌ 간단한 문제

---

## 🎯 적응형 잔차 샘플링

### 문제 상황

잔차가 **불균등하게 분포**합니다!

```
영역 A: 잔차 = 0.001  (작음, 잘 만족)
영역 B: 잔차 = 0.1    (큼, 못 만족)
```

영역 B에 더 많은 포인트가 필요합니다!

### 적응형 샘플링 아이디어

**전략:**
1. 잔차가 큰 영역 찾기
2. 해당 영역에 새 포인트 추가
3. 반복

### 사용 방법

```python
from src.data import AdaptiveResidualSampler

# 초기 샘플러
initial_sampler = LatinHypercubeSampler(
    n_points=500,
    domain_min=torch.tensor([0.0, 0.0]),
    domain_max=torch.tensor([1.0, 1.0])
)

# 적응형 샘플러
adaptive_sampler = AdaptiveResidualSampler(
    initial_sampler=initial_sampler,
    refinement_percentage=0.15,  # 상위 15% 잔차 영역 정제
    max_points=2000              # 최대 포인트 수
)

print(f"초기 포인트: {adaptive_sampler.get_num_points()}")
```

### 훈련 중 정제

```python
# 훈련 루프
num_refinements = 5  # 정제 횟수
refine_interval = 200  # 200 에포크마다

for epoch in range(num_epochs):
    # 현재 포인트 가져오기
    x_collocation = adaptive_sampler.get_points()
    
    # 훈련 (일반적인 방법)
    loss, metrics = loss_function(model, x_collocation, x_bc, u_bc)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    # 정제 (주기적)
    if (epoch + 1) % refine_interval == 0 and epoch < num_refinements * refine_interval:
        # 잔차 계산
        with torch.no_grad():
            residuals = compute_residuals(model, x_collocation)
        
        # 포인트 정제
        adaptive_sampler.refine(residuals)
        
        print(f"Epoch {epoch+1}: 포인트 정제 완료")
        print(f"  새 포인트 수: {adaptive_sampler.get_num_points()}")
        print(f"  평균 잔차: {residuals.abs().mean().item():.6e}")
```

### 정제 과정 시각화

```python
import matplotlib.pyplot as plt

# 정제 전후 비교
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# 정제 전
points_before = adaptive_sampler.get_points()
axes[0].scatter(points_before[:, 0].numpy(), points_before[:, 1].numpy(), 
                s=5, alpha=0.5)
axes[0].set_xlabel('x')
axes[0].set_ylabel('y')
axes[0].set_title(f'Before Refinement ({len(points_before)} points)')
axes[0].axis('equal')
axes[0].grid(True, alpha=0.3)

# 잔차 계산 및 정제
with torch.no_grad():
    residuals = compute_residuals(model, points_before)
adaptive_sampler.refine(residuals)

# 정제 후
points_after = adaptive_sampler.get_points()
axes[1].scatter(points_after[:, 0].numpy(), points_after[:, 1].numpy(), 
                s=5, alpha=0.5)
axes[1].set_xlabel('x')
axes[1].set_ylabel('y')
axes[1].set_title(f'After Refinement ({len(points_after)} points)')
axes[1].axis('equal')
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

### 언제 사용하나요?

**사용 권장:**
- ✅ 복잡한 해 (급격한 변화, 특이점)
- ✅ 큰 도메인
- ✅ 오차가 특정 영역에 집중

**사용 불필요:**
- ❌ 간단하고 부드러운 해
- ❌ 이미 충분한 포인트
- ❌ 작은 1D 문제

---

## 🧊 3D 문제 해결

### 3D Poisson 방정식

**방정식:**
```
∇²u = ∂²u/∂x² + ∂²u/∂y² + ∂²u/∂z² = f(x,y,z)
```

### 모델 정의

```python
from src.models import Scaled_cPIKAN
import torch

# 3D 모델
model_3d = Scaled_cPIKAN(
    layers_hidden=[3, 100, 100, 100, 1],  # 입력 3차원 (x,y,z)
    degree=4,
    domain_min=torch.tensor([0.0, 0.0, 0.0]),
    domain_max=torch.tensor([1.0, 1.0, 1.0])
)

print(f"3D 모델 파라미터: {sum(p.numel() for p in model_3d.parameters())}")
```

### 3D 데이터 준비

```python
from src.data import LatinHypercubeSampler

# 콜로케이션 포인트 (3D LHS)
sampler_3d = LatinHypercubeSampler(
    n_points=5000,  # 3D는 더 많은 포인트 필요
    domain_min=torch.tensor([0.0, 0.0, 0.0]),
    domain_max=torch.tensor([1.0, 1.0, 1.0])
)
xyz_collocation = sampler_3d.sample()

# 경계 조건 (6개 면)
def create_3d_boundary(n_per_face=50):
    """3D 큐브의 6개 면 경계 데이터"""
    boundaries = []
    
    # x=0 면
    y = torch.linspace(0, 1, n_per_face)
    z = torch.linspace(0, 1, n_per_face)
    Y, Z = torch.meshgrid(y, z, indexing='ij')
    X = torch.zeros_like(Y)
    boundaries.append(torch.stack([X.flatten(), Y.flatten(), Z.flatten()], dim=1))
    
    # x=1 면
    X = torch.ones_like(Y)
    boundaries.append(torch.stack([X.flatten(), Y.flatten(), Z.flatten()], dim=1))
    
    # y=0 면
    x = torch.linspace(0, 1, n_per_face)
    z = torch.linspace(0, 1, n_per_face)
    X, Z = torch.meshgrid(x, z, indexing='ij')
    Y = torch.zeros_like(X)
    boundaries.append(torch.stack([X.flatten(), Y.flatten(), Z.flatten()], dim=1))
    
    # y=1 면
    Y = torch.ones_like(X)
    boundaries.append(torch.stack([X.flatten(), Y.flatten(), Z.flatten()], dim=1))
    
    # z=0 면
    x = torch.linspace(0, 1, n_per_face)
    y = torch.linspace(0, 1, n_per_face)
    X, Y = torch.meshgrid(x, y, indexing='ij')
    Z = torch.zeros_like(X)
    boundaries.append(torch.stack([X.flatten(), Y.flatten(), Z.flatten()], dim=1))
    
    # z=1 면
    Z = torch.ones_like(X)
    boundaries.append(torch.stack([X.flatten(), Y.flatten(), Z.flatten()], dim=1))
    
    # 모든 면 합치기
    xyz_bc = torch.cat(boundaries, dim=0)
    u_bc = torch.zeros(len(xyz_bc), 1)  # 예: u=0
    
    return xyz_bc, u_bc

xyz_bc, u_bc = create_3d_boundary(n_per_face=20)
print(f"3D 경계 포인트: {xyz_bc.shape}")
```

### 3D 손실 함수

```python
def poisson_3d_loss(model, xyz_col, xyz_bc, u_bc, lambda_bc=10.0):
    """3D Poisson 방정식 손실"""
    
    # PDE 잔차
    xyz_col = xyz_col.requires_grad_(True)
    u = model(xyz_col)
    
    # 1차 미분
    grad_u = torch.autograd.grad(
        u, xyz_col, torch.ones_like(u), create_graph=True
    )[0]
    
    # 2차 미분 (Laplacian)
    laplacian = 0
    for i in range(3):  # x, y, z
        grad_u_i = grad_u[:, i:i+1]
        grad2_u_i = torch.autograd.grad(
            grad_u_i, xyz_col, torch.ones_like(grad_u_i), create_graph=True
        )[0][:, i:i+1]
        laplacian += grad2_u_i
    
    # 소스 항
    f = -3 * torch.pi**2 * torch.sin(torch.pi * xyz_col[:, 0:1]) * \
        torch.sin(torch.pi * xyz_col[:, 1:2]) * torch.sin(torch.pi * xyz_col[:, 2:3])
    
    residual = laplacian - f
    loss_pde = torch.mean(residual ** 2)
    
    # 경계 조건
    u_bc_pred = model(xyz_bc)
    loss_bc = torch.mean((u_bc_pred - u_bc) ** 2)
    
    # 총 손실
    loss_total = loss_pde + lambda_bc * loss_bc
    
    return loss_total, {'loss_pde': loss_pde.item(), 'loss_bc': loss_bc.item()}
```

### 3D 결과 시각화

```python
def visualize_3d_slices(model, z_values=[0.25, 0.5, 0.75]):
    """3D 결과를 2D 슬라이스로 시각화"""
    
    fig, axes = plt.subplots(1, len(z_values), figsize=(15, 4))
    
    for ax, z_val in zip(axes, z_values):
        # 그리드 생성
        x = torch.linspace(0, 1, 50)
        y = torch.linspace(0, 1, 50)
        X, Y = torch.meshgrid(x, y, indexing='ij')
        Z = torch.ones_like(X) * z_val
        
        xyz = torch.stack([X.flatten(), Y.flatten(), Z.flatten()], dim=1)
        
        # 예측
        with torch.no_grad():
            U = model(xyz).reshape(50, 50)
        
        # 플롯
        contour = ax.contourf(X.numpy(), Y.numpy(), U.numpy(), levels=15, cmap='viridis')
        ax.set_xlabel('x')
        ax.set_ylabel('y')
        ax.set_title(f'z = {z_val:.2f}')
        ax.axis('equal')
        fig.colorbar(contour, ax=ax)
    
    plt.tight_layout()
    plt.show()

# 사용
visualize_3d_slices(model_3d, z_values=[0.25, 0.5, 0.75])
```

### 3D 예제 실행

프로젝트에 완전한 예제가 있습니다:

```bash
python examples/solve_3d_poisson.py --help
```

**주요 플래그:**
- `--n-collocation`: 콜로케이션 포인트 수 (기본 5000)
- `--epochs-adam`: Adam 에포크 (기본 2000)
- `--steps-lbfgs`: L-BFGS 스텝 (기본 100)

---

## ⚡ 성능 최적화

### 1. Mixed Precision Training

**이점:** 메모리 절약 + 속도 향상 (2~3배)

```python
from torch.cuda.amp import autocast, GradScaler

# Scaler 생성
scaler = GradScaler()

# 훈련 루프
for epoch in range(num_epochs):
    optimizer.zero_grad()
    
    # Mixed precision forward
    with autocast():
        loss, metrics = loss_function(model, x_collocation, x_bc, u_bc)
    
    # Scaled backward
    scaler.scale(loss).backward()
    scaler.step(optimizer)
    scaler.update()
```

### 2. 배치 처리

큰 데이터셋을 작은 배치로 나누기:

```python
def split_into_batches(x, batch_size):
    """데이터를 배치로 분할"""
    n = len(x)
    indices = torch.randperm(n)
    batches = []
    for i in range(0, n, batch_size):
        batch_indices = indices[i:i+batch_size]
        batches.append(x[batch_indices])
    return batches

# 사용
batch_size = 500
batches = split_into_batches(x_collocation, batch_size)

for epoch in range(num_epochs):
    for batch in batches:
        loss, _ = loss_function(model, batch, x_bc, u_bc)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
```

### 3. 체크포인트 저장

훈련 중간 상태 저장:

```python
def save_checkpoint(model, optimizer, epoch, loss, path='checkpoint.pth'):
    """체크포인트 저장"""
    torch.save({
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'loss': loss
    }, path)
    print(f"✅ 체크포인트 저장: {path}")

def load_checkpoint(model, optimizer, path='checkpoint.pth'):
    """체크포인트 로드"""
    checkpoint = torch.load(path)
    model.load_state_dict(checkpoint['model_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    epoch = checkpoint['epoch']
    loss = checkpoint['loss']
    print(f"✅ 체크포인트 로드: Epoch {epoch}, Loss {loss:.6f}")
    return epoch, loss

# 사용
if (epoch + 1) % 500 == 0:
    save_checkpoint(model, optimizer, epoch, loss.item())
```

---

## 🐛 문제 해결 (Troubleshooting)

### 문제 1: NaN 손실

**증상:**
```
Epoch 150: Loss = nan
```

**원인 및 해결:**

1. **학습률이 너무 높음**
   ```python
   optimizer = Adam(model.parameters(), lr=1e-4)  # 1e-3 → 1e-4
   ```

2. **그래디언트 폭발**
   ```python
   torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
   ```

3. **수치적 불안정성**
   ```python
   # 손실 함수에서
   loss = torch.mean(residual ** 2 + 1e-10)  # epsilon 추가
   ```

### 문제 2: 훈련이 멈춤 (Plateau)

**증상:**
```
Epoch 500: Loss = 0.05
Epoch 1000: Loss = 0.05  (변화 없음)
```

**해결:**

1. **학습률 스케줄러 사용**
   ```python
   from torch.optim.lr_scheduler import ReduceLROnPlateau
   scheduler = ReduceLROnPlateau(optimizer, patience=50, factor=0.5)
   ```

2. **L-BFGS로 전환**
   ```python
   # Adam이 plateau에 도달하면 L-BFGS로
   optimizer = LBFGS(model.parameters(), lr=1.0)
   ```

3. **모델 크기 증가**
   ```python
   layers_hidden=[2, 50, 50, 50, 1]  # 은닉층 추가
   ```

### 문제 3: 경계 조건이 만족 안 됨

**증상:**
```
Loss_pde: 0.001 (좋음)
Loss_bc: 0.5 (나쁨)
```

**해결:**

1. **경계 가중치 증가**
   ```python
   lambda_bc = 100.0  # 10.0 → 100.0
   ```

2. **경계 포인트 증가**
   ```python
   x_bc = create_boundary_data(n_boundary=200)  # 50 → 200
   ```

3. **동적 가중치 사용**
   ```python
   dynamic_loss = DynamicWeightedLoss(base_loss, alpha=1.5)
   ```

### 문제 4: 메모리 부족

**증상:**
```
RuntimeError: CUDA out of memory
```

**해결:**

1. **배치 크기 감소**
   ```python
   n_collocation = 500  # 1000 → 500
   ```

2. **모델 크기 감소**
   ```python
   layers_hidden=[2, 30, 30, 1]  # 50 → 30
   ```

3. **체크포인팅 사용**
   ```python
   from torch.utils.checkpoint import checkpoint
   # (고급 기법, 문서 참조)
   ```

---

## 🎯 체크포인트

다음 항목을 모두 이해했으면 가이드를 완료한 것입니다!

- [ ] 동적 손실 가중치 사용법 이해
- [ ] 적응형 샘플링 구현 성공
- [ ] 3D 문제 해결 경험
- [ ] 성능 최적화 기법 학습
- [ ] 일반적인 문제 디버깅 가능

---

## 🎉 축하합니다!

모든 고급 기능을 마스터했습니다! 이제 여러분은 Scaled-cPIKAN 전문가입니다!

**다음 단계:**

- 🔬 자신만의 PDE 문제 정의 및 해결
- 📝 연구 논문 작성
- 🌐 커뮤니티에 기여
- 📚 최신 PINN 연구 동향 팔로우

**추가 자료:**

- PINN 최신 논문: [arXiv.org](https://arxiv.org)
- 커뮤니티: [GitHub Discussions](https://github.com)
- 튜토리얼: [DeepXDE](https://deepxde.readthedocs.io/)

---

## 📚 전체 참고 자료

### 논문

1. **PINN 원논문:** Raissi et al., "Physics-informed neural networks" (2019)
2. **KAN:** Liu et al., "KAN: Kolmogorov-Arnold Networks" (2024)
3. **GradNorm:** Chen et al., "GradNorm: Gradient Normalization for Adaptive Loss Balancing" (2018)
4. **Adaptive Sampling:** Wu et al., "A comprehensive study of non-adaptive and residual-based adaptive sampling" (2023)

### 웹사이트

- **PyTorch 공식 문서:** [https://pytorch.org/docs/](https://pytorch.org/docs/)
- **DeepXDE:** [https://deepxde.readthedocs.io/](https://deepxde.readthedocs.io/)
- **SciML (Julia):** [https://sciml.ai/](https://sciml.ai/)

---

## 👋 마무리

**질문이나 피드백이 있으신가요?**

프로젝트 저장소의 Issues나 Discussions에 남겨주세요!

**행복한 PINN 여행 되세요! 🚀**

---

👉 [메인 가이드로 돌아가기](../사용자_가이드.md)

---

*마지막 업데이트: 2025년 1월*
