# 3단계: 모델 정의 🟡

> **난이도:** 중급  
> **소요 시간:** 30분  
> **사전 지식:** PyTorch 기초, 신경망 개념

---

## 🎯 학습 목표

이 단계를 마치면 다음을 할 수 있습니다:

- ✅ Scaled-cPIKAN 모델의 구조 이해하기
- ✅ Chebyshev 다항식의 역할 파악하기
- ✅ KAN (Kolmogorov-Arnold Networks) 개념 이해하기
- ✅ 모델 파라미터 설정 및 초기화하기
- ✅ 도메인 스케일링의 중요성 알기

---

## 🧠 [PINN 모델](./concepts/모델정의_PINN_손실함수_구조의_이유.md)이란?

### 기본 아이디어

일반 신경망:
```
입력 (x) → 신경망 → 출력 (y)
```

PINN:
```
입력 (좌표) → 신경망 → 출력 (물리량) → 방정식 검사
     x, y         u(x,y)            ∂u/∂x, ∂²u/∂x²
```

**핵심:** 신경망이 물리 법칙을 만족하는 함수를 근사합니다!

---

## 🌟 Scaled-cPIKAN이란?

### 이름의 의미

- **Scaled:** 입력을 [-1, 1]로 스케일링
- **c:** Chebyshev 다항식 사용
- **PIKAN:** Physics-Informed Kolmogorov-Arnold Network

### 왜 특별한가요?

#### 일반 신경망 (MLP)
```
입력 → [선형 변환 + 활성화 함수] × N → 출력
       (ReLU, tanh 등)
```

#### Scaled-cPIKAN (KAN)
```
입력 → [Chebyshev 다항식 전개] × N → 출력
       (학습 가능한 계수)
```

**장점:**
- 📈 더 정확한 함수 근사
- 🔬 물리적으로 의미 있는 기저 함수
- ⚡ 적은 파라미터로 높은 성능

---

## 📐 Chebyshev 다항식 기초

### 무엇인가요?

직교 다항식의 일종으로, 모든 함수를 근사할 수 있습니다.

**재귀 정의:**
```
T₀(x) = 1
T₁(x) = x
Tₙ₊₁(x) = 2x·Tₙ(x) - Tₙ₋₁(x)
```

**처음 몇 항:**
```
T₀(x) = 1
T₁(x) = x
T₂(x) = 2x² - 1
T₃(x) = 4x³ - 3x
T₄(x) = 8x⁴ - 8x² + 1
```

### 왜 사용하나요?

**함수 근사:**
```
f(x) ≈ a₀T₀(x) + a₁T₁(x) + a₂T₂(x) + ...
```

계수 `a₀, a₁, a₂, ...`를 학습하면 복잡한 함수를 표현할 수 있습니다!

### 시각화

```python
import torch
import matplotlib.pyplot as plt

def chebyshev_polynomial(x, k):
    """Chebyshev 다항식 Tₖ(x) 계산"""
    if k == 0:
        return torch.ones_like(x)
    elif k == 1:
        return x
    else:
        T_prev2 = torch.ones_like(x)  # T₀
        T_prev1 = x                    # T₁
        for _ in range(2, k + 1):
            T_current = 2 * x * T_prev1 - T_prev2
            T_prev2 = T_prev1
            T_prev1 = T_current
        return T_current

# 시각화
x = torch.linspace(-1, 1, 200)
plt.figure(figsize=(10, 6))

for k in range(5):
    T_k = chebyshev_polynomial(x, k)
    plt.plot(x.numpy(), T_k.numpy(), label=f'T_{k}(x)')

plt.xlabel('x')
plt.ylabel('Tₖ(x)')
plt.title('처음 5개의 Chebyshev 다항식')
plt.legend()
plt.grid(True, alpha=0.3)
plt.axhline(0, color='black', linewidth=0.5)
plt.axvline(0, color='black', linewidth=0.5)
plt.show()
```

---

## 🏗️ ChebyKANLayer 구조

### 레이어의 역할

하나의 KAN 레이어는 다음을 수행합니다:

```
입력 (batch, in_features) → Chebyshev 전개 → 출력 (batch, out_features)
```

### 수학적 표현

입력 `x ∈ ℝⁿ`에 대해:
```
yⱼ = Σᵢ Σₖ aⱼᵢₖ · Tₖ(xᵢ)
```

여기서:
- `yⱼ`: j번째 출력 뉴런
- `xᵢ`: i번째 입력 뉴런
- `Tₖ`: k차 Chebyshev 다항식
- `aⱼᵢₖ`: 학습 가능한 계수 (파라미터)

### 구현 살펴보기

```python
from src.models import ChebyKANLayer
import torch

# 레이어 생성
layer = ChebyKANLayer(
    in_features=2,      # 입력 차원
    out_features=5,     # 출력 차원
    degree=3            # Chebyshev 다항식 차수 (K)
)

# 입력 생성
x = torch.randn(10, 2)  # (batch=10, in_features=2)

# Forward pass
y = layer(x)

print(f"입력 shape: {x.shape}")
print(f"출력 shape: {y.shape}")
print(f"파라미터 개수: {layer.coefficients.numel()}")
```

**출력:**
```
입력 shape: torch.Size([10, 2])
출력 shape: torch.Size([10, 5])
파라미터 개수: 40  (= out_features × in_features × (degree + 1) = 5×2×4)
```

---

## 🏛️ Scaled_cPIKAN 전체 아키텍처

### 모델 구조

```
입력 (x, y, ...)
    ↓ [도메인 스케일링: [-1, 1]]
    ↓
ChebyKANLayer (입력 → 은닉층1)
    ↓
ChebyKANLayer (은닉층1 → 은닉층2)
    ↓
    ... (여러 은닉층)
    ↓
ChebyKANLayer (은닉층N → 출력)
    ↓
출력 (u, v, ...)
```

### 코드로 모델 만들기

#### 기본 예제

```python
from src.models import Scaled_cPIKAN
import torch

# 모델 생성
model = Scaled_cPIKAN(
    layers_hidden=[2, 20, 20, 1],  # [입력, 은닉1, 은닉2, 출력]
    degree=3,                       # Chebyshev 차수
    domain_min=torch.tensor([0.0, 0.0]),  # 도메인 최솟값
    domain_max=torch.tensor([1.0, 1.0])   # 도메인 최댓값
)

print(model)
```

**출력:**
```
Scaled_cPIKAN(
  (layers): ModuleList(
    (0): ChebyKANLayer(in=2, out=20, degree=3)
    (1): ChebyKANLayer(in=20, out=20, degree=3)
    (2): ChebyKANLayer(in=20, out=1, degree=3)
  )
)
```

### 파라미터 설명

#### `layers_hidden`

은닉층의 뉴런 개수를 리스트로 지정:

```python
layers_hidden=[2, 20, 20, 1]
```

- `2`: 입력 차원 (예: x, y 좌표)
- `20, 20`: 두 개의 은닉층, 각각 20개 뉴런
- `1`: 출력 차원 (예: 온도 u)

**권장 설정:**
- 1D 문제: `[1, 20, 20, 1]`
- 2D 문제: `[2, 50, 50, 1]`
- 3D 문제: `[3, 100, 100, 1]`

#### `degree`

Chebyshev 다항식의 최대 차수:

```python
degree=3  # T₀, T₁, T₂, T₃ 사용
```

**권장 설정:**
- 부드러운 함수: `degree=3~5`
- 복잡한 함수: `degree=5~7`
- 매우 복잡: `degree=7~10`

**주의:** `degree`가 크면 파라미터 수가 급증합니다!

#### `domain_min`, `domain_max`

문제의 도메인 범위:

```python
# 2D 사각형 도메인: [0,1] × [0,1]
domain_min=torch.tensor([0.0, 0.0])
domain_max=torch.tensor([1.0, 1.0])
```

---

## 🔄 도메인 스케일링

### 왜 필요한가요?

Chebyshev 다항식은 **[-1, 1]** 범위에서 정의됩니다!

**문제:** 도메인이 [0, 10]이면?  
**해결:** [-1, 1]로 아핀 변환!

### 수학적 변환

```
x_scaled = 2 * (x - x_min) / (x_max - x_min) - 1
```

**예시:**
```
도메인 [0, 10] → [-1, 1]
x=0   → -1
x=5   → 0
x=10  → +1
```

### 자동 처리

`Scaled_cPIKAN`은 자동으로 스케일링합니다:

```python
# 모델 내부에서 자동 처리
# 사용자는 원래 도메인의 좌표를 입력하면 됨!

x_input = torch.tensor([[0.5, 0.5], [0.8, 0.2]])  # 원래 도메인
u_output = model(x_input)  # 모델이 자동으로 스케일링
```

---

## 🔧 모델 사용 예제

### 전체 워크플로우

```python
import torch
from src.models import Scaled_cPIKAN

# 1. 모델 생성
model = Scaled_cPIKAN(
    layers_hidden=[2, 30, 30, 1],
    degree=4,
    domain_min=torch.tensor([0.0, 0.0]),
    domain_max=torch.tensor([1.0, 1.0])
)

# 2. GPU로 이동 (선택사항)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)
print(f"Device: {device}")

# 3. 입력 데이터 준비
x = torch.rand(100, 2).to(device)  # 100개 랜덤 포인트

# 4. Forward pass
u = model(x)

print(f"입력: {x.shape}")
print(f"출력: {u.shape}")

# 5. 파라미터 정보
total_params = sum(p.numel() for p in model.parameters())
print(f"총 파라미터 개수: {total_params}")
```

---

## 🧪 실습: 1D 문제 모델 정의

### 문제 설정

Helmholtz 방정식을 위한 모델 만들기:
```
입력: x ∈ [0, 1]
출력: u(x)
```

### 코드

```python
import torch
from src.models import Scaled_cPIKAN
import matplotlib.pyplot as plt

# 모델 생성
model = Scaled_cPIKAN(
    layers_hidden=[1, 20, 20, 1],  # 1D 입력, 1D 출력
    degree=3,
    domain_min=torch.tensor([0.0]),
    domain_max=torch.tensor([1.0])
)

# 테스트 입력
x_test = torch.linspace(0, 1, 100).unsqueeze(1)  # (100, 1)

# 초기 예측 (훈련 전)
with torch.no_grad():
    u_initial = model(x_test)

# 시각화
plt.figure(figsize=(10, 4))
plt.plot(x_test.numpy(), u_initial.numpy(), label='초기 예측 (훈련 전)')
plt.xlabel('x')
plt.ylabel('u(x)')
plt.title('모델 초기 상태')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

print(f"✅ 모델 생성 완료!")
print(f"   파라미터 개수: {sum(p.numel() for p in model.parameters())}")
```

---

## 🎛️ 하이퍼파라미터 튜닝

### 은닉층 개수

**더 많은 층:**
- ✅ 복잡한 함수 표현 가능
- ❌ 훈련 시간 증가
- ❌ 과적합 위험

**권장:**
- 간단한 문제: 2~3층
- 중간 복잡도: 3~5층
- 복잡한 문제: 5~10층

### 뉴런 개수

**더 많은 뉴런:**
- ✅ 표현력 향상
- ❌ 메모리 사용 증가

**권장:**
- 1D: 20~50
- 2D: 50~100
- 3D: 100~200

### Chebyshev 차수

**더 높은 차수:**
- ✅ 정확도 향상
- ❌ 파라미터 폭발적 증가
- ❌ 수치적 불안정성

**권장:**
- 시작: `degree=3`
- 필요시 증가: `degree=5`
- 최대: `degree=10`

### 예시 비교

```python
# 작은 모델 (빠름, 정확도 낮음)
model_small = Scaled_cPIKAN(
    layers_hidden=[2, 20, 1],
    degree=3,
    domain_min=torch.tensor([0.0, 0.0]),
    domain_max=torch.tensor([1.0, 1.0])
)

# 큰 모델 (느림, 정확도 높음)
model_large = Scaled_cPIKAN(
    layers_hidden=[2, 100, 100, 100, 1],
    degree=7,
    domain_min=torch.tensor([0.0, 0.0]),
    domain_max=torch.tensor([1.0, 1.0])
)

print(f"작은 모델 파라미터: {sum(p.numel() for p in model_small.parameters())}")
print(f"큰 모델 파라미터: {sum(p.numel() for p in model_large.parameters())}")
```

---

## 🎯 체크포인트

다음 항목을 모두 이해했으면 다음 단계로 진행하세요!

- [ ] Chebyshev 다항식의 재귀 정의 이해
- [ ] ChebyKANLayer의 동작 원리 파악
- [ ] Scaled_cPIKAN 모델 생성 성공
- [ ] 도메인 스케일링의 중요성 이해
- [ ] 하이퍼파라미터 의미 파악
- [ ] 파라미터 개수 계산 가능

---

## 🆘 자주 묻는 질문

### Q1: 일반 MLP 대신 KAN을 쓰는 이유는?

**A:** KAN은 함수 근사에 더 효율적입니다. 같은 정확도를 얻는 데 더 적은 파라미터가 필요합니다.

### Q2: Chebyshev 차수를 무한히 크게 하면?

**A:** 이론적으론 완벽한 근사가 가능하지만, 실제로는:
1. 파라미터가 너무 많아짐
2. 수치적 불안정 발생
3. 과적합 위험

**권장:** `degree=3~7`

### Q3: 도메인이 [-5, 5]인데 실수로 [0, 1]로 설정하면?

**A:** 모델이 잘못된 범위로 스케일링하여 예측이 부정확해집니다. 반드시 실제 도메인 범위를 정확히 입력하세요!

### Q4: GPU가 없으면 얼마나 느린가요?

**A:** 문제 크기에 따라 다르지만:
- 작은 1D 문제: 2~5배 느림
- 큰 3D 문제: 50~100배 느림

CPU로도 가능하지만 인내심이 필요합니다!

---

## 📚 추가 자료

- **Chebyshev 다항식 이론:** [Wikipedia](https://en.wikipedia.org/wiki/Chebyshev_polynomials)
- **KAN 논문:** Liu et al., "KAN: Kolmogorov-Arnold Networks" (2024)
- **PyTorch nn.Module 문서:** [https://pytorch.org/docs/stable/generated/torch.nn.Module.html](https://pytorch.org/docs/stable/generated/torch.nn.Module.html)

---

## 🎉 축하합니다!

모델 정의를 마스터했습니다! 이제 손실 함수를 정의할 준비가 되었습니다.

**다음 단계:**

👉 [4단계: 손실 함수로 이동하기](04_손실함수.md)

또는 [메인 가이드로 돌아가기](../사용자_가이드.md)

---

*마지막 업데이트: 2025년 1월*
