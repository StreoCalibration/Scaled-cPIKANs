# 4단계: 손실 함수 🟡

> **난이도:** 중급  
> **소요 시간:** 40분  
> **사전 지식:** 미적분 기초, 자동 미분 개념

---

## 💡 개념이 어렵다면?

<details>
<summary><b>📚 "PDE", "잔차", "손실"이라는 단어가 생소하신가요?</b></summary>

각 용어의 정확한 뜻부터 실생활 비유, 수학 공식, 실행 가능한 코드까지
**완전 초보자를 위한 상세 설명**을 준비했습니다!

👉 **[PDE 잔차 손실 - 완전 초보자 가이드](concepts/PDE_잔차_손실_상세설명.md)**

이 문서를 먼저 읽고 오시면 이 가이드가 훨씬 쉬워집니다! 😊

</details>

---

## 🎯 학습 목표

이 단계를 마치면 다음을 할 수 있습니다:

- ✅ PINN 손실 함수의 구성 요소 이해하기
- ✅ PDE 잔차 손실 계산하기
- ✅ 경계 조건 손실과 초기 조건 손실 정의하기
- ✅ PyTorch autograd로 미분 계산하기
- ✅ 다중 손실 가중치 조정하기

---

## 🧩 PINN 손실 함수의 구조

### 일반 딥러닝 vs PINN

**일반 딥러닝:**
```
Loss = MSE(예측, 정답)
```

**PINN:**
```
Loss = 잔차 손실 + 경계 조건 손실 + 초기 조건 손실
       ↓            ↓                 ↓
      방정식     경계에서 값        시작 시점 값
```

### 수학적 표현

```
L_total = λ₁·L_residual + λ₂·L_BC + λ₃·L_IC
```

여기서:
- `L_residual`: PDE 잔차 손실 (방정식을 얼마나 만족하는가)
- `L_BC`: 경계 조건 손실 (Boundary Condition)
- `L_IC`: 초기 조건 손실 (Initial Condition)
- `λ₁, λ₂, λ₃`: 가중치 (각 항목의 중요도)

### 🤔 왜 3가지 손실로 구성했을까?

<details>
<summary><b>💡 "왜 잔차, 경계, 초기 조건이 모두 필요한가요?"</b></summary>

단순히 "이렇게 하면 된다"가 아니라, **왜 3가지 손실이 모두 필요한지** 궁금하신가요?

수학적 정당성(Well-posed Problem), 물리적 의미, 실제 예제까지
**깊이 있는 설명**을 준비했습니다!

👉 **[PINN 손실 함수 구조의 이유](concepts/PINN_손실함수_구조의_이유.md)**

이 문서를 읽으면:
- Hadamard의 Well-posed Problem 이해
- 각 손실이 없으면 왜 문제가 생기는지 실험
- 1D 열 방정식으로 3가지 케이스 비교
- 자동차 여행 비유로 직관적 이해

</details>

---

## 📐 PDE 잔차 손실

> 💡 **용어가 어렵다면?** [PDE 잔차 손실 완전 정복하기](concepts/PDE_잔차_손실_상세설명.md)

### 개념 이해하기

PDE를 **잔차 형태**로 표현합니다:

**예시: Poisson 방정식**
```
원래 방정식: ∇²u = f
잔차 형태:   ∇²u - f = 0
```

**잔차 (Residual):**
```
R = ∇²u - f
```

잔차가 0에 가까울수록 방정식을 잘 만족합니다!

### 손실 함수

```
L_residual = (1/N) Σ |R(xᵢ)|²
```

N개의 콜로케이션 포인트에서 잔차의 평균 제곱 오차

### PyTorch로 미분 계산하기

#### 기본: 1차 미분

```python
import torch

# 입력 준비 (requires_grad=True 필수!)
x = torch.tensor([[0.5]], requires_grad=True)

# 모델 예측
u = model(x)

# 1차 미분 계산: du/dx
du_dx = torch.autograd.grad(
    outputs=u,
    inputs=x,
    grad_outputs=torch.ones_like(u),
    create_graph=True  # 2차 미분을 위해 필요
)[0]

print(f"u = {u.item():.4f}")
print(f"du/dx = {du_dx.item():.4f}")
```

#### 고급: 2차 미분

```python
# 2차 미분 계산: d²u/dx²
d2u_dx2 = torch.autograd.grad(
    outputs=du_dx,
    inputs=x,
    grad_outputs=torch.ones_like(du_dx),
    create_graph=True
)[0]

print(f"d²u/dx² = {d2u_dx2.item():.4f}")
```

### 간단한 예제: 1D Helmholtz 방정식

**방정식:**
```
d²u/dx² + k²u = f(x)
```

**잔차:**
```
R = d²u/dx² + k²u - f(x)
```

**기본 코드:**
```python
def helmholtz_residual(model, x, k, f):
    """
    Helmholtz 방정식 잔차 계산
    
    Args:
        model: PINN 모델
        x: 콜로케이션 포인트 (N, 1)
        k: 파수
        f: 소스 항 함수
    
    Returns:
        residual: 잔차 (N, 1)
    """
    x = x.requires_grad_(True)
    
    # 모델 예측
    u = model(x)
    
    # 1차 미분
    du_dx = torch.autograd.grad(
        outputs=u,
        inputs=x,
        grad_outputs=torch.ones_like(u),
        create_graph=True
    )[0]
    
    # 2차 미분
    d2u_dx2 = torch.autograd.grad(
        outputs=du_dx,
        inputs=x,
        grad_outputs=torch.ones_like(du_dx),
        create_graph=True
    )[0]
    
    # 잔차 계산
    residual = d2u_dx2 + k**2 * u - f(x)
    
    return residual

# 손실 계산
x_collocation = torch.linspace(0, 1, 100).unsqueeze(1).requires_grad_(True)
k = 1.0
f = lambda x: -k**2 * torch.sin(torch.pi * x)

residuals = helmholtz_residual(model, x_collocation, k, f)
loss_residual = torch.mean(residuals ** 2)

print(f"잔차 손실: {loss_residual.item():.6f}")
```

---

<details>
<summary><b>🎵 Helmholtz 방정식 완전 정복하기</b></summary>

위 예제는 기본적인 사용법만 보여줍니다.

**물리적 의미**, **완전한 구현**, **해석적 해와 비교**, **파수 k의 영향 실험** 등
처음부터 끝까지 모든 것을 담은 완전한 가이드를 준비했습니다!

� **[1D Helmholtz 방정식 완전 가이드](examples/Helmholtz_1D_완전_가이드.md)**

이 가이드에는:
- ✅ 음향파, 전자기파 등 물리적 배경 (실생활 예시 포함)
- ✅ 9단계 완전 구현 (모델 정의부터 시각화까지)
- ✅ 해석적 해 유도 및 검증
- ✅ 4가지 결과 시각화 (예측, 오차, 손실, 잔차)
- ✅ 파수 k 변화 실험 (자동화)
- ✅ 5가지 실습 과제
- ✅ 문제 해결 가이드

**복사-붙여넣기로 바로 실행 가능!** (약 1.5시간 소요)

</details>

---

## 🔲 경계 조건 손실

> 💡 **경계 조건이 처음이신가요?** [경계 조건 완전 가이드](concepts/경계_조건_완전_가이드.md)를 먼저 읽어보세요!

<details>
<summary><b>🔲 "Dirichlet, Neumann이 뭐죠? 왜 필요한가요?"</b></summary>

경계 조건은 PDE의 해를 유일하게 결정하는 필수 요소입니다!

**4가지 경계 조건의 모든 것**을 다룬 완전 가이드:

👉 **[경계 조건 완전 가이드](concepts/경계_조건_완전_가이드.md)**

이 가이드에는:
- ✅ 왜 경계 조건이 필요한가? (수학적/물리적 이유)
- ✅ Dirichlet vs Neumann vs Robin vs Periodic 차이
- ✅ 각각의 물리적 의미 (온도, 열 유속, 대류 등)
- ✅ 언제 어떤 것을 사용하는가?
- ✅ PINN 구현 방법 (autograd 활용)
- ✅ 4가지 경계 조건 비교 실험 (완전한 코드)

**복사-붙여넣기로 바로 실행 가능!** (약 1시간 소요)

</details>

---

### Dirichlet 경계 조건 (값 고정)

**정의:** 경계에서 값이 고정
```
u(x_boundary) = g(x_boundary)
```

**물리적 의미:**
- 🌡️ 온도 고정 (막대 끝이 얼음물)
- 🏗️ 변위 고정 (고정 지지)
- ⚡ 전위 고정 (접지)

**손실:**
```
L_BC = (1/N_BC) Σ |u(xᵢ) - g(xᵢ)|²
```

**코드:**
```python
def dirichlet_bc_loss(model, x_bc, u_bc):
    """
    Dirichlet 경계 조건 손실
    
    Args:
        model: PINN 모델
        x_bc: 경계 포인트 (N_BC, dim)
        u_bc: 경계 값 (N_BC, 1)
    
    Returns:
        loss: 경계 조건 손실
    """
    u_pred = model(x_bc)
    loss = torch.mean((u_pred - u_bc) ** 2)
    return loss

# 예시: u(0) = 0, u(1) = 0
x_bc = torch.tensor([[0.0], [1.0]])
u_bc = torch.tensor([[0.0], [0.0]])

loss_bc = dirichlet_bc_loss(model, x_bc, u_bc)
print(f"경계 조건 손실: {loss_bc.item():.6f}")
```

---

### Neumann 경계 조건 (미분 고정)

**정의:** 경계에서 미분이 고정
```
∂u/∂n|_boundary = h(x_boundary)
```

**물리적 의미:**
- 🌡️ 열 유속 지정 (단열: ∂u/∂n = 0)
- 🏗️ 힘 지정 (자유 단: ∂u/∂n = 0)
- 🌊 압력 구배

**손실:**
```
L_BC = (1/N_BC) Σ |∂u/∂n|ₓᵢ - h(xᵢ)|²
```

**코드:**
```python
def neumann_bc_loss(model, x_bc, dudn_bc):
    """
    Neumann 경계 조건 손실
    
    Args:
        model: PINN 모델
        x_bc: 경계 포인트 (N_BC, 1)
        dudn_bc: 경계 미분 값 (N_BC, 1)
    
    Returns:
        loss: Neumann 경계 조건 손실
    """
    x_bc = x_bc.requires_grad_(True)
    u = model(x_bc)
    
    # 미분 계산
    du_dx = torch.autograd.grad(
        outputs=u,
        inputs=x_bc,
        grad_outputs=torch.ones_like(u),
        create_graph=True
    )[0]
    
    # 손실
    loss = torch.mean((du_dx - dudn_bc) ** 2)
    return loss
```

---

## ⏰ 초기 조건 손실

### 정의

시간 의존 문제에서 t=0일 때의 조건:
```
u(x, t=0) = u₀(x)
```

**손실:**
```
L_IC = (1/N_IC) Σ |u(xᵢ, 0) - u₀(xᵢ)|²
```

### 코드

```python
def initial_condition_loss(model, x_ic, u_ic):
    """
    초기 조건 손실
    
    Args:
        model: PINN 모델
        x_ic: 초기 포인트 (x, t=0) (N_IC, 2)
        u_ic: 초기 값 (N_IC, 1)
    
    Returns:
        loss: 초기 조건 손실
    """
    u_pred = model(x_ic)
    loss = torch.mean((u_pred - u_ic) ** 2)
    return loss

# 예시: Heat equation u(x, 0) = sin(πx)
x_spatial = torch.linspace(0, 1, 100)
t_initial = torch.zeros(100)
x_ic = torch.stack([x_spatial, t_initial], dim=1)
u_ic = torch.sin(torch.pi * x_spatial).unsqueeze(1)

loss_ic = initial_condition_loss(model, x_ic, u_ic)
print(f"초기 조건 손실: {loss_ic.item():.6f}")
```

---

## ⚖️ 손실 가중치 조정

### 왜 필요한가요?

각 손실 항목의 **스케일**이 다를 수 있습니다!

**예시:**
```
L_residual = 100.0
L_BC = 0.01
L_IC = 0.001
```

가중치 없이 더하면 잔차 손실이 지배적입니다!

### 가중치 사용

```python
# 가중치 설정
lambda_residual = 1.0
lambda_bc = 100.0    # 경계 조건 강조
lambda_ic = 100.0    # 초기 조건 강조

# 총 손실
loss_total = (
    lambda_residual * loss_residual +
    lambda_bc * loss_bc +
    lambda_ic * loss_ic
)

print(f"총 손실: {loss_total.item():.6f}")
```

### 가중치 선택 팁

**경험적 규칙:**
1. 모든 항목이 비슷한 크기가 되도록 조정
2. 더 중요한 조건에 높은 가중치
3. 시행착오를 통해 최적화

**예시:**
```python
# 초기 훈련에서 각 손실 확인
with torch.no_grad():
    print(f"L_residual: {loss_residual.item():.6f}")
    print(f"L_BC: {loss_bc.item():.6f}")
    print(f"L_IC: {loss_ic.item():.6f}")

# 가중치 조정
# 목표: 모든 항목이 1.0 정도가 되도록
lambda_residual = 1.0 / loss_residual.item()
lambda_bc = 1.0 / loss_bc.item()
lambda_ic = 1.0 / loss_ic.item()
```

---

## 🏗️ PhysicsInformedLoss 클래스

### 프로젝트의 손실 클래스

`src/loss.py`에 구현되어 있습니다:

```python
from src.loss import PhysicsInformedLoss

# 손실 함수 생성
loss_fn = PhysicsInformedLoss(
    pde_fn=my_pde_residual,        # PDE 잔차 함수
    boundary_weight=10.0,          # 경계 조건 가중치
    initial_weight=10.0,           # 초기 조건 가중치
    data_weight=1.0                # 데이터 손실 가중치 (있는 경우)
)
```

### 사용 예제

```python
import torch
from src.loss import PhysicsInformedLoss

# PDE 잔차 함수 정의
def poisson_residual(model, x):
    """Poisson 방정식: ∇²u = f"""
    x = x.requires_grad_(True)
    u = model(x)
    
    # 2차 미분 (Laplacian)
    grad_u = torch.autograd.grad(
        outputs=u,
        inputs=x,
        grad_outputs=torch.ones_like(u),
        create_graph=True
    )[0]
    
    laplacian = 0
    for i in range(x.shape[1]):
        grad_u_i = grad_u[:, i:i+1]
        grad2_u_i = torch.autograd.grad(
            outputs=grad_u_i,
            inputs=x,
            grad_outputs=torch.ones_like(grad_u_i),
            create_graph=True
        )[0][:, i:i+1]
        laplacian += grad2_u_i
    
    # 소스 항
    f = -2 * torch.pi**2 * torch.sin(torch.pi * x[:, 0:1]) * torch.sin(torch.pi * x[:, 1:2])
    
    return laplacian - f

# 손실 함수 생성
loss_fn = PhysicsInformedLoss(
    pde_fn=poisson_residual,
    boundary_weight=10.0
)

# 데이터 준비
x_collocation = torch.rand(1000, 2, requires_grad=True)
x_boundary = torch.rand(100, 2)
u_boundary = torch.zeros(100, 1)

# 손실 계산
loss, metrics = loss_fn(
    model=model,
    x_pde=x_collocation,
    x_bc=x_boundary,
    u_bc=u_boundary
)

print(f"총 손실: {loss.item():.6f}")
print(f"메트릭: {metrics}")
```

---

## 🧪 실습: 완전한 손실 함수 구현

### 목표

1D Helmholtz 방정식을 위한 완전한 손실 함수 작성

### 전체 코드

```python
import torch
from src.models import Scaled_cPIKAN

def helmholtz_loss_function(model, x_collocation, x_bc, u_bc, k=1.0, lambda_bc=10.0):
    """
    Helmholtz 방정식 완전한 손실 함수
    
    방정식: d²u/dx² + k²u = 0
    경계 조건: u(0) = u(1) = 0
    """
    # 1. PDE 잔차 손실
    x_col = x_collocation.requires_grad_(True)
    u = model(x_col)
    
    du_dx = torch.autograd.grad(
        outputs=u, inputs=x_col,
        grad_outputs=torch.ones_like(u),
        create_graph=True
    )[0]
    
    d2u_dx2 = torch.autograd.grad(
        outputs=du_dx, inputs=x_col,
        grad_outputs=torch.ones_like(du_dx),
        create_graph=True
    )[0]
    
    residual = d2u_dx2 + k**2 * u
    loss_pde = torch.mean(residual ** 2)
    
    # 2. 경계 조건 손실
    u_bc_pred = model(x_bc)
    loss_bc = torch.mean((u_bc_pred - u_bc) ** 2)
    
    # 3. 총 손실
    loss_total = loss_pde + lambda_bc * loss_bc
    
    # 메트릭
    metrics = {
        'loss_pde': loss_pde.item(),
        'loss_bc': loss_bc.item(),
        'loss_total': loss_total.item()
    }
    
    return loss_total, metrics

# 모델 및 데이터 준비
model = Scaled_cPIKAN(
    layers_hidden=[1, 20, 20, 1],
    degree=3,
    domain_min=torch.tensor([0.0]),
    domain_max=torch.tensor([1.0])
)

x_collocation = torch.linspace(0.01, 0.99, 100).unsqueeze(1)
x_bc = torch.tensor([[0.0], [1.0]])
u_bc = torch.zeros(2, 1)

# 손실 계산
loss, metrics = helmholtz_loss_function(model, x_collocation, x_bc, u_bc)

print(f"PDE 손실: {metrics['loss_pde']:.6f}")
print(f"BC 손실: {metrics['loss_bc']:.6f}")
print(f"총 손실: {metrics['loss_total']:.6f}")
```

---

## 🔍 손실 함수 디버깅

### 체크리스트

#### 1. 그래디언트 확인

```python
# 손실에 대한 그래디언트가 있는지 확인
loss.backward()
for name, param in model.named_parameters():
    if param.grad is not None:
        print(f"{name}: grad norm = {param.grad.norm().item():.6f}")
    else:
        print(f"{name}: NO GRADIENT!")
```

#### 2. NaN/Inf 체크

```python
if torch.isnan(loss) or torch.isinf(loss):
    print("경고: 손실이 NaN 또는 Inf입니다!")
    print(f"L_pde: {loss_pde.item()}")
    print(f"L_bc: {loss_bc.item()}")
```

#### 3. 스케일 확인

```python
print(f"L_pde 스케일: {loss_pde.item():.2e}")
print(f"L_bc 스케일: {loss_bc.item():.2e}")
print(f"비율: {loss_pde.item() / loss_bc.item():.2f}")
```

**이상적:** 비율이 1~100 사이

---

## 🎯 체크포인트

다음 항목을 모두 이해했으면 다음 단계로 진행하세요!

- [ ] PDE 잔차 손실의 개념 이해
- [ ] `torch.autograd.grad` 사용법 숙지
- [ ] 1차, 2차 미분 계산 가능
- [ ] Dirichlet/Neumann 경계 조건 구현
- [ ] 초기 조건 손실 작성
- [ ] 손실 가중치 조정 방법 이해

---

## 🆘 자주 묻는 질문

### Q1: "RuntimeError: grad can be implicitly created only for scalar outputs"

**A:** `grad_outputs` 파라미터를 추가하세요:
```python
grad = torch.autograd.grad(
    outputs=u,
    inputs=x,
    grad_outputs=torch.ones_like(u)  ← 이것!
)
```

### Q2: 잔차 손실이 줄어들지 않아요

**A:** 다음을 확인하세요:
1. `requires_grad=True` 설정 확인
2. `create_graph=True` 설정 확인
3. 학습률이 너무 작지 않은지 확인

> 💡 **잔차가 뭔지 잘 모르겠다면?** [기초부터 배우기](concepts/PDE_잔차_손실_상세설명.md#-part-1-단어-하나하나-분해하기)

### Q3: 경계 조건이 만족되지 않아요

**A:** `lambda_bc` 가중치를 증가시키세요 (예: 10 → 100).

### Q4: 2D Laplacian은 어떻게 계산하나요?

**A:** 각 차원에 대해 2차 미분을 계산하고 합산:
```python
laplacian = d2u_dx2 + d2u_dy2
```

---

## � 특수 응용: 물리 기반 데이터 일치 손실

<details>
<summary><b>🌊 위상 천이 간섭법에서 물리 모델 활용</b></summary>

이 프로젝트의 주요 응용 중 하나는 **레이저 간섭 패턴 → 3D 높이 복원**입니다.

여기서는 물리 모델이 두 가지 방식으로 사용됩니다:

### 1️⃣ 순방향 모델 (데이터 생성)
```python
# 높이 → 위상 → Bucket 이미지
phase = (4 * np.pi * height) / wavelength
bucket = A + B * np.cos(phase + delta)
```

### 2️⃣ 역방향 검증 (손실 함수)
```python
# 신경망이 예측한 높이가 물리적으로 올바른가?
predicted_height = model(buckets)  # 신경망 추론

# 예측한 높이로 Bucket을 재생성
predicted_phase = (4 * np.pi * predicted_height) / wavelength
predicted_buckets = A + B * torch.cos(predicted_phase + delta)

# 실제 측정 Bucket과 비교
loss = MSE(predicted_buckets, real_buckets)
```

**핵심 아이디어:**
- ✅ 신경망이 예측한 높이가 맞다면, 그 높이로 생성한 Bucket 이미지가 실제 측정값과 일치해야 함
- ✅ 이는 물리 법칙을 암묵적으로 학습에 활용하는 것 (Physics-Informed)

### 구현 예시 (src/loss.py)

```python
class UNetPhysicsLoss(nn.Module):
    """
    UNet의 물리 기반 손실 함수.
    
    예측한 높이로 Bucket을 재생성하고, 실제 Bucket과 비교.
    """
    def __init__(self, wavelengths, delta_phases, ...):
        self.wavelengths = wavelengths  # [5.0, 5.5, 6.05, 6.655] μm
        self.delta_phases = delta_phases  # [0, π/2, π, 3π/2]
        ...
    
    def forward(self, predicted_height, input_buckets, ...):
        """
        Args:
            predicted_height: 신경망 출력 (B, 1, H, W)
            input_buckets: 실제 측정 (B, 16, H, W)
        """
        # 예측 높이 → 재생성 Bucket
        reconstructed_buckets = []
        for i, λ in enumerate(self.wavelengths):
            for j, δ in enumerate(self.delta_phases):
                phase = (4 * torch.pi * predicted_height) / λ
                bucket = A + B * torch.cos(phase + δ)
                reconstructed_buckets.append(bucket)
        
        # 물리적 일치도
        data_loss = F.mse_loss(
            torch.cat(reconstructed_buckets, dim=1),
            input_buckets
        )
        
        return total_loss, {"data_consistency": data_loss, ...}
```

**이 방식의 장점:**
1. **물리 법칙 준수**: 신경망이 물리적으로 불가능한 높이를 예측하면 손실이 커짐
2. **적은 데이터**: 물리 모델이 일종의 정규화(regularization) 역할
3. **해석 가능성**: 예측 높이가 왜 틀렸는지 물리적 관점에서 분석 가능

**완전한 배경 지식:**
👉 **[위상 천이 간섭법 완전 가이드](concepts/위상천이간섭법_완전_가이드.md)**

이 가이드에서:
- ✅ 위상-높이 관계식 유도
- ✅ Bucket 생성 알고리즘
- ✅ 다중 파장 기법
- ✅ 물리 손실 함수 설계 철학

</details>

---

## �📚 추가 자료

### 공식 문서
- **PyTorch Autograd 튜토리얼:** [https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html)
- **PINN 원논문:** Raissi et al., "Physics-informed neural networks" (2019)

### 프로젝트 개념 가이드
- **[PDE 잔차 손실 완전 가이드](concepts/PDE_잔차_손실_상세설명.md)** - 초보자를 위한 상세 설명
  - 각 용어의 정확한 뜻
  - 실생활 비유와 예시
  - 단계별 수학 설명
  - 실행 가능한 코드
  - 연습 문제와 해답

---

## 🎉 축하합니다!

손실 함수를 마스터했습니다! 이제 모델을 훈련할 준비가 되었습니다.

**다음 단계:**

👉 [5단계: 훈련 과정으로 이동하기](05_훈련과정.md)

또는 [메인 가이드로 돌아가기](../사용자_가이드.md)

---

*마지막 업데이트: 2025년 1월*
