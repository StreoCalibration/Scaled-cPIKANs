# 4단계: 손실 함수 🟡

> **난이도:** 중급  
> **소요 시간:** 40분  
> **사전 지식:** 미적분 기초, 자동 미분 개념

---

## 🎯 학습 목표

이 단계를 마치면 다음을 할 수 있습니다:

- ✅ PINN 손실 함수의 구성 요소 이해하기
- ✅ PDE 잔차 손실 계산하기
- ✅ 경계 조건 손실과 초기 조건 손실 정의하기
- ✅ PyTorch autograd로 미분 계산하기
- ✅ 다중 손실 가중치 조정하기

---

## 🧩 PINN 손실 함수의 구조

### 일반 딥러닝 vs PINN

**일반 딥러닝:**
```
Loss = MSE(예측, 정답)
```

**PINN:**
```
Loss = 잔차 손실 + 경계 조건 손실 + 초기 조건 손실
       ↓            ↓                 ↓
      방정식     경계에서 값        시작 시점 값
```

### 수학적 표현

```
L_total = λ₁·L_residual + λ₂·L_BC + λ₃·L_IC
```

여기서:
- `L_residual`: PDE 잔차 손실 (방정식을 얼마나 만족하는가)
- `L_BC`: 경계 조건 손실 (Boundary Condition)
- `L_IC`: 초기 조건 손실 (Initial Condition)
- `λ₁, λ₂, λ₃`: 가중치 (각 항목의 중요도)

---

## 📐 PDE 잔차 손실

### 개념 이해하기

PDE를 **잔차 형태**로 표현합니다:

**예시: Poisson 방정식**
```
원래 방정식: ∇²u = f
잔차 형태:   ∇²u - f = 0
```

**잔차 (Residual):**
```
R = ∇²u - f
```

잔차가 0에 가까울수록 방정식을 잘 만족합니다!

### 손실 함수

```
L_residual = (1/N) Σ |R(xᵢ)|²
```

N개의 콜로케이션 포인트에서 잔차의 평균 제곱 오차

### PyTorch로 미분 계산하기

#### 기본: 1차 미분

```python
import torch

# 입력 준비 (requires_grad=True 필수!)
x = torch.tensor([[0.5]], requires_grad=True)

# 모델 예측
u = model(x)

# 1차 미분 계산: du/dx
du_dx = torch.autograd.grad(
    outputs=u,
    inputs=x,
    grad_outputs=torch.ones_like(u),
    create_graph=True  # 2차 미분을 위해 필요
)[0]

print(f"u = {u.item():.4f}")
print(f"du/dx = {du_dx.item():.4f}")
```

#### 고급: 2차 미분

```python
# 2차 미분 계산: d²u/dx²
d2u_dx2 = torch.autograd.grad(
    outputs=du_dx,
    inputs=x,
    grad_outputs=torch.ones_like(du_dx),
    create_graph=True
)[0]

print(f"d²u/dx² = {d2u_dx2.item():.4f}")
```

### 완전한 예제: 1D Helmholtz

**방정식:**
```
d²u/dx² + k²u = f(x)
```

**잔차:**
```
R = d²u/dx² + k²u - f(x)
```

**코드:**
```python
def helmholtz_residual(model, x, k, f):
    """
    Helmholtz 방정식 잔차 계산
    
    Args:
        model: PINN 모델
        x: 콜로케이션 포인트 (N, 1)
        k: 파수
        f: 소스 항 함수
    
    Returns:
        residual: 잔차 (N, 1)
    """
    x = x.requires_grad_(True)
    
    # 모델 예측
    u = model(x)
    
    # 1차 미분
    du_dx = torch.autograd.grad(
        outputs=u,
        inputs=x,
        grad_outputs=torch.ones_like(u),
        create_graph=True
    )[0]
    
    # 2차 미분
    d2u_dx2 = torch.autograd.grad(
        outputs=du_dx,
        inputs=x,
        grad_outputs=torch.ones_like(du_dx),
        create_graph=True
    )[0]
    
    # 잔차 계산
    residual = d2u_dx2 + k**2 * u - f(x)
    
    return residual

# 손실 계산
x_collocation = torch.linspace(0, 1, 100).unsqueeze(1).requires_grad_(True)
k = 1.0
f = lambda x: -k**2 * torch.sin(torch.pi * x)

residuals = helmholtz_residual(model, x_collocation, k, f)
loss_residual = torch.mean(residuals ** 2)

print(f"잔차 손실: {loss_residual.item():.6f}")
```

---

## 🔲 경계 조건 손실

### Dirichlet 경계 조건

**정의:** 경계에서 값이 고정
```
u(x_boundary) = g(x_boundary)
```

**손실:**
```
L_BC = (1/N_BC) Σ |u(xᵢ) - g(xᵢ)|²
```

**코드:**
```python
def dirichlet_bc_loss(model, x_bc, u_bc):
    """
    Dirichlet 경계 조건 손실
    
    Args:
        model: PINN 모델
        x_bc: 경계 포인트 (N_BC, dim)
        u_bc: 경계 값 (N_BC, 1)
    
    Returns:
        loss: 경계 조건 손실
    """
    u_pred = model(x_bc)
    loss = torch.mean((u_pred - u_bc) ** 2)
    return loss

# 예시: u(0) = 0, u(1) = 0
x_bc = torch.tensor([[0.0], [1.0]])
u_bc = torch.tensor([[0.0], [0.0]])

loss_bc = dirichlet_bc_loss(model, x_bc, u_bc)
print(f"경계 조건 손실: {loss_bc.item():.6f}")
```

### Neumann 경계 조건

**정의:** 경계에서 미분이 고정
```
∂u/∂n|_boundary = h(x_boundary)
```

**손실:**
```
L_BC = (1/N_BC) Σ |∂u/∂n|ₓᵢ - h(xᵢ)|²
```

**코드:**
```python
def neumann_bc_loss(model, x_bc, dudn_bc):
    """
    Neumann 경계 조건 손실
    
    Args:
        model: PINN 모델
        x_bc: 경계 포인트 (N_BC, 1)
        dudn_bc: 경계 미분 값 (N_BC, 1)
    
    Returns:
        loss: Neumann 경계 조건 손실
    """
    x_bc = x_bc.requires_grad_(True)
    u = model(x_bc)
    
    # 미분 계산
    du_dx = torch.autograd.grad(
        outputs=u,
        inputs=x_bc,
        grad_outputs=torch.ones_like(u),
        create_graph=True
    )[0]
    
    # 손실
    loss = torch.mean((du_dx - dudn_bc) ** 2)
    return loss
```

---

## ⏰ 초기 조건 손실

### 정의

시간 의존 문제에서 t=0일 때의 조건:
```
u(x, t=0) = u₀(x)
```

**손실:**
```
L_IC = (1/N_IC) Σ |u(xᵢ, 0) - u₀(xᵢ)|²
```

### 코드

```python
def initial_condition_loss(model, x_ic, u_ic):
    """
    초기 조건 손실
    
    Args:
        model: PINN 모델
        x_ic: 초기 포인트 (x, t=0) (N_IC, 2)
        u_ic: 초기 값 (N_IC, 1)
    
    Returns:
        loss: 초기 조건 손실
    """
    u_pred = model(x_ic)
    loss = torch.mean((u_pred - u_ic) ** 2)
    return loss

# 예시: Heat equation u(x, 0) = sin(πx)
x_spatial = torch.linspace(0, 1, 100)
t_initial = torch.zeros(100)
x_ic = torch.stack([x_spatial, t_initial], dim=1)
u_ic = torch.sin(torch.pi * x_spatial).unsqueeze(1)

loss_ic = initial_condition_loss(model, x_ic, u_ic)
print(f"초기 조건 손실: {loss_ic.item():.6f}")
```

---

## ⚖️ 손실 가중치 조정

### 왜 필요한가요?

각 손실 항목의 **스케일**이 다를 수 있습니다!

**예시:**
```
L_residual = 100.0
L_BC = 0.01
L_IC = 0.001
```

가중치 없이 더하면 잔차 손실이 지배적입니다!

### 가중치 사용

```python
# 가중치 설정
lambda_residual = 1.0
lambda_bc = 100.0    # 경계 조건 강조
lambda_ic = 100.0    # 초기 조건 강조

# 총 손실
loss_total = (
    lambda_residual * loss_residual +
    lambda_bc * loss_bc +
    lambda_ic * loss_ic
)

print(f"총 손실: {loss_total.item():.6f}")
```

### 가중치 선택 팁

**경험적 규칙:**
1. 모든 항목이 비슷한 크기가 되도록 조정
2. 더 중요한 조건에 높은 가중치
3. 시행착오를 통해 최적화

**예시:**
```python
# 초기 훈련에서 각 손실 확인
with torch.no_grad():
    print(f"L_residual: {loss_residual.item():.6f}")
    print(f"L_BC: {loss_bc.item():.6f}")
    print(f"L_IC: {loss_ic.item():.6f}")

# 가중치 조정
# 목표: 모든 항목이 1.0 정도가 되도록
lambda_residual = 1.0 / loss_residual.item()
lambda_bc = 1.0 / loss_bc.item()
lambda_ic = 1.0 / loss_ic.item()
```

---

## 🏗️ PhysicsInformedLoss 클래스

### 프로젝트의 손실 클래스

`src/loss.py`에 구현되어 있습니다:

```python
from src.loss import PhysicsInformedLoss

# 손실 함수 생성
loss_fn = PhysicsInformedLoss(
    pde_fn=my_pde_residual,        # PDE 잔차 함수
    boundary_weight=10.0,          # 경계 조건 가중치
    initial_weight=10.0,           # 초기 조건 가중치
    data_weight=1.0                # 데이터 손실 가중치 (있는 경우)
)
```

### 사용 예제

```python
import torch
from src.loss import PhysicsInformedLoss

# PDE 잔차 함수 정의
def poisson_residual(model, x):
    """Poisson 방정식: ∇²u = f"""
    x = x.requires_grad_(True)
    u = model(x)
    
    # 2차 미분 (Laplacian)
    grad_u = torch.autograd.grad(
        outputs=u,
        inputs=x,
        grad_outputs=torch.ones_like(u),
        create_graph=True
    )[0]
    
    laplacian = 0
    for i in range(x.shape[1]):
        grad_u_i = grad_u[:, i:i+1]
        grad2_u_i = torch.autograd.grad(
            outputs=grad_u_i,
            inputs=x,
            grad_outputs=torch.ones_like(grad_u_i),
            create_graph=True
        )[0][:, i:i+1]
        laplacian += grad2_u_i
    
    # 소스 항
    f = -2 * torch.pi**2 * torch.sin(torch.pi * x[:, 0:1]) * torch.sin(torch.pi * x[:, 1:2])
    
    return laplacian - f

# 손실 함수 생성
loss_fn = PhysicsInformedLoss(
    pde_fn=poisson_residual,
    boundary_weight=10.0
)

# 데이터 준비
x_collocation = torch.rand(1000, 2, requires_grad=True)
x_boundary = torch.rand(100, 2)
u_boundary = torch.zeros(100, 1)

# 손실 계산
loss, metrics = loss_fn(
    model=model,
    x_pde=x_collocation,
    x_bc=x_boundary,
    u_bc=u_boundary
)

print(f"총 손실: {loss.item():.6f}")
print(f"메트릭: {metrics}")
```

---

## 🧪 실습: 완전한 손실 함수 구현

### 목표

1D Helmholtz 방정식을 위한 완전한 손실 함수 작성

### 전체 코드

```python
import torch
from src.models import Scaled_cPIKAN

def helmholtz_loss_function(model, x_collocation, x_bc, u_bc, k=1.0, lambda_bc=10.0):
    """
    Helmholtz 방정식 완전한 손실 함수
    
    방정식: d²u/dx² + k²u = 0
    경계 조건: u(0) = u(1) = 0
    """
    # 1. PDE 잔차 손실
    x_col = x_collocation.requires_grad_(True)
    u = model(x_col)
    
    du_dx = torch.autograd.grad(
        outputs=u, inputs=x_col,
        grad_outputs=torch.ones_like(u),
        create_graph=True
    )[0]
    
    d2u_dx2 = torch.autograd.grad(
        outputs=du_dx, inputs=x_col,
        grad_outputs=torch.ones_like(du_dx),
        create_graph=True
    )[0]
    
    residual = d2u_dx2 + k**2 * u
    loss_pde = torch.mean(residual ** 2)
    
    # 2. 경계 조건 손실
    u_bc_pred = model(x_bc)
    loss_bc = torch.mean((u_bc_pred - u_bc) ** 2)
    
    # 3. 총 손실
    loss_total = loss_pde + lambda_bc * loss_bc
    
    # 메트릭
    metrics = {
        'loss_pde': loss_pde.item(),
        'loss_bc': loss_bc.item(),
        'loss_total': loss_total.item()
    }
    
    return loss_total, metrics

# 모델 및 데이터 준비
model = Scaled_cPIKAN(
    layers_hidden=[1, 20, 20, 1],
    degree=3,
    domain_min=torch.tensor([0.0]),
    domain_max=torch.tensor([1.0])
)

x_collocation = torch.linspace(0.01, 0.99, 100).unsqueeze(1)
x_bc = torch.tensor([[0.0], [1.0]])
u_bc = torch.zeros(2, 1)

# 손실 계산
loss, metrics = helmholtz_loss_function(model, x_collocation, x_bc, u_bc)

print(f"PDE 손실: {metrics['loss_pde']:.6f}")
print(f"BC 손실: {metrics['loss_bc']:.6f}")
print(f"총 손실: {metrics['loss_total']:.6f}")
```

---

## 🔍 손실 함수 디버깅

### 체크리스트

#### 1. 그래디언트 확인

```python
# 손실에 대한 그래디언트가 있는지 확인
loss.backward()
for name, param in model.named_parameters():
    if param.grad is not None:
        print(f"{name}: grad norm = {param.grad.norm().item():.6f}")
    else:
        print(f"{name}: NO GRADIENT!")
```

#### 2. NaN/Inf 체크

```python
if torch.isnan(loss) or torch.isinf(loss):
    print("경고: 손실이 NaN 또는 Inf입니다!")
    print(f"L_pde: {loss_pde.item()}")
    print(f"L_bc: {loss_bc.item()}")
```

#### 3. 스케일 확인

```python
print(f"L_pde 스케일: {loss_pde.item():.2e}")
print(f"L_bc 스케일: {loss_bc.item():.2e}")
print(f"비율: {loss_pde.item() / loss_bc.item():.2f}")
```

**이상적:** 비율이 1~100 사이

---

## 🎯 체크포인트

다음 항목을 모두 이해했으면 다음 단계로 진행하세요!

- [ ] PDE 잔차 손실의 개념 이해
- [ ] `torch.autograd.grad` 사용법 숙지
- [ ] 1차, 2차 미분 계산 가능
- [ ] Dirichlet/Neumann 경계 조건 구현
- [ ] 초기 조건 손실 작성
- [ ] 손실 가중치 조정 방법 이해

---

## 🆘 자주 묻는 질문

### Q1: "RuntimeError: grad can be implicitly created only for scalar outputs"

**A:** `grad_outputs` 파라미터를 추가하세요:
```python
grad = torch.autograd.grad(
    outputs=u,
    inputs=x,
    grad_outputs=torch.ones_like(u)  ← 이것!
)
```

### Q2: 잔차 손실이 줄어들지 않아요

**A:** 다음을 확인하세요:
1. `requires_grad=True` 설정 확인
2. `create_graph=True` 설정 확인
3. 학습률이 너무 작지 않은지 확인

### Q3: 경계 조건이 만족되지 않아요

**A:** `lambda_bc` 가중치를 증가시키세요 (예: 10 → 100).

### Q4: 2D Laplacian은 어떻게 계산하나요?

**A:** 각 차원에 대해 2차 미분을 계산하고 합산:
```python
laplacian = d2u_dx2 + d2u_dy2
```

---

## 📚 추가 자료

- **PyTorch Autograd 튜토리얼:** [https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html)
- **PINN 원논문:** Raissi et al., "Physics-informed neural networks" (2019)

---

## 🎉 축하합니다!

손실 함수를 마스터했습니다! 이제 모델을 훈련할 준비가 되었습니다.

**다음 단계:**

👉 [5단계: 훈련 과정으로 이동하기](05_훈련과정.md)

또는 [메인 가이드로 돌아가기](../사용자_가이드.md)

---

*마지막 업데이트: 2025년 1월*
