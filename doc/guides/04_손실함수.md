# 4ë‹¨ê³„: ì†ì‹¤ í•¨ìˆ˜ ğŸŸ¡

> **ë‚œì´ë„:** ì¤‘ê¸‰  
> **ì†Œìš” ì‹œê°„:** 40ë¶„  
> **ì‚¬ì „ ì§€ì‹:** ë¯¸ì ë¶„ ê¸°ì´ˆ, ìë™ ë¯¸ë¶„ ê°œë…

---

## ğŸ¯ í•™ìŠµ ëª©í‘œ

ì´ ë‹¨ê³„ë¥¼ ë§ˆì¹˜ë©´ ë‹¤ìŒì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

- âœ… PINN ì†ì‹¤ í•¨ìˆ˜ì˜ êµ¬ì„± ìš”ì†Œ ì´í•´í•˜ê¸°
- âœ… PDE ì”ì°¨ ì†ì‹¤ ê³„ì‚°í•˜ê¸°
- âœ… ê²½ê³„ ì¡°ê±´ ì†ì‹¤ê³¼ ì´ˆê¸° ì¡°ê±´ ì†ì‹¤ ì •ì˜í•˜ê¸°
- âœ… PyTorch autogradë¡œ ë¯¸ë¶„ ê³„ì‚°í•˜ê¸°
- âœ… ë‹¤ì¤‘ ì†ì‹¤ ê°€ì¤‘ì¹˜ ì¡°ì •í•˜ê¸°

---

## ğŸ§© PINN ì†ì‹¤ í•¨ìˆ˜ì˜ êµ¬ì¡°

### ì¼ë°˜ ë”¥ëŸ¬ë‹ vs PINN

**ì¼ë°˜ ë”¥ëŸ¬ë‹:**
```
Loss = MSE(ì˜ˆì¸¡, ì •ë‹µ)
```

**PINN:**
```
Loss = ì”ì°¨ ì†ì‹¤ + ê²½ê³„ ì¡°ê±´ ì†ì‹¤ + ì´ˆê¸° ì¡°ê±´ ì†ì‹¤
       â†“            â†“                 â†“
      ë°©ì •ì‹     ê²½ê³„ì—ì„œ ê°’        ì‹œì‘ ì‹œì  ê°’
```

### ìˆ˜í•™ì  í‘œí˜„

```
L_total = Î»â‚Â·L_residual + Î»â‚‚Â·L_BC + Î»â‚ƒÂ·L_IC
```

ì—¬ê¸°ì„œ:
- `L_residual`: PDE ì”ì°¨ ì†ì‹¤ (ë°©ì •ì‹ì„ ì–¼ë§ˆë‚˜ ë§Œì¡±í•˜ëŠ”ê°€)
- `L_BC`: ê²½ê³„ ì¡°ê±´ ì†ì‹¤ (Boundary Condition)
- `L_IC`: ì´ˆê¸° ì¡°ê±´ ì†ì‹¤ (Initial Condition)
- `Î»â‚, Î»â‚‚, Î»â‚ƒ`: ê°€ì¤‘ì¹˜ (ê° í•­ëª©ì˜ ì¤‘ìš”ë„)

---

## ğŸ“ PDE ì”ì°¨ ì†ì‹¤

### ê°œë… ì´í•´í•˜ê¸°

PDEë¥¼ **ì”ì°¨ í˜•íƒœ**ë¡œ í‘œí˜„í•©ë‹ˆë‹¤:

**ì˜ˆì‹œ: Poisson ë°©ì •ì‹**
```
ì›ë˜ ë°©ì •ì‹: âˆ‡Â²u = f
ì”ì°¨ í˜•íƒœ:   âˆ‡Â²u - f = 0
```

**ì”ì°¨ (Residual):**
```
R = âˆ‡Â²u - f
```

ì”ì°¨ê°€ 0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ë°©ì •ì‹ì„ ì˜ ë§Œì¡±í•©ë‹ˆë‹¤!

### ì†ì‹¤ í•¨ìˆ˜

```
L_residual = (1/N) Î£ |R(xáµ¢)|Â²
```

Nê°œì˜ ì½œë¡œì¼€ì´ì…˜ í¬ì¸íŠ¸ì—ì„œ ì”ì°¨ì˜ í‰ê·  ì œê³± ì˜¤ì°¨

### PyTorchë¡œ ë¯¸ë¶„ ê³„ì‚°í•˜ê¸°

#### ê¸°ë³¸: 1ì°¨ ë¯¸ë¶„

```python
import torch

# ì…ë ¥ ì¤€ë¹„ (requires_grad=True í•„ìˆ˜!)
x = torch.tensor([[0.5]], requires_grad=True)

# ëª¨ë¸ ì˜ˆì¸¡
u = model(x)

# 1ì°¨ ë¯¸ë¶„ ê³„ì‚°: du/dx
du_dx = torch.autograd.grad(
    outputs=u,
    inputs=x,
    grad_outputs=torch.ones_like(u),
    create_graph=True  # 2ì°¨ ë¯¸ë¶„ì„ ìœ„í•´ í•„ìš”
)[0]

print(f"u = {u.item():.4f}")
print(f"du/dx = {du_dx.item():.4f}")
```

#### ê³ ê¸‰: 2ì°¨ ë¯¸ë¶„

```python
# 2ì°¨ ë¯¸ë¶„ ê³„ì‚°: dÂ²u/dxÂ²
d2u_dx2 = torch.autograd.grad(
    outputs=du_dx,
    inputs=x,
    grad_outputs=torch.ones_like(du_dx),
    create_graph=True
)[0]

print(f"dÂ²u/dxÂ² = {d2u_dx2.item():.4f}")
```

### ì™„ì „í•œ ì˜ˆì œ: 1D Helmholtz

**ë°©ì •ì‹:**
```
dÂ²u/dxÂ² + kÂ²u = f(x)
```

**ì”ì°¨:**
```
R = dÂ²u/dxÂ² + kÂ²u - f(x)
```

**ì½”ë“œ:**
```python
def helmholtz_residual(model, x, k, f):
    """
    Helmholtz ë°©ì •ì‹ ì”ì°¨ ê³„ì‚°
    
    Args:
        model: PINN ëª¨ë¸
        x: ì½œë¡œì¼€ì´ì…˜ í¬ì¸íŠ¸ (N, 1)
        k: íŒŒìˆ˜
        f: ì†ŒìŠ¤ í•­ í•¨ìˆ˜
    
    Returns:
        residual: ì”ì°¨ (N, 1)
    """
    x = x.requires_grad_(True)
    
    # ëª¨ë¸ ì˜ˆì¸¡
    u = model(x)
    
    # 1ì°¨ ë¯¸ë¶„
    du_dx = torch.autograd.grad(
        outputs=u,
        inputs=x,
        grad_outputs=torch.ones_like(u),
        create_graph=True
    )[0]
    
    # 2ì°¨ ë¯¸ë¶„
    d2u_dx2 = torch.autograd.grad(
        outputs=du_dx,
        inputs=x,
        grad_outputs=torch.ones_like(du_dx),
        create_graph=True
    )[0]
    
    # ì”ì°¨ ê³„ì‚°
    residual = d2u_dx2 + k**2 * u - f(x)
    
    return residual

# ì†ì‹¤ ê³„ì‚°
x_collocation = torch.linspace(0, 1, 100).unsqueeze(1).requires_grad_(True)
k = 1.0
f = lambda x: -k**2 * torch.sin(torch.pi * x)

residuals = helmholtz_residual(model, x_collocation, k, f)
loss_residual = torch.mean(residuals ** 2)

print(f"ì”ì°¨ ì†ì‹¤: {loss_residual.item():.6f}")
```

---

## ğŸ”² ê²½ê³„ ì¡°ê±´ ì†ì‹¤

### Dirichlet ê²½ê³„ ì¡°ê±´

**ì •ì˜:** ê²½ê³„ì—ì„œ ê°’ì´ ê³ ì •
```
u(x_boundary) = g(x_boundary)
```

**ì†ì‹¤:**
```
L_BC = (1/N_BC) Î£ |u(xáµ¢) - g(xáµ¢)|Â²
```

**ì½”ë“œ:**
```python
def dirichlet_bc_loss(model, x_bc, u_bc):
    """
    Dirichlet ê²½ê³„ ì¡°ê±´ ì†ì‹¤
    
    Args:
        model: PINN ëª¨ë¸
        x_bc: ê²½ê³„ í¬ì¸íŠ¸ (N_BC, dim)
        u_bc: ê²½ê³„ ê°’ (N_BC, 1)
    
    Returns:
        loss: ê²½ê³„ ì¡°ê±´ ì†ì‹¤
    """
    u_pred = model(x_bc)
    loss = torch.mean((u_pred - u_bc) ** 2)
    return loss

# ì˜ˆì‹œ: u(0) = 0, u(1) = 0
x_bc = torch.tensor([[0.0], [1.0]])
u_bc = torch.tensor([[0.0], [0.0]])

loss_bc = dirichlet_bc_loss(model, x_bc, u_bc)
print(f"ê²½ê³„ ì¡°ê±´ ì†ì‹¤: {loss_bc.item():.6f}")
```

### Neumann ê²½ê³„ ì¡°ê±´

**ì •ì˜:** ê²½ê³„ì—ì„œ ë¯¸ë¶„ì´ ê³ ì •
```
âˆ‚u/âˆ‚n|_boundary = h(x_boundary)
```

**ì†ì‹¤:**
```
L_BC = (1/N_BC) Î£ |âˆ‚u/âˆ‚n|â‚“áµ¢ - h(xáµ¢)|Â²
```

**ì½”ë“œ:**
```python
def neumann_bc_loss(model, x_bc, dudn_bc):
    """
    Neumann ê²½ê³„ ì¡°ê±´ ì†ì‹¤
    
    Args:
        model: PINN ëª¨ë¸
        x_bc: ê²½ê³„ í¬ì¸íŠ¸ (N_BC, 1)
        dudn_bc: ê²½ê³„ ë¯¸ë¶„ ê°’ (N_BC, 1)
    
    Returns:
        loss: Neumann ê²½ê³„ ì¡°ê±´ ì†ì‹¤
    """
    x_bc = x_bc.requires_grad_(True)
    u = model(x_bc)
    
    # ë¯¸ë¶„ ê³„ì‚°
    du_dx = torch.autograd.grad(
        outputs=u,
        inputs=x_bc,
        grad_outputs=torch.ones_like(u),
        create_graph=True
    )[0]
    
    # ì†ì‹¤
    loss = torch.mean((du_dx - dudn_bc) ** 2)
    return loss
```

---

## â° ì´ˆê¸° ì¡°ê±´ ì†ì‹¤

### ì •ì˜

ì‹œê°„ ì˜ì¡´ ë¬¸ì œì—ì„œ t=0ì¼ ë•Œì˜ ì¡°ê±´:
```
u(x, t=0) = uâ‚€(x)
```

**ì†ì‹¤:**
```
L_IC = (1/N_IC) Î£ |u(xáµ¢, 0) - uâ‚€(xáµ¢)|Â²
```

### ì½”ë“œ

```python
def initial_condition_loss(model, x_ic, u_ic):
    """
    ì´ˆê¸° ì¡°ê±´ ì†ì‹¤
    
    Args:
        model: PINN ëª¨ë¸
        x_ic: ì´ˆê¸° í¬ì¸íŠ¸ (x, t=0) (N_IC, 2)
        u_ic: ì´ˆê¸° ê°’ (N_IC, 1)
    
    Returns:
        loss: ì´ˆê¸° ì¡°ê±´ ì†ì‹¤
    """
    u_pred = model(x_ic)
    loss = torch.mean((u_pred - u_ic) ** 2)
    return loss

# ì˜ˆì‹œ: Heat equation u(x, 0) = sin(Ï€x)
x_spatial = torch.linspace(0, 1, 100)
t_initial = torch.zeros(100)
x_ic = torch.stack([x_spatial, t_initial], dim=1)
u_ic = torch.sin(torch.pi * x_spatial).unsqueeze(1)

loss_ic = initial_condition_loss(model, x_ic, u_ic)
print(f"ì´ˆê¸° ì¡°ê±´ ì†ì‹¤: {loss_ic.item():.6f}")
```

---

## âš–ï¸ ì†ì‹¤ ê°€ì¤‘ì¹˜ ì¡°ì •

### ì™œ í•„ìš”í•œê°€ìš”?

ê° ì†ì‹¤ í•­ëª©ì˜ **ìŠ¤ì¼€ì¼**ì´ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤!

**ì˜ˆì‹œ:**
```
L_residual = 100.0
L_BC = 0.01
L_IC = 0.001
```

ê°€ì¤‘ì¹˜ ì—†ì´ ë”í•˜ë©´ ì”ì°¨ ì†ì‹¤ì´ ì§€ë°°ì ì…ë‹ˆë‹¤!

### ê°€ì¤‘ì¹˜ ì‚¬ìš©

```python
# ê°€ì¤‘ì¹˜ ì„¤ì •
lambda_residual = 1.0
lambda_bc = 100.0    # ê²½ê³„ ì¡°ê±´ ê°•ì¡°
lambda_ic = 100.0    # ì´ˆê¸° ì¡°ê±´ ê°•ì¡°

# ì´ ì†ì‹¤
loss_total = (
    lambda_residual * loss_residual +
    lambda_bc * loss_bc +
    lambda_ic * loss_ic
)

print(f"ì´ ì†ì‹¤: {loss_total.item():.6f}")
```

### ê°€ì¤‘ì¹˜ ì„ íƒ íŒ

**ê²½í—˜ì  ê·œì¹™:**
1. ëª¨ë“  í•­ëª©ì´ ë¹„ìŠ·í•œ í¬ê¸°ê°€ ë˜ë„ë¡ ì¡°ì •
2. ë” ì¤‘ìš”í•œ ì¡°ê±´ì— ë†’ì€ ê°€ì¤‘ì¹˜
3. ì‹œí–‰ì°©ì˜¤ë¥¼ í†µí•´ ìµœì í™”

**ì˜ˆì‹œ:**
```python
# ì´ˆê¸° í›ˆë ¨ì—ì„œ ê° ì†ì‹¤ í™•ì¸
with torch.no_grad():
    print(f"L_residual: {loss_residual.item():.6f}")
    print(f"L_BC: {loss_bc.item():.6f}")
    print(f"L_IC: {loss_ic.item():.6f}")

# ê°€ì¤‘ì¹˜ ì¡°ì •
# ëª©í‘œ: ëª¨ë“  í•­ëª©ì´ 1.0 ì •ë„ê°€ ë˜ë„ë¡
lambda_residual = 1.0 / loss_residual.item()
lambda_bc = 1.0 / loss_bc.item()
lambda_ic = 1.0 / loss_ic.item()
```

---

## ğŸ—ï¸ PhysicsInformedLoss í´ë˜ìŠ¤

### í”„ë¡œì íŠ¸ì˜ ì†ì‹¤ í´ë˜ìŠ¤

`src/loss.py`ì— êµ¬í˜„ë˜ì–´ ìˆìŠµë‹ˆë‹¤:

```python
from src.loss import PhysicsInformedLoss

# ì†ì‹¤ í•¨ìˆ˜ ìƒì„±
loss_fn = PhysicsInformedLoss(
    pde_fn=my_pde_residual,        # PDE ì”ì°¨ í•¨ìˆ˜
    boundary_weight=10.0,          # ê²½ê³„ ì¡°ê±´ ê°€ì¤‘ì¹˜
    initial_weight=10.0,           # ì´ˆê¸° ì¡°ê±´ ê°€ì¤‘ì¹˜
    data_weight=1.0                # ë°ì´í„° ì†ì‹¤ ê°€ì¤‘ì¹˜ (ìˆëŠ” ê²½ìš°)
)
```

### ì‚¬ìš© ì˜ˆì œ

```python
import torch
from src.loss import PhysicsInformedLoss

# PDE ì”ì°¨ í•¨ìˆ˜ ì •ì˜
def poisson_residual(model, x):
    """Poisson ë°©ì •ì‹: âˆ‡Â²u = f"""
    x = x.requires_grad_(True)
    u = model(x)
    
    # 2ì°¨ ë¯¸ë¶„ (Laplacian)
    grad_u = torch.autograd.grad(
        outputs=u,
        inputs=x,
        grad_outputs=torch.ones_like(u),
        create_graph=True
    )[0]
    
    laplacian = 0
    for i in range(x.shape[1]):
        grad_u_i = grad_u[:, i:i+1]
        grad2_u_i = torch.autograd.grad(
            outputs=grad_u_i,
            inputs=x,
            grad_outputs=torch.ones_like(grad_u_i),
            create_graph=True
        )[0][:, i:i+1]
        laplacian += grad2_u_i
    
    # ì†ŒìŠ¤ í•­
    f = -2 * torch.pi**2 * torch.sin(torch.pi * x[:, 0:1]) * torch.sin(torch.pi * x[:, 1:2])
    
    return laplacian - f

# ì†ì‹¤ í•¨ìˆ˜ ìƒì„±
loss_fn = PhysicsInformedLoss(
    pde_fn=poisson_residual,
    boundary_weight=10.0
)

# ë°ì´í„° ì¤€ë¹„
x_collocation = torch.rand(1000, 2, requires_grad=True)
x_boundary = torch.rand(100, 2)
u_boundary = torch.zeros(100, 1)

# ì†ì‹¤ ê³„ì‚°
loss, metrics = loss_fn(
    model=model,
    x_pde=x_collocation,
    x_bc=x_boundary,
    u_bc=u_boundary
)

print(f"ì´ ì†ì‹¤: {loss.item():.6f}")
print(f"ë©”íŠ¸ë¦­: {metrics}")
```

---

## ğŸ§ª ì‹¤ìŠµ: ì™„ì „í•œ ì†ì‹¤ í•¨ìˆ˜ êµ¬í˜„

### ëª©í‘œ

1D Helmholtz ë°©ì •ì‹ì„ ìœ„í•œ ì™„ì „í•œ ì†ì‹¤ í•¨ìˆ˜ ì‘ì„±

### ì „ì²´ ì½”ë“œ

```python
import torch
from src.models import Scaled_cPIKAN

def helmholtz_loss_function(model, x_collocation, x_bc, u_bc, k=1.0, lambda_bc=10.0):
    """
    Helmholtz ë°©ì •ì‹ ì™„ì „í•œ ì†ì‹¤ í•¨ìˆ˜
    
    ë°©ì •ì‹: dÂ²u/dxÂ² + kÂ²u = 0
    ê²½ê³„ ì¡°ê±´: u(0) = u(1) = 0
    """
    # 1. PDE ì”ì°¨ ì†ì‹¤
    x_col = x_collocation.requires_grad_(True)
    u = model(x_col)
    
    du_dx = torch.autograd.grad(
        outputs=u, inputs=x_col,
        grad_outputs=torch.ones_like(u),
        create_graph=True
    )[0]
    
    d2u_dx2 = torch.autograd.grad(
        outputs=du_dx, inputs=x_col,
        grad_outputs=torch.ones_like(du_dx),
        create_graph=True
    )[0]
    
    residual = d2u_dx2 + k**2 * u
    loss_pde = torch.mean(residual ** 2)
    
    # 2. ê²½ê³„ ì¡°ê±´ ì†ì‹¤
    u_bc_pred = model(x_bc)
    loss_bc = torch.mean((u_bc_pred - u_bc) ** 2)
    
    # 3. ì´ ì†ì‹¤
    loss_total = loss_pde + lambda_bc * loss_bc
    
    # ë©”íŠ¸ë¦­
    metrics = {
        'loss_pde': loss_pde.item(),
        'loss_bc': loss_bc.item(),
        'loss_total': loss_total.item()
    }
    
    return loss_total, metrics

# ëª¨ë¸ ë° ë°ì´í„° ì¤€ë¹„
model = Scaled_cPIKAN(
    layers_hidden=[1, 20, 20, 1],
    degree=3,
    domain_min=torch.tensor([0.0]),
    domain_max=torch.tensor([1.0])
)

x_collocation = torch.linspace(0.01, 0.99, 100).unsqueeze(1)
x_bc = torch.tensor([[0.0], [1.0]])
u_bc = torch.zeros(2, 1)

# ì†ì‹¤ ê³„ì‚°
loss, metrics = helmholtz_loss_function(model, x_collocation, x_bc, u_bc)

print(f"PDE ì†ì‹¤: {metrics['loss_pde']:.6f}")
print(f"BC ì†ì‹¤: {metrics['loss_bc']:.6f}")
print(f"ì´ ì†ì‹¤: {metrics['loss_total']:.6f}")
```

---

## ğŸ” ì†ì‹¤ í•¨ìˆ˜ ë””ë²„ê¹…

### ì²´í¬ë¦¬ìŠ¤íŠ¸

#### 1. ê·¸ë˜ë””ì–¸íŠ¸ í™•ì¸

```python
# ì†ì‹¤ì— ëŒ€í•œ ê·¸ë˜ë””ì–¸íŠ¸ê°€ ìˆëŠ”ì§€ í™•ì¸
loss.backward()
for name, param in model.named_parameters():
    if param.grad is not None:
        print(f"{name}: grad norm = {param.grad.norm().item():.6f}")
    else:
        print(f"{name}: NO GRADIENT!")
```

#### 2. NaN/Inf ì²´í¬

```python
if torch.isnan(loss) or torch.isinf(loss):
    print("ê²½ê³ : ì†ì‹¤ì´ NaN ë˜ëŠ” Infì…ë‹ˆë‹¤!")
    print(f"L_pde: {loss_pde.item()}")
    print(f"L_bc: {loss_bc.item()}")
```

#### 3. ìŠ¤ì¼€ì¼ í™•ì¸

```python
print(f"L_pde ìŠ¤ì¼€ì¼: {loss_pde.item():.2e}")
print(f"L_bc ìŠ¤ì¼€ì¼: {loss_bc.item():.2e}")
print(f"ë¹„ìœ¨: {loss_pde.item() / loss_bc.item():.2f}")
```

**ì´ìƒì :** ë¹„ìœ¨ì´ 1~100 ì‚¬ì´

---

## ğŸ¯ ì²´í¬í¬ì¸íŠ¸

ë‹¤ìŒ í•­ëª©ì„ ëª¨ë‘ ì´í•´í–ˆìœ¼ë©´ ë‹¤ìŒ ë‹¨ê³„ë¡œ ì§„í–‰í•˜ì„¸ìš”!

- [ ] PDE ì”ì°¨ ì†ì‹¤ì˜ ê°œë… ì´í•´
- [ ] `torch.autograd.grad` ì‚¬ìš©ë²• ìˆ™ì§€
- [ ] 1ì°¨, 2ì°¨ ë¯¸ë¶„ ê³„ì‚° ê°€ëŠ¥
- [ ] Dirichlet/Neumann ê²½ê³„ ì¡°ê±´ êµ¬í˜„
- [ ] ì´ˆê¸° ì¡°ê±´ ì†ì‹¤ ì‘ì„±
- [ ] ì†ì‹¤ ê°€ì¤‘ì¹˜ ì¡°ì • ë°©ë²• ì´í•´

---

## ğŸ†˜ ìì£¼ ë¬»ëŠ” ì§ˆë¬¸

### Q1: "RuntimeError: grad can be implicitly created only for scalar outputs"

**A:** `grad_outputs` íŒŒë¼ë¯¸í„°ë¥¼ ì¶”ê°€í•˜ì„¸ìš”:
```python
grad = torch.autograd.grad(
    outputs=u,
    inputs=x,
    grad_outputs=torch.ones_like(u)  â† ì´ê²ƒ!
)
```

### Q2: ì”ì°¨ ì†ì‹¤ì´ ì¤„ì–´ë“¤ì§€ ì•Šì•„ìš”

**A:** ë‹¤ìŒì„ í™•ì¸í•˜ì„¸ìš”:
1. `requires_grad=True` ì„¤ì • í™•ì¸
2. `create_graph=True` ì„¤ì • í™•ì¸
3. í•™ìŠµë¥ ì´ ë„ˆë¬´ ì‘ì§€ ì•Šì€ì§€ í™•ì¸

### Q3: ê²½ê³„ ì¡°ê±´ì´ ë§Œì¡±ë˜ì§€ ì•Šì•„ìš”

**A:** `lambda_bc` ê°€ì¤‘ì¹˜ë¥¼ ì¦ê°€ì‹œí‚¤ì„¸ìš” (ì˜ˆ: 10 â†’ 100).

### Q4: 2D Laplacianì€ ì–´ë–»ê²Œ ê³„ì‚°í•˜ë‚˜ìš”?

**A:** ê° ì°¨ì›ì— ëŒ€í•´ 2ì°¨ ë¯¸ë¶„ì„ ê³„ì‚°í•˜ê³  í•©ì‚°:
```python
laplacian = d2u_dx2 + d2u_dy2
```

---

## ğŸ“š ì¶”ê°€ ìë£Œ

- **PyTorch Autograd íŠœí† ë¦¬ì–¼:** [https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html)
- **PINN ì›ë…¼ë¬¸:** Raissi et al., "Physics-informed neural networks" (2019)

---

## ğŸ‰ ì¶•í•˜í•©ë‹ˆë‹¤!

ì†ì‹¤ í•¨ìˆ˜ë¥¼ ë§ˆìŠ¤í„°í–ˆìŠµë‹ˆë‹¤! ì´ì œ ëª¨ë¸ì„ í›ˆë ¨í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤.

**ë‹¤ìŒ ë‹¨ê³„:**

ğŸ‘‰ [5ë‹¨ê³„: í›ˆë ¨ ê³¼ì •ìœ¼ë¡œ ì´ë™í•˜ê¸°](05_í›ˆë ¨ê³¼ì •.md)

ë˜ëŠ” [ë©”ì¸ ê°€ì´ë“œë¡œ ëŒì•„ê°€ê¸°](../ì‚¬ìš©ì_ê°€ì´ë“œ.md)

---

*ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸: 2025ë…„ 1ì›”*
