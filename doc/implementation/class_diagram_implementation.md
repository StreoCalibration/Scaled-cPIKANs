# Scaled-cPIKAN í´ë˜ìŠ¤ ë‹¤ì´ì–´ê·¸ë¨ ë° êµ¬í˜„ ìƒì„¸

**ì‘ì„±ì¼**: 2025-10-25  
**ë²„ì „**: 1.0  
**ëª©ì **: Scaled-cPIKAN ì½”ë“œë² ì´ìŠ¤ì˜ í´ë˜ìŠ¤ êµ¬ì¡°, ì˜ì¡´ì„±, ë©”ì„œë“œ ì‹œê·¸ë‹ˆì²˜ë¥¼ ì‹œê°í™”í•˜ê³  ìƒì„¸íˆ ì„¤ëª…

---

## ğŸ“‹ ëª©ì°¨

1. [ì „ì²´ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜](#ì „ì²´-ì‹œìŠ¤í…œ-ì•„í‚¤í…ì²˜)
2. [src/models.py í´ë˜ìŠ¤ ë‹¤ì´ì–´ê·¸ë¨](#srcmodelspy-í´ë˜ìŠ¤-ë‹¤ì´ì–´ê·¸ë¨)
3. [src/loss.py í´ë˜ìŠ¤ ë‹¤ì´ì–´ê·¸ë¨](#srclosspy-í´ë˜ìŠ¤-ë‹¤ì´ì–´ê·¸ë¨)
4. [src/data.py í´ë˜ìŠ¤ ë‹¤ì´ì–´ê·¸ë¨](#srcdatapy-í´ë˜ìŠ¤-ë‹¤ì´ì–´ê·¸ë¨)
5. [src/train.py í´ë˜ìŠ¤ ë‹¤ì´ì–´ê·¸ë¨](#srctrainpy-í´ë˜ìŠ¤-ë‹¤ì´ì–´ê·¸ë¨)
6. [src/data_generator.py í´ë˜ìŠ¤ ë‹¤ì´ì–´ê·¸ë¨](#srcdata_generatorpy-í´ë˜ìŠ¤-ë‹¤ì´ì–´ê·¸ë¨)
7. [í´ë˜ìŠ¤ ê°„ ìƒí˜¸ì‘ìš© ì‹œí€€ìŠ¤ ë‹¤ì´ì–´ê·¸ë¨](#í´ë˜ìŠ¤-ê°„-ìƒí˜¸ì‘ìš©-ì‹œí€€ìŠ¤-ë‹¤ì´ì–´ê·¸ë¨)
8. [ì˜ì¡´ì„± ê·¸ë˜í”„](#ì˜ì¡´ì„±-ê·¸ë˜í”„)
9. [ì„¤ê³„ íŒ¨í„´ ë¶„ì„](#ì„¤ê³„-íŒ¨í„´-ë¶„ì„)

---

## ì „ì²´ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

```mermaid
graph TD
    User[ì‚¬ìš©ì/ìŠ¤í¬ë¦½íŠ¸] --> Pipeline[run_pipeline.py]
    Pipeline --> DataGen[data_generator.py]
    Pipeline --> Trainer[train.py]
    
    Trainer --> Models[models.py]
    Trainer --> Loss[loss.py]
    Trainer --> Data[data.py]
    
    DataGen --> Data
    Models --> Loss
    
    subgraph "í•µì‹¬ ëª¨ë“ˆ"
        Models
        Loss
        Data
        Trainer
    end
    
    subgraph "ìœ í‹¸ë¦¬í‹°"
        DataGen
    end
    
    subgraph "ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸"
        Pipeline
        Demo1[solve_helmholtz_1d.py]
        Demo2[solve_reconstruction_pinn.py]
    end
```

---

## src/models.py í´ë˜ìŠ¤ ë‹¤ì´ì–´ê·¸ë¨

### í´ë˜ìŠ¤ ê³„ì¸µ êµ¬ì¡°

```mermaid
classDiagram
    class nn_Module {
        <<PyTorch>>
        +forward()
        +parameters()
        +train()
        +eval()
    }
    
    class ChebyKANLayer {
        -in_features: int
        -out_features: int
        -degree: int
        -coefficients: Parameter
        -bounds: Tuple~float float~
        +__init__(in_features, out_features, degree, bounds)
        +forward(x: Tensor) Tensor
        -_affine_scale(x: Tensor, x_min: float, x_max: float) Tensor
        -_chebyshev_basis(x: Tensor, degree: int) Tensor
    }
    
    class Scaled_cPIKAN {
        -input_dim: int
        -hidden_dim: int
        -output_dim: int
        -depth: int
        -degree: int
        -bounds: Tuple~Tuple~
        -layers: ModuleList
        +__init__(input_dim, hidden_dim, output_dim, depth, degree, bounds)
        +forward(x: Tensor) Tensor
    }
    
    class UNet {
        -in_channels: int
        -out_channels: int
        -features: List~int~
        -encoder: ModuleList
        -decoder: ModuleList
        -bottleneck: Sequential
        +__init__(in_channels, out_channels, features)
        +forward(x: Tensor) Tensor
    }
    
    nn_Module <|-- ChebyKANLayer
    nn_Module <|-- Scaled_cPIKAN
    nn_Module <|-- UNet
    Scaled_cPIKAN o-- ChebyKANLayer : contains
```

### í´ë˜ìŠ¤ ìƒì„¸

#### 1. ChebyKANLayer

**ëª©ì **: ì²´ë¹„ì‡¼í”„ ë‹¤í•­ì‹ ê¸°ì €ë¥¼ ì‚¬ìš©í•˜ëŠ” Kolmogorov-Arnold Network ë ˆì´ì–´

**ì´ë¡ ì  ë°°ê²½**: 
- [Kolmogorov-Arnold Networks (KAN)](../theory/theoretical_background.md#kolmogorov-arnold-networks-kan)
- [Chebyshev ë‹¤í•­ì‹ ì´ë¡ ](../theory/theoretical_background.md#chebyshev-ë‹¤í•­ì‹-ì´ë¡ )
- [ë„ë©”ì¸ ìŠ¤ì¼€ì¼ë§](../theory/theoretical_background.md#ë„ë©”ì¸-ìŠ¤ì¼€ì¼ë§)

**ì†ì„±**:
- `in_features: int` - ì…ë ¥ ì°¨ì›
- `out_features: int` - ì¶œë ¥ ì°¨ì›
- `degree: int` - ì²´ë¹„ì‡¼í”„ ë‹¤í•­ì‹ ì°¨ìˆ˜ (K)
- `coefficients: nn.Parameter` - í•™ìŠµ ê°€ëŠ¥í•œ ì²´ë¹„ì‡¼í”„ ê³„ìˆ˜, í˜•íƒœ `(out_features, in_features, degree+1)`
- `bounds: Tuple[float, float]` - ì…ë ¥ ë„ë©”ì¸ ë²”ìœ„ `(x_min, x_max)`

**ë©”ì„œë“œ**:

```python
def forward(x: torch.Tensor) -> torch.Tensor:
    """
    Args:
        x: (batch_size, in_features) ì…ë ¥ í…ì„œ
    
    Returns:
        (batch_size, out_features) ì¶œë ¥ í…ì„œ
    
    ì ˆì°¨:
        1. xë¥¼ boundsë¥¼ ì´ìš©í•´ [-1, 1]ë¡œ ìŠ¤ì¼€ì¼ë§
        2. ì²´ë¹„ì‡¼í”„ ê¸°ì € ê³„ì‚°: T_0(x), T_1(x), ..., T_K(x)
        3. einsumìœ¼ë¡œ ê³„ìˆ˜ì™€ ê³±ì…ˆ: "bik,oik->bo"
    """
```

#### 2. Scaled_cPIKAN

**ëª©ì **: ì—¬ëŸ¬ `ChebyKANLayer`ë¥¼ ìŒ“ì•„ ê¹Šì€ ì‹ ê²½ë§ êµ¬ì„±

**ì´ë¡ ì  ë°°ê²½**: 
- [Physics-Informed Neural Networks (PINN)](../theory/theoretical_background.md#physics-informed-neural-networks-pinn)
- [Kolmogorov-Arnold Networks (KAN)](../theory/theoretical_background.md#kolmogorov-arnold-networks-kan)
- [ë„ë©”ì¸ ìŠ¤ì¼€ì¼ë§](../theory/theoretical_background.md#ë„ë©”ì¸-ìŠ¤ì¼€ì¼ë§)

**ì†ì„±**:
- `input_dim: int` - ì…ë ¥ ì°¨ì› (ì˜ˆ: 2D ë¬¸ì œì˜ ê²½ìš° 2)
- `hidden_dim: int` - ì€ë‹‰ì¸µ ì°¨ì›
- `output_dim: int` - ì¶œë ¥ ì°¨ì› (ì˜ˆ: ìŠ¤ì¹¼ë¼ ì¥ì˜ ê²½ìš° 1)
- `depth: int` - ë ˆì´ì–´ ê°œìˆ˜
- `degree: int` - ì²´ë¹„ì‡¼í”„ ë‹¤í•­ì‹ ì°¨ìˆ˜
- `bounds: Tuple[Tuple[float, float], ...]` - ê° ì…ë ¥ ì°¨ì›ì˜ ë„ë©”ì¸ ë²”ìœ„
- `layers: nn.ModuleList` - `ChebyKANLayer` ë¦¬ìŠ¤íŠ¸

**ë©”ì„œë“œ**:
```python
def forward(x: torch.Tensor) -> torch.Tensor:
    """
    Args:
        x: (batch_size, input_dim) ì…ë ¥ í…ì„œ
    
    Returns:
        (batch_size, output_dim) ì¶œë ¥ í…ì„œ
    
    ì ˆì°¨:
        ìˆœì°¨ì ìœ¼ë¡œ ëª¨ë“  ë ˆì´ì–´ë¥¼ í†µê³¼
    """
```

#### 3. UNet

**ëª©ì **: ì‚¬ì „í•™ìŠµì„ ìœ„í•œ ì»¨ë³¼ë£¨ì…˜ ì‹ ê²½ë§ (ì¬êµ¬ì„± ì‘ì—…)

**ì´ë¡ ì  ë°°ê²½**: 
- [ìµœì í™” ì•Œê³ ë¦¬ì¦˜](../theory/theoretical_background.md#ìµœì í™”-ì•Œê³ ë¦¬ì¦˜) - Phase 1: ì‚¬ì „í•™ìŠµ

**ì†ì„±**:
- `in_channels: int` - ì…ë ¥ ì±„ë„ ìˆ˜ (ì˜ˆ: ë²„í‚· ì´ë¯¸ì§€ ê°œìˆ˜)
- `out_channels: int` - ì¶œë ¥ ì±„ë„ ìˆ˜ (ì˜ˆ: ë†’ì´ ë§µ = 1)
- `features: List[int]` - ê° ë ˆë²¨ì˜ íŠ¹ì§• ë§µ ê°œìˆ˜ (ì˜ˆ: [64, 128, 256, 512])
- `encoder: nn.ModuleList` - ì¸ì½”ë” ë¸”ë¡ë“¤
- `decoder: nn.ModuleList` - ë””ì½”ë” ë¸”ë¡ë“¤
- `bottleneck: nn.Sequential` - ë³‘ëª© ë ˆì´ì–´

**ë©”ì„œë“œ**:
```python
def forward(x: torch.Tensor) -> torch.Tensor:
    """
    Args:
        x: (batch_size, in_channels, H, W) ì…ë ¥ ì´ë¯¸ì§€
    
    Returns:
        (batch_size, out_channels, H, W) ì¬êµ¬ì„±ëœ ì¶œë ¥
    
    ì ˆì°¨:
        1. ì¸ì½”ë”: í•´ìƒë„ ê°ì†Œ, íŠ¹ì§• ì¶”ì¶œ
        2. ë³‘ëª©: ìµœì†Œ í•´ìƒë„ì—ì„œ ì²˜ë¦¬
        3. ë””ì½”ë”: í•´ìƒë„ ë³µì›, ìŠ¤í‚µ ì—°ê²° í™œìš©
    """
```

---

## src/loss.py í´ë˜ìŠ¤ ë‹¤ì´ì–´ê·¸ë¨

```mermaid
classDiagram
    class UnetLoss {
        -reconstruction_weight: float
        -smoothness_weight: float
        +__init__(reconstruction_weight, smoothness_weight)
        +forward(pred: Tensor, target: Tensor) Tuple~Tensor dict~
        -compute_reconstruction_loss(pred, target) Tensor
        -compute_smoothness_loss(pred) Tensor
    }
    
    class PinnLoss {
        -pde_weight: float
        -bc_weight: float
        -ic_weight: float
        -data_weight: float
        -smoothness_weight: float
        +__init__(...)
        +forward(model, inputs, targets, domain_data) Tuple~Tensor dict~
        -compute_pde_residual(model, x) Tensor
        -compute_bc_loss(model, x_bc, u_bc) Tensor
        -compute_data_loss(pred, target) Tensor
        -compute_smoothness_loss(pred) Tensor
    }
    
    class WaferReconstructionLoss {
        -pde_weight: float
        -data_weight: float
        -smoothness_weight: float
        -wavelengths: ndarray
        +__init__(wavelengths, ...)
        +forward(model, inputs, bucket_images) Tuple~Tensor dict~
        -compute_wafer_pde_residual(h_pred, wavelengths) Tensor
        -compute_bucket_data_loss(h_pred, bucket_images, wavelengths) Tensor
        -compute_smoothness_loss(h_pred) Tensor
    }
    
    UnetLoss --|> nn_Module : inherits
    PinnLoss --|> nn_Module : inherits
    WaferReconstructionLoss --|> nn_Module : inherits
```

### í´ë˜ìŠ¤ ìƒì„¸

#### 1. UnetLoss

**ëª©ì **: UNet ì‚¬ì „í•™ìŠµìš© ì†ì‹¤ í•¨ìˆ˜ (ì¬êµ¬ì„± + ì •ê·œí™”)

**ì´ë¡ ì  ë°°ê²½**: 
- [ìµœì í™” ì•Œê³ ë¦¬ì¦˜](../theory/theoretical_background.md#ìµœì í™”-ì•Œê³ ë¦¬ì¦˜) - ì‚¬ì „í•™ìŠµ ë‹¨ê³„

**ì†ì„±**:
- `reconstruction_weight: float` - MSE ì†ì‹¤ ê°€ì¤‘ì¹˜
- `smoothness_weight: float` - ìŠ¤ë¬´ë”© ì •ê·œí™” ê°€ì¤‘ì¹˜

**ë°˜í™˜ê°’**:
```python
(loss: torch.Tensor, metrics: Dict[str, float])
```

#### 2. PinnLoss

**ëª©ì **: PINN í›ˆë ¨ìš© ë¬¼ë¦¬ ê¸°ë°˜ ì†ì‹¤ í•¨ìˆ˜

**ì´ë¡ ì  ë°°ê²½**: 
- [Physics-Informed Neural Networks (PINN)](../theory/theoretical_background.md#physics-informed-neural-networks-pinn)
- [PINN ì†ì‹¤ í•¨ìˆ˜](../theory/theoretical_background.md#pinn-ì†ì‹¤-í•¨ìˆ˜)
- [ìë™ ë¯¸ë¶„](../theory/theoretical_background.md#ìë™-ë¯¸ë¶„)

**ì†ì„±**:
- `pde_weight: float` - PDE ì”ì°¨ ì†ì‹¤ ê°€ì¤‘ì¹˜
- `bc_weight: float` - ê²½ê³„ ì¡°ê±´ ì†ì‹¤ ê°€ì¤‘ì¹˜
- `ic_weight: float` - ì´ˆê¸° ì¡°ê±´ ì†ì‹¤ ê°€ì¤‘ì¹˜
- `data_weight: float` - ë°ì´í„° ì í•© ì†ì‹¤ ê°€ì¤‘ì¹˜
- `smoothness_weight: float` - ìŠ¤ë¬´ë”© ì •ê·œí™” ê°€ì¤‘ì¹˜

#### 3. WaferReconstructionLoss

**ëª©ì **: ì›¨ì´í¼ ìœ„ìƒ ì¬êµ¬ì„±ì„ ìœ„í•œ ë¬¼ë¦¬ ê¸°ë°˜ ì†ì‹¤ í•¨ìˆ˜

**ì´ë¡ ì  ë°°ê²½**: 
- [ìœ„ìƒ ì¬êµ¬ì„± ì´ë¡ ](../theory/theoretical_background.md#ìœ„ìƒ-ì¬êµ¬ì„±-ì´ë¡ )
- [PINN ì†ì‹¤ í•¨ìˆ˜](../theory/theoretical_background.md#pinn-ì†ì‹¤-í•¨ìˆ˜)

**ì†ì„±**:
- `pde_weight: float` - PDE ì”ì°¨ ì†ì‹¤ ê°€ì¤‘ì¹˜
- `data_weight: float` - ë²„í‚· ì´ë¯¸ì§€ ë°ì´í„° ì í•© ì†ì‹¤ ê°€ì¤‘ì¹˜
- `smoothness_weight: float` - ìŠ¤ë¬´ë”© ì •ê·œí™” ê°€ì¤‘ì¹˜
- `wavelengths: ndarray` - ì¸¡ì •ì— ì‚¬ìš©ëœ íŒŒì¥ ë°°ì—´

**ë©”ì„œë“œ**:
```python
def forward(model, inputs, bucket_images) -> Tuple[Tensor, Dict]:
    """
    Args:
        model: Scaled_cPIKAN ëª¨ë¸
        inputs: (2, H, W) - ì¢Œí‘œ ê·¸ë¦¬ë“œ
        bucket_images: (N_wavelengths, H, W) - ì¸¡ì •ëœ ë²„í‚· ì´ë¯¸ì§€
    
    Returns:
        (loss, metrics) - ì´ ì†ì‹¤ê³¼ ì„¸ë¶€ ë©”íŠ¸ë¦­
    
    ì†ì‹¤ êµ¬ì„±:
        1. PDE ì”ì°¨: íŒŒì¥ë³„ ìœ„ìƒ-ë†’ì´ ê´€ê³„ (h = Î» * Ï† / (4Ï€))
        2. ë°ì´í„° ì í•©: ì˜ˆì¸¡ ë²„í‚· ì´ë¯¸ì§€ vs ì‹¤ì œ ì¸¡ì •ê°’
        3. ìŠ¤ë¬´ë”©: í‘œë©´ ë§¤ë„ëŸ¬ì›€ ì •ê·œí™”
    """
```

---

## src/data.py í´ë˜ìŠ¤ ë‹¤ì´ì–´ê·¸ë¨

```mermaid
classDiagram
    class LatinHypercubeSampler {
        -bounds: List~Tuple~float float~~
        -n_samples: int
        +__init__(bounds, n_samples, seed)
        +sample() ndarray
    }
    
    class PinnPatchDataset {
        -data_dir: Path
        -patch_size: Tuple~int int~
        -samples: List~Path~
        -wavelengths: ndarray
        +__init__(data_dir, patch_size, num_patches_per_sample, seed)
        +__len__() int
        +__getitem__(idx: int) Tuple~Tensor Tensor Tensor~
        -_extract_random_patch(ground_truth) Tuple~ndarray Tensor~
    }
    
    class WaferPatchDataset {
        -data_dir: Path
        -patch_size: Tuple~int int~
        -samples: List~Path~
        -wavelengths: ndarray
        +__init__(data_dir, patch_size, num_patches_per_sample, seed)
        +__len__() int
        +__getitem__(idx: int) Tuple~Tensor Tensor~
        -_extract_random_patch(bucket_stack) Tensor
    }
    
    Dataset <|-- PinnPatchDataset
    Dataset <|-- WaferPatchDataset
    
    PinnPatchDataset ..> LatinHypercubeSampler : uses
    WaferPatchDataset ..> LatinHypercubeSampler : uses
```

### í´ë˜ìŠ¤ ìƒì„¸

#### 1. LatinHypercubeSampler

**ëª©ì **: ì¤€-ëª¬í…Œì¹´ë¥¼ë¡œ ìƒ˜í”Œë§ìœ¼ë¡œ ê· ì¼í•œ ì½œë¡œì¼€ì´ì…˜ í¬ì¸íŠ¸ ìƒì„±

**ì´ë¡ ì  ë°°ê²½**: 
- [Latin Hypercube Sampling](../theory/theoretical_background.md#latin-hypercube-sampling)

**ì‚¬ìš© ì˜ˆì‹œ**:
```python
sampler = LatinHypercubeSampler(
    bounds=[(0, 1), (0, 1)],  # x, y ë²”ìœ„
    n_samples=1000
)
points = sampler.sample()  # (1000, 2) numpy array
```

#### 2. PinnPatchDataset

**ëª©ì **: í•©ì„± í›ˆë ¨ ë°ì´í„°ì—ì„œ íŒ¨ì¹˜ë¥¼ ì¶”ì¶œí•˜ì—¬ PINN í›ˆë ¨ìš© ë°ì´í„° ì œê³µ

**ì´ë¡ ì  ë°°ê²½**: 
- [Physics-Informed Neural Networks (PINN)](../theory/theoretical_background.md#physics-informed-neural-networks-pinn)
- [Latin Hypercube Sampling](../theory/theoretical_background.md#latin-hypercube-sampling)

**ì†ì„±**:
- `data_dir: Path` - ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ
- `patch_size: Tuple[int, int]` - íŒ¨ì¹˜ í¬ê¸° (H, W)
- `samples: List[Path]` - ìƒ˜í”Œ ë””ë ‰í† ë¦¬ ë¦¬ìŠ¤íŠ¸
- `wavelengths: ndarray` - íŒŒì¥ ë°°ì—´

**ë©”ì„œë“œ**:
```python
def __getitem__(idx: int) -> Tuple[Tensor, Tensor, Tensor]:
    """
    Returns:
        inputs: (2, H, W) - ì •ê·œí™”ëœ (x, y) ì¢Œí‘œ ê·¸ë¦¬ë“œ
        targets: (1, H, W) - ground truth ë†’ì´ ë§µ
        domain: (N, 2) - Latin Hypercubeë¡œ ìƒ˜í”Œë§ëœ ì½œë¡œì¼€ì´ì…˜ í¬ì¸íŠ¸
    """
```

#### 3. WaferPatchDataset

**ëª©ì **: ì‹¤ì œ ë²„í‚· ì´ë¯¸ì§€ì—ì„œ íŒ¨ì¹˜ë¥¼ ì¶”ì¶œí•˜ì—¬ ë¯¸ì„¸ì¡°ì •ìš© ë°ì´í„° ì œê³µ

**ì´ë¡ ì  ë°°ê²½**: 
- [ìœ„ìƒ ì¬êµ¬ì„± ì´ë¡ ](../theory/theoretical_background.md#ìœ„ìƒ-ì¬êµ¬ì„±-ì´ë¡ )
- [Latin Hypercube Sampling](../theory/theoretical_background.md#latin-hypercube-sampling)

**ì†ì„±**:
- `data_dir: Path` - ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ
- `patch_size: Tuple[int, int]` - íŒ¨ì¹˜ í¬ê¸° (H, W)
- `samples: List[Path]` - ìƒ˜í”Œ ë””ë ‰í† ë¦¬ ë¦¬ìŠ¤íŠ¸
- `wavelengths: ndarray` - íŒŒì¥ ë°°ì—´

**ë©”ì„œë“œ**:
```python
def __getitem__(idx: int) -> Tuple[Tensor, Tensor]:
    """
    Returns:
        inputs: (2, H, W) - ì •ê·œí™”ëœ (x, y) ì¢Œí‘œ ê·¸ë¦¬ë“œ
        bucket_images: (N_wavelengths, H, W) - ì¸¡ì •ëœ ë²„í‚· ì´ë¯¸ì§€
    """
```

---

## src/train.py í´ë˜ìŠ¤ ë‹¤ì´ì–´ê·¸ë¨

```mermaid
classDiagram
    class Trainer {
        -model: nn.Module
        -loss_fn: nn.Module
        -device: torch.device
        -optimizer: Optimizer
        -scheduler: _LRScheduler
        +__init__(model, loss_fn, device, optimizer_config)
        +train_epoch(dataloader) float
        +validate(dataloader) float
        +fit(train_loader, val_loader, epochs, callbacks) History
        +switch_to_lbfgs(lbfgs_config) None
        -_compute_batch_loss(batch) Tensor
    }
    
    class History {
        +train_losses: List~float~
        +val_losses: List~float~
        +train_metrics: List~Dict~
        +val_metrics: List~Dict~
        +epoch_times: List~float~
        +add_epoch(train_loss, val_loss, ...)
        +get_best_epoch() int
        +plot(save_path) None
    }
    
    Trainer o-- History : creates
    Trainer o-- nn_Module : model
    Trainer o-- nn_Module : loss_fn
```

### í´ë˜ìŠ¤ ìƒì„¸

#### 1. Trainer

**ëª©ì **: í†µí•© í›ˆë ¨ ë£¨í”„ (Adam + L-BFGS ì§€ì›)

**ì´ë¡ ì  ë°°ê²½**: 
- [ìµœì í™” ì•Œê³ ë¦¬ì¦˜](../theory/theoretical_background.md#ìµœì í™”-ì•Œê³ ë¦¬ì¦˜) - 2ë‹¨ê³„ ìµœì í™” ì „ëµ

**ì‚¬ìš© ì˜ˆì‹œ**:
```python
trainer = Trainer(
    model=scaled_cpikan,
    loss_fn=pinn_loss,
    device='cuda',
    optimizer_config={
        'type': 'adam',
        'lr': 1e-3,
        'betas': (0.9, 0.999)
    }
)

# Phase 1: Adam
history1 = trainer.fit(train_loader, val_loader, epochs=1000)

# Phase 2: L-BFGS
trainer.switch_to_lbfgs({'lr': 1.0, 'max_iter': 20})
history2 = trainer.fit(train_loader, val_loader, epochs=100)
```

#### 2. History

**ëª©ì **: í›ˆë ¨ ê³¼ì •ì˜ ë©”íŠ¸ë¦­ ê¸°ë¡ ë° ì‹œê°í™”

**ì†ì„±**:
- `train_losses: List[float]` - ì—í¬í¬ë³„ í›ˆë ¨ ì†ì‹¤
- `val_losses: List[float]` - ì—í¬í¬ë³„ ê²€ì¦ ì†ì‹¤
- `train_metrics: List[Dict]` - ì—í¬í¬ë³„ í›ˆë ¨ ë©”íŠ¸ë¦­ (PDE ì”ì°¨, BC ì†ì‹¤ ë“±)
- `val_metrics: List[Dict]` - ì—í¬í¬ë³„ ê²€ì¦ ë©”íŠ¸ë¦­
- `epoch_times: List[float]` - ì—í¬í¬ë³„ ì†Œìš” ì‹œê°„

**ë©”ì„œë“œ**:
```python
def add_epoch(train_loss, val_loss, train_metrics, val_metrics, epoch_time):
    """ì—í¬í¬ ê²°ê³¼ë¥¼ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€"""

def get_best_epoch() -> int:
    """ê²€ì¦ ì†ì‹¤ì´ ê°€ì¥ ë‚®ì•˜ë˜ ì—í¬í¬ ë²ˆí˜¸ ë°˜í™˜"""

def plot(save_path: str) -> None:
    """ì†ì‹¤ ê³¡ì„ ê³¼ ë©”íŠ¸ë¦­ì„ matplotlibìœ¼ë¡œ ì‹œê°í™”"""
```

---

## src/data_generator.py í´ë˜ìŠ¤ ë‹¤ì´ì–´ê·¸ë¨

```mermaid
classDiagram
    class SyntheticDataGenerator {
        -output_dir: Path
        -img_size: Tuple~int int~
        -wavelengths: ndarray
        -noise_level: float
        +__init__(output_dir, img_size, wavelengths, noise_level, seed)
        +generate_samples(n_samples) None
        -_generate_single_sample(sample_id) None
        -_create_random_surface(width, height) ndarray
        -_compute_bucket_images(height_map, wavelengths) ndarray
        -_apply_gaussian_blur(image, sigma) ndarray
    }
    
    class BucketImageGenerator {
        +generate_bucket_images(height_map, wavelengths, noise_std) ndarray
        -_compute_phase_shift(height, wavelength) ndarray
        -_intensity_to_uint8(intensity) ndarray
    }
    
    SyntheticDataGenerator ..> BucketImageGenerator : uses
```

### í´ë˜ìŠ¤ ìƒì„¸

#### 1. SyntheticDataGenerator

**ëª©ì **: PINN ì‚¬ì „í•™ìŠµì„ ìœ„í•œ í•©ì„± í›ˆë ¨ ë°ì´í„° ìƒì„±

**ì´ë¡ ì  ë°°ê²½**: 
- [ìœ„ìƒ ì¬êµ¬ì„± ì´ë¡ ](../theory/theoretical_background.md#ìœ„ìƒ-ì¬êµ¬ì„±-ì´ë¡ )

**ì†ì„±**:
- `output_dir: Path` - ìƒì„±ëœ ë°ì´í„° ì €ì¥ ê²½ë¡œ
- `img_size: Tuple[int, int]` - ì´ë¯¸ì§€ í¬ê¸° (H, W)
- `wavelengths: ndarray` - ì‹œë®¬ë ˆì´ì…˜ì— ì‚¬ìš©í•  íŒŒì¥ ë°°ì—´
- `noise_level: float` - ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆ í‘œì¤€í¸ì°¨
- `seed: int` - ëœë¤ ì‹œë“œ

**ë©”ì„œë“œ**:
```python
def generate_samples(n_samples: int) -> None:
    """
    Args:
        n_samples: ìƒì„±í•  ìƒ˜í”Œ ê°œìˆ˜
    
    ì ˆì°¨:
        1. _create_random_surface: ë¬´ì‘ìœ„ ë†’ì´ ë§µ ìƒì„± (Perlin noise ë“±)
        2. _compute_bucket_images: ë†’ì´ â†’ ìœ„ìƒ â†’ ê°•ë„ ë³€í™˜
        3. _apply_gaussian_blur: í˜„ì‹¤ì ì¸ ë…¸ì´ì¦ˆ ì¶”ê°€
        4. ì €ì¥: sample_xxx/ground_truth.npy, bucket_*.bmp
    """
```

#### 2. BucketImageGenerator

**ëª©ì **: ë†’ì´ ë§µì—ì„œ ë²„í‚· ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ëŠ” ë¬¼ë¦¬ ì‹œë®¬ë ˆì´í„°

**ì´ë¡ ì  ë°°ê²½**: 
- [ìœ„ìƒ ì¬êµ¬ì„± ì´ë¡ ](../theory/theoretical_background.md#ìœ„ìƒ-ì¬êµ¬ì„±-ì´ë¡ )

**ë©”ì„œë“œ**:
```python
def generate_bucket_images(height_map, wavelengths, noise_std) -> ndarray:
    """
    ë¬¼ë¦¬ ê¸°ë°˜ ë²„í‚· ì´ë¯¸ì§€ ìƒì„±
    
    ê³µì‹:
        Ï†(x, y) = 4Ï€ * h(x, y) / Î»  (ìœ„ìƒ)
        I(x, y) = I_0 * (1 + cos(Ï†))  (ê°„ì„­ ê°•ë„)
    
    Returns:
        (N_wavelengths, H, W) - ë²„í‚· ì´ë¯¸ì§€ ë°°ì—´
    """
```

---

## í´ë˜ìŠ¤ ê°„ ìƒí˜¸ì‘ìš© ì‹œí€€ìŠ¤ ë‹¤ì´ì–´ê·¸ë¨

### ì‹œë‚˜ë¦¬ì˜¤ 1: ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰

```mermaid
sequenceDiagram
    participant User
    participant Pipeline as run_pipeline.py
    participant DataGen as SyntheticDataGenerator
    participant Trainer
    participant Model as Scaled_cPIKAN
    participant Loss as PinnLoss
    participant Data as PinnPatchDataset
    
    User->>Pipeline: python run_pipeline.py
    Pipeline->>DataGen: generate_samples(1000)
    DataGen->>DataGen: _create_random_surface()
    DataGen->>DataGen: _compute_bucket_images()
    DataGen-->>Pipeline: í•©ì„± ë°ì´í„° ì €ì¥ ì™„ë£Œ
    
    Pipeline->>Data: PinnPatchDataset(data_dir)
    Data->>Data: _load_samples()
    Data-->>Pipeline: DataLoader ì¤€ë¹„ ì™„ë£Œ
    
    Pipeline->>Model: Scaled_cPIKAN(...)
    Pipeline->>Loss: PinnLoss(...)
    Pipeline->>Trainer: Trainer(model, loss_fn)
    
    loop 1000 epochs (Adam)
        Pipeline->>Trainer: train_epoch()
        Trainer->>Data: __getitem__(idx)
        Data-->>Trainer: (inputs, targets, domain)
        Trainer->>Model: forward(inputs)
        Model->>Model: ChebyKANLayer.forward()
        Model-->>Trainer: predictions
        Trainer->>Loss: forward(model, inputs, targets, domain)
        Loss->>Loss: compute_pde_residual()
        Loss->>Loss: compute_bc_loss()
        Loss-->>Trainer: (loss, metrics)
        Trainer->>Model: backward()
        Trainer->>Model: optimizer.step()
    end
    
    Pipeline->>Trainer: switch_to_lbfgs()
    
    loop 100 epochs (L-BFGS)
        Pipeline->>Trainer: train_epoch()
        Note over Trainer,Loss: L-BFGSëŠ” ë‚´ë¶€ì ìœ¼ë¡œ<br/>ì—¬ëŸ¬ ë²ˆ forward í˜¸ì¶œ
    end
    
    Pipeline-->>User: í›ˆë ¨ ì™„ë£Œ, ê²°ê³¼ ì €ì¥
```

---

## ì˜ì¡´ì„± ê·¸ë˜í”„

```mermaid
graph LR
    models[models.py] --> torch[PyTorch]
    loss[loss.py] --> torch
    loss --> models
    data[data.py] --> torch
    data --> scipy[SciPy]
    data --> PIL[Pillow]
    train[train.py] --> torch
    train --> models
    train --> loss
    train --> data
    datagen[data_generator.py] --> numpy[NumPy]
    datagen --> PIL
    
    pipeline[run_pipeline.py] --> train
    pipeline --> datagen
    
    style torch fill:#f9f,stroke:#333,stroke-width:2px
    style models fill:#bbf,stroke:#333,stroke-width:2px
    style loss fill:#bfb,stroke:#333,stroke-width:2px
    style data fill:#fbb,stroke:#333,stroke-width:2px
```

---

## ì„¤ê³„ íŒ¨í„´ ë¶„ì„

### 1. ì „ëµ íŒ¨í„´ (Strategy Pattern)

**ì ìš© ìœ„ì¹˜**: `Trainer` í´ë˜ìŠ¤ì˜ ì˜µí‹°ë§ˆì´ì € ì „í™˜

```python
class Trainer:
    def __init__(self, ..., optimizer_config):
        if optimizer_config['type'] == 'adam':
            self.optimizer = torch.optim.Adam(...)
        elif optimizer_config['type'] == 'lbfgs':
            self.optimizer = torch.optim.LBFGS(...)
    
    def switch_to_lbfgs(self, config):
        # ëŸ°íƒ€ì„ì— ì „ëµ êµì²´
        self.optimizer = torch.optim.LBFGS(self.model.parameters(), **config)
```

**ì¥ì **: í›ˆë ¨ ì¤‘ê°„ì— ìµœì í™” ì•Œê³ ë¦¬ì¦˜ ë³€ê²½ ê°€ëŠ¥

### 2. í…œí”Œë¦¿ ë©”ì„œë“œ íŒ¨í„´ (Template Method Pattern)

**ì ìš© ìœ„ì¹˜**: ì†ì‹¤ í•¨ìˆ˜ í´ë˜ìŠ¤ë“¤ (`UnetLoss`, `PinnLoss`, `WaferReconstructionLoss`)

```python
class BaseLoss(nn.Module):
    def forward(self, *args, **kwargs):
        # í…œí”Œë¦¿ ë©”ì„œë“œ
        loss = 0.0
        metrics = {}
        
        # ê° ì„œë¸Œí´ë˜ìŠ¤ê°€ êµ¬í˜„
        loss += self.compute_main_loss(*args)
        loss += self.compute_regularization(*args)
        
        return loss, metrics
```

### 3. íŒ©í† ë¦¬ íŒ¨í„´ (Factory Pattern)

**ì ìš© ìœ„ì¹˜**: ë°ì´í„°ì…‹ ìƒì„±

```python
def create_dataset(config):
    if config['type'] == 'pinn':
        return PinnPatchDataset(...)
    elif config['type'] == 'wafer':
        return WaferPatchDataset(...)
```

### 4. ì˜µì €ë²„ íŒ¨í„´ (Observer Pattern)

**ì ìš© ìœ„ì¹˜**: `History` í´ë˜ìŠ¤

```python
class History:
    def add_epoch(self, train_loss, val_loss, ...):
        # í›ˆë ¨ ì§„í–‰ ìƒí™© ê¸°ë¡
        self.train_losses.append(train_loss)
        # í•„ìš” ì‹œ ì½œë°± í˜¸ì¶œ
        for callback in self.callbacks:
            callback.on_epoch_end(...)
```

---

## í•µì‹¬ ë°ì´í„° íë¦„

### ì…ë ¥ â†’ ëª¨ë¸ â†’ ì†ì‹¤ â†’ ì—­ì „íŒŒ

```
ì…ë ¥ ë°ì´í„° (batch_size, input_dim)
    â†“
[Scaled_cPIKAN]
    â†“ ChebyKANLayer 1: (input_dim) â†’ (hidden_dim)
    â†“ ChebyKANLayer 2: (hidden_dim) â†’ (hidden_dim)
    â†“ ...
    â†“ ChebyKANLayer N: (hidden_dim) â†’ (output_dim)
    â†“
ì˜ˆì¸¡ê°’ (batch_size, output_dim)
    â†“
[PinnLoss]
    â†“ PDE ì”ì°¨ ê³„ì‚° (torch.autograd.grad)
    â†“ BC ì†ì‹¤ ê³„ì‚°
    â†“ ë°ì´í„° ì í•© ì†ì‹¤
    â†“
ì´ ì†ì‹¤ (ìŠ¤ì¹¼ë¼)
    â†“
[Optimizer]
    â†“ backward()
    â†“ step()
    â†“
íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ (coefficients)
```

---

## í´ë˜ìŠ¤ë³„ ì±…ì„ ìš”ì•½

| í´ë˜ìŠ¤ | ì±…ì„ | ì£¼ìš” ë©”ì„œë“œ | ì˜ì¡´ì„± |
|--------|------|-------------|--------|
| `ChebyKANLayer` | ì²´ë¹„ì‡¼í”„ ë‹¤í•­ì‹ ê¸°ë°˜ ë ˆì´ì–´ | `forward`, `_chebyshev_basis` | PyTorch |
| `Scaled_cPIKAN` | ì „ì²´ PINN ë„¤íŠ¸ì›Œí¬ | `forward` | `ChebyKANLayer` |
| `UNet` | ì‚¬ì „í•™ìŠµìš© UNet | `forward` | PyTorch |
| `PinnLoss` | ë¬¼ë¦¬ ê¸°ë°˜ ì†ì‹¤ ê³„ì‚° | `compute_pde_residual`, `compute_bc_loss` | `torch.autograd` |
| `PinnPatchDataset` | í›ˆë ¨ ë°ì´í„° ë¡œë”© | `__getitem__`, `_extract_random_patch` | `LatinHypercubeSampler` |
| `Trainer` | í›ˆë ¨ ë£¨í”„ ê´€ë¦¬ | `fit`, `train_epoch`, `switch_to_lbfgs` | ëª¨ë“  ëª¨ë“ˆ |
| `SyntheticDataGenerator` | í•©ì„± ë°ì´í„° ìƒì„± | `generate_samples`, `_compute_bucket_images` | NumPy, Pillow |

---

## í™•ì¥ ê°€ì´ë“œ

### ìƒˆë¡œìš´ ëª¨ë¸ ì¶”ê°€

1. `src/models.py`ì— `nn.Module` ìƒì† í´ë˜ìŠ¤ ì¶”ê°€
2. `forward` ë©”ì„œë“œ êµ¬í˜„
3. `examples/` í´ë”ì— ë°ëª¨ ìŠ¤í¬ë¦½íŠ¸ ì¶”ê°€

```python
class NewModel(nn.Module):
    def __init__(self, ...):
        super().__init__()
        # ë ˆì´ì–´ ì´ˆê¸°í™”
    
    def forward(self, x):
        # ì •ë°©í–¥ ê³„ì‚°
        return output
```

### ìƒˆë¡œìš´ ì†ì‹¤ í•¨ìˆ˜ ì¶”ê°€

1. `src/loss.py`ì— `nn.Module` ìƒì† í´ë˜ìŠ¤ ì¶”ê°€
2. `forward` ë©”ì„œë“œì—ì„œ `(loss, metrics)` ë°˜í™˜

```python
class NewLoss(nn.Module):
    def forward(self, model, inputs, targets):
        # ì†ì‹¤ ê³„ì‚°
        loss = ...
        metrics = {'loss_component1': ..., 'loss_component2': ...}
        return loss, metrics
```

---

**ë¬¸ì„œ ë²„ì „**: 1.0  
**ìµœì¢… ì—…ë°ì´íŠ¸**: 2025-10-25  
**ì‘ì„±ì**: Scaled-cPIKAN ê°œë°œíŒ€  
**ê´€ë ¨ ë¬¸ì„œ**: [ì´ë¡ ì  ë°°ê²½](../theory/theoretical_background.md)
