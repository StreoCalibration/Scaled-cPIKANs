# Scaled-cPIKAN 성능 향상 — 즉시 적용 1~3단계 실행 가이드

본 문서는 현재 코드/문서/테스트 결과를 기반으로 단기간에 성능과 안정성을 끌어올리기 위한 “즉시 적용 권장” 1~3단계를 체계적으로 정리합니다. 각 항목마다
- 왜 문제가 되는지(원인),
- 무엇을 근거로 그렇게 판단했는지(분석 방법),
- 왜 그 방식으로 고치는지(수정 이유),
- 무엇을 어떻게 바꿀지(수정 방법 개요),
- 성공 기준/검증 절차/위험과 롤백
을 명확히 제시합니다.

문서 대상 저장소: `Scaled-cPIKANs` (branch: main)

---

## 요약: 1~3단계 한눈에 보기

- 1단계(1~2일): 학습 안정화와 성능/자원 효율 즉시 개선
  - 그래디언트 클리핑 추가 (폭발 방지)
  - Mixed Precision Training(AMP) 적용 (속도↑, 메모리↓)
  - Early Stopping 도입 (과적합/불필요 연산 방지)

- 2단계(3~5일): 문제 난이도에 따른 손실/샘플링/배치 효율 개선
  - 동적 손실 가중치(DynamicWeightedLoss) 기본 활성화
  - 적응형 콜로케이션 샘플링(AdaptiveResidualSampler) 통합
  - 배치 처리 최적화(패치 다중화/실효 배치)

- 3단계(1~2주): 수렴/안정성 및 계산 효율 고도화
  - 학습률 스케줄러 개선(Cosine/Plateau/MultiStep 옵션화)
  - 초기화 전략 실험(Kaiming/Xavier/Orthogonal/SmallRandom)
  - 2차 도함수(Laplacian) 계산 효율화(오토그라드 호출 최소화)

---

## 배경: 현재 징후와 분석 근거

- 테스트 로그(`test_output.txt`)와 예제 실행 결과에서 다음 징후 확인:
  - 일부 PDE 스모크 테스트의 상대 오차가 높게 보고됨(예: Poisson 1D 상대 L2 에러 6.9), 에포크 부족과 안정화 부족 징후.
  - U-Net 재구성 평가(`outputs/results/metrics.json`)에서 RMSE/MAE/MAPE가 높음 → 손실 스케일 불일치/정규화 부족 가능성.
  - `src/train.py`에 그래디언트 클리핑/AMP/얼리 스토핑 미적용 확인.
  - 고급 기능(동적 가중치/적응형 샘플링)은 구현되어 있으나 파이프라인에 기본 통합되지는 않음.
  - `examples/solve_3d_poisson.py` 실행이 실패(Exit Code: 1) → 초기 안정화(클리핑/AMP/얼리스톱) 우선 필요.

이상의 근거로 “학습 안정화→손실/샘플링/배치 개선→스케줄러/초기화/자동미분 효율” 순으로 적용하는 것이 비용 대비 효과가 큼을 판단했습니다.

---

## 1단계: 학습 안정화(클리핑/AMP/얼리 스토핑)

### 1-1. 그래디언트 클리핑 추가
- 원인
  - PINN/UNet 모두 고차 도함수 및 복합 손실로 인해 그래디언트 폭발이 간헐적으로 발생할 수 있음.
  - 현재 `src/train.py`에 클리핑 미구현.
- 분석
  - 코드 베이스 점검 결과, `optimizer.step()` 이전의 클리핑 호출 없음.
  - 스모크 테스트에서 불안정한 손실 패턴이 간헐적으로 관찰.
- 수정 이유
  - `clip_grad_norm_`는 빠르고 안전한 1차 방어선으로, 수렴 안정성 개선에 효과적.
- 수정 방법(개요)
  - 위치: `src/train.py` → `Trainer._train_adam()`의 `total_loss.backward()` 이후, `optimizer.step()` 이전.
  - 내용: `torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)` 호출 추가.
- 성공 기준
  - NaN/Inf 무발생, 손실 곡선의 급등락 감소, 실패하던 예제(`solve_3d_poisson.py`)의 초기 학습 안정화.
- 검증 절차
  - `python -m unittest discover tests` 전부 통과 유지.
  - `examples/solve_3d_poisson.py` 실행 시 초기 스텝이 안정적으로 진행되는지 로그 확인.
- 위험/롤백
  - 클리핑이 과도하면 학습 속도가 느려질 수 있음 → `max_norm`을 0.5~2.0 사이로 탐색.

### 1-2. Mixed Precision Training(AMP) 적용
- 원인
  - 현재 FP32만 사용, GPU 메모리/연산 낭비 발생.
- 분석
  - `src/train.py`에 `autocast/GradScaler` 미적용 확인.
  - PINN의 오토그라드가 메모리 집약적 → AMP 이점 큼.
- 수정 이유
  - 속도 1.5~2배, 메모리 30~50% 절감 기대. 더 큰 실효 배치 가능.
- 수정 방법(개요)
  - 위치: `src/train.py`
  - 내용: `torch.cuda.amp.autocast`로 forward 감싸고, `GradScaler`로 `backward/step/update` 처리.
  - 그래디언트 클리핑 병행 시 `scaler.unscale_(optimizer)` 후 클리핑 적용.
- 성공 기준
  - 동일 설정 대비 에포크당 시간 단축, OOM 감소, 정확도 동일 이상.
- 검증 절차
  - 단위 테스트 통과, 훈련 시간/메모리 사용량 비교 기록.
- 위험/롤백
  - 특정 연산자에서 AMP가 이득이 적을 수 있음 → `enabled=False`로 빠른 롤백 가능.

### 1-3. Early Stopping 도입
- 원인
  - 고정 에포크로 불필요한 연산/과적합 가능성.
- 분석
  - 현재 트레이너에 얼리 스토핑 로직 없음.
- 수정 이유
  - 수렴 감지로 시간 절약, 과적합 방지.
- 수정 방법(개요)
  - 위치: `src/train.py`
  - 내용: 단순 `patience/min_delta` 기반 조기 종료 훅 추가. Adam 단계에 먼저 적용.
- 성공 기준
  - 유효 손실 감소가 정체될 때 조기 종료 로그 출력, 전체 훈련 시간 절감.
- 검증 절차
  - 동일 설정 반복 시 에포크 수가 자연스럽게 줄어드는지 확인.
- 위험/롤백
  - 조기 종료 과도 시 미수렴 → `patience`를 충분히 크게 시작(예: 500), 점진 조정.

---

## 2단계: 손실/샘플링/배치 효율 개선

### 2-1. 동적 손실 가중치(DynamicWeightedLoss) 기본 활성화
- 원인
  - PDE/BC/IC/Data 손실 스케일이 일관되지 않아 한 항이 학습을 지배할 위험.
- 분석
  - `src/loss.py`에 `DynamicWeightedLoss` 구현 완료, 예제 파이프라인에서 기본 사용은 아님 확인.
- 수정 이유
  - 수동 튜닝 부담 감소, 안정적 수렴 유도.
- 수정 방법(개요)
  - 위치: `examples/*` (PINN 예제들)
  - 내용: `PhysicsInformedLoss`를 `DynamicWeightedLoss`로 래핑하여 사용(`loss_names=['loss_pde','loss_bc',...]`).
- 성공 기준
  - 손실 항목별 학습 속도 균형화(로그의 weight 변화가 정상 범위), 최종 오차 개선.
- 검증 절차
  - 동일 에포크/포인트 수에서 상대 L2/MAE 개선 확인.
- 위험/롤백
  - 초기에 가중치 불안정 가능 → `alpha/learning_rate` 보수적으로 시작.

### 2-2. 적응형 콜로케이션 샘플링 통합
- 원인
  - 균일 샘플만으로는 잔차가 큰 영역 학습 비효율.
- 분석
  - `src/data.py`에 `AdaptiveResidualSampler` 구현 완료, 예제 기본 통합은 아님.
- 수정 이유
  - 동일 포인트로 더 낮은 오차/더 적은 포인트로 동일 오차 달성 가능.
- 수정 방법(개요)
  - 위치: `examples/solve_*`
  - 내용: 훈련-잔차평가-정제 주기를 2~5회 반복. `refinement_ratio=0.2`, `percentile=75`로 시작.
- 성공 기준
  - 정제 반복마다 PDE 잔차 분포의 상위 백분위 감소, 최종 오차 개선.
- 검증 절차
  - 각 정제 단계 포인트 수/상위 잔차 로그 기록, 종단 오차 비교.
- 위험/롤백
  - 정제 과도 시 국소 과적합 → 최대 포인트 상한/정제 회수 제한.

### 2-3. 배치 처리 최적화(패치 다중화/실효 배치)
- 원인
  - PINN/패치 데이터셋이 `batch_size=1` 위주, GPU 활용률 저조.
- 분석
  - `src/data.py`의 `PinnPatchDataset`이 에폭당 랜덤 패치 생성 구조. 병렬화 여지 있음.
- 수정 이유
  - 계산 효율과 그래디언트 추정 안정성 동시 개선.
- 수정 방법(개요)
  - 위치: `src/data.py`/예제 로더
  - 내용: 한 호출에 여러 패치를 반환하거나 DataLoader의 `batch_size`를 활용. AMP와 병행해 실효 배치 확대.
- 성공 기준
  - 동일 에포크당 시간 대비 더 많은 샘플 처리, 수렴 속도/최종 오차 개선.
- 검증 절차
  - GPU 사용률/시간 로그, 동일 시간 내 성능 비교.
- 위험/롤백
  - 메모리 초과/OOM → AMP 활용 및 배치 단계적 증가.

---

## 3단계: 수렴/초기화/자동미분 효율 고도화

### 3-1. 학습률 스케줄러 개선(옵션화)
- 원인
  - 단일 `ExponentialLR(gamma=0.9995)`가 모든 문제에 최적은 아님.
- 분석
  - 다양한 PDE/역문제에서 다른 스케줄러가 유리한 사례 다수.
- 수정 이유
  - 수렴 속도/최종 성능 트레이드오프 최적화.
- 수정 방법(개요)
  - 위치: `src/train.py`
  - 내용: `scheduler_type` 파라미터(`exponential|cosine|plateau|multistep`)와 `scheduler_params` 도입.
- 성공 기준
  - 동일 에포크 대비 더 낮은 손실/빠른 수렴.
- 검증 절차
  - 문제 유형별 AB 테스트(2~3 스케줄러) 후 기본값 선정.

### 3-2. 초기화 전략 실험
- 원인
  - 현재 Kaiming uniform 고정. 역문제/깊은 네트워크에는 다른 전략이 유리할 수 있음.
- 분석
  - 체비쇼프 계수 텐서 구조상 Orthogonal/Xavier/소규모 균등 초기화의 장단점 존재.
- 수정 이유
  - 초기 수렴 안정성/속도 개선.
- 수정 방법(개요)
  - 위치: `src/models.py` → `ChebyKANLayer` 생성자에 `init_method` 옵션 추가.
- 성공 기준
  - 초기 수렴 단계 손실 하강률↑, 재현성 안정.
- 검증 절차
  - 소규모 스윕으로 best 초기화 선택.

### 3-3. 2차 도함수(Laplacian) 계산 효율화
- 원인
  - `autograd.grad(...sum())` 반복 호출은 비용 큼.
- 분석
  - 동일 좌표 집합에서의 2차 도함수를 보다 벡터화/공유 그래프 방식으로 계산 가능.
- 수정 이유
  - 계산 시간 20~40% 절감 기대(문제 규모에 비례).
- 수정 방법(개요)
  - 위치: `src/loss.py`의 PINN 계열 손실 2차 미분 부분
  - 내용: Jacobian/Hessian 대각 추출 벡터화, 필요 시 `torch.func.vmap` 고려(PyTorch 2.x).
- 성공 기준
  - 동일 입력 대비 벽시계시간 감소, 정확도 동일.
- 검증 절차
  - 프로파일링(steps/sec), 메모리 사용량 비교.

---

## 실행 순서 체크리스트

1) 1단계 적용 → 전체 테스트 → 3D Poisson 예제 재실행(기본 안정화 확인)
2) 2단계 적용(각 항목 개별 검증) → PINN 예제 종단 오차/시간 비교
3) 3단계 옵션 도입 → 문제 유형별 스케줄러/초기화 추천값 도출

---

## 기대 효과(정량/정성)

- 1단계: NaN/Inf 감소, 속도 1.5~2x, OOM 감소, 과적합 감소
- 2단계: 동일 포인트로 더 낮은 오차, 자원 효율↑, 수렴 안정↑
- 3단계: 문제별 수렴 최적화, 초기 수렴 개선, 계산 효율성↑

---

## 참고(근거 문헌/문서)
- PyTorch AMP/GradScaler/gradient clipping 공식 가이드(혼합정밀/클리핑 사용 패턴)
- 저장소 코드 근거: `src/train.py`, `src/loss.py`, `src/models.py`, `src/data.py`, 예제/테스트 로그

---

본 가이드에 따라 1단계부터 순차 적용하면, 현재 보고된 예제 실패(Exit Code: 1) 및 높은 오차 지표에 대한 안정화·개선 효과가 먼저 확인될 것이며, 2~3단계를 거치면서 정확도/효율이 단계적으로 향상될 것입니다.
