"""
v2-P1 ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ìŠ¤í¬ë¦½íŠ¸

íƒ€ì¼ë§ ì¶”ë¡  ë° AMPì˜ ì •ëŸ‰ì  ì„±ê³¼ë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤.
"""

import torch
import torch.nn as nn
import numpy as np
import time
import json
import sys
import os
from pathlib import Path

# src ë””ë ‰í† ë¦¬ ì¶”ê°€
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from src.models import UNet
from src.utils.tiling import infer_with_tiling, estimate_memory_usage, tile_image, blend_tiles


def get_gpu_memory_mb():
    """í˜„ì¬ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (MB)"""
    if torch.cuda.is_available():
        return torch.cuda.max_memory_allocated() / (1024 ** 2)
    return 0


def reset_gpu_memory():
    """GPU ë©”ëª¨ë¦¬ í†µê³„ ì´ˆê¸°í™”"""
    if torch.cuda.is_available():
        torch.cuda.reset_peak_memory_stats()
        torch.cuda.empty_cache()


class SimpleUNet(nn.Module):
    """ë²¤ì¹˜ë§ˆí¬ìš© ê°„ë‹¨í•œ UNet"""
    def __init__(self, in_channels=16, out_channels=1):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Conv2d(in_channels, 32, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(32, 64, 3, padding=1),
            nn.ReLU(inplace=True),
        )
        self.decoder = nn.Sequential(
            nn.Conv2d(64, 32, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(32, out_channels, 3, padding=1),
        )
    
    def forward(self, x):
        x = self.encoder(x)
        x = self.decoder(x)
        return x


def benchmark_tiling_inference(device='cuda'):
    """íƒ€ì¼ë§ ì¶”ë¡  ë²¤ì¹˜ë§ˆí¬"""
    print("\n" + "="*70)
    print("ğŸ“Š ë²¤ì¹˜ë§ˆí¬ 1: íƒ€ì¼ë§ ì¶”ë¡  ì„±ëŠ¥")
    print("="*70)
    
    results = {}
    
    # ëª¨ë¸ ìƒì„±
    model = SimpleUNet(in_channels=16, out_channels=1).to(device)
    model.eval()
    
    # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ í¬ê¸°
    test_sizes = [
        (16, 512, 512, "ì‘ì€ ì´ë¯¸ì§€"),
        (16, 1024, 1024, "ì¤‘ê°„ ì´ë¯¸ì§€"),
        (16, 2048, 2048, "í° ì´ë¯¸ì§€"),
    ]
    
    for channels, height, width, label in test_sizes:
        print(f"\ní…ŒìŠ¤íŠ¸: {label} ({channels}Ã—{height}Ã—{width})")
        img = np.random.rand(channels, height, width).astype(np.float32)
        
        # 1. ì§ì ‘ ì¶”ë¡  (ê°€ëŠ¥í•œ ê²½ìš°)
        if height <= 1024:  # ë©”ëª¨ë¦¬ ì œì•½
            print("  ë°©ë²• 1: ì§ì ‘ ì¶”ë¡ ")
            reset_gpu_memory()
            
            try:
                start_time = time.time()
                with torch.no_grad():
                    img_tensor = torch.from_numpy(img).unsqueeze(0).to(device)
                    _ = model(img_tensor)
                    torch.cuda.synchronize()
                direct_time = time.time() - start_time
                direct_memory = get_gpu_memory_mb()
                
                print(f"    ì‹œê°„: {direct_time:.3f}ì´ˆ")
                print(f"    ë©”ëª¨ë¦¬: {direct_memory:.2f} MB")
                
                results[f"{label}_direct"] = {
                    "time_sec": direct_time,
                    "memory_mb": direct_memory
                }
            except RuntimeError as e:
                print(f"    ì‹¤íŒ¨: {str(e)}")
                results[f"{label}_direct"] = {"error": "OOM"}
        else:
            print("  ë°©ë²• 1: ì§ì ‘ ì¶”ë¡  - ê±´ë„ˆëœ€ (ë©”ëª¨ë¦¬ ë¶€ì¡± ì˜ˆìƒ)")
            results[f"{label}_direct"] = {"skipped": True}
        
        # 2. íƒ€ì¼ë§ ì¶”ë¡ 
        print("  ë°©ë²• 2: íƒ€ì¼ë§ ì¶”ë¡  (512Ã—512, ì˜¤ë²„ë© 128)")
        reset_gpu_memory()
        
        start_time = time.time()
        result = infer_with_tiling(
            img,
            model,
            tile_size=512,
            overlap=128,
            device=device,
            batch_size=1,
            verbose=False
        )
        torch.cuda.synchronize()
        tiling_time = time.time() - start_time
        tiling_memory = get_gpu_memory_mb()
        
        print(f"    ì‹œê°„: {tiling_time:.3f}ì´ˆ")
        print(f"    ë©”ëª¨ë¦¬: {tiling_memory:.2f} MB")
        
        results[f"{label}_tiling"] = {
            "time_sec": tiling_time,
            "memory_mb": tiling_memory,
            "output_shape": result.shape
        }
        
        # ë¹„êµ
        if f"{label}_direct" in results and "time_sec" in results[f"{label}_direct"]:
            speedup = results[f"{label}_direct"]["time_sec"] / tiling_time
            memory_reduction = (1 - tiling_memory / results[f"{label}_direct"]["memory_mb"]) * 100
            print(f"    ë¹„êµ: ì†ë„ {speedup:.2f}ë°°, ë©”ëª¨ë¦¬ {memory_reduction:.1f}% ì ˆê°")
    
    return results


def benchmark_reconstruction_accuracy():
    """ì¬êµ¬ì„± ì •í™•ë„ ë²¤ì¹˜ë§ˆí¬"""
    print("\n" + "="*70)
    print("ğŸ“Š ë²¤ì¹˜ë§ˆí¬ 2: íƒ€ì¼ë§ ì¬êµ¬ì„± ì •í™•ë„")
    print("="*70)
    
    results = {}
    
    # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€: ê· ì¼í•œ ê°’
    print("\ní…ŒìŠ¤íŠ¸ 1: ê· ì¼í•œ ì´ë¯¸ì§€ (ì™„ë²½í•œ ì¬êµ¬ì„± ê¸°ëŒ€)")
    img = np.ones((3, 1024, 1024)) * 0.5
    tiles, shape = tile_image(img, tile_size=512, overlap=128)
    reconstructed = blend_tiles(tiles, shape, tile_size=512, overlap=128, device='cpu')
    
    l2_error = np.sqrt(np.mean((reconstructed - img) ** 2))
    relative_l2 = l2_error / (np.sqrt(np.mean(img ** 2)) + 1e-10)
    max_error = np.max(np.abs(reconstructed - img))
    
    print(f"  L2 ì˜¤ì°¨: {l2_error:.6e}")
    print(f"  ìƒëŒ€ L2 ì˜¤ì°¨: {relative_l2:.6e}")
    print(f"  ìµœëŒ€ ì ˆëŒ€ ì˜¤ì°¨: {max_error:.6e}")
    
    results["uniform_image"] = {
        "l2_error": float(l2_error),
        "relative_l2_error": float(relative_l2),
        "max_abs_error": float(max_error)
    }
    
    # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€: ê·¸ë˜ë””ì–¸íŠ¸
    print("\ní…ŒìŠ¤íŠ¸ 2: ê·¸ë˜ë””ì–¸íŠ¸ ì´ë¯¸ì§€")
    x = np.linspace(0, 1, 1024)
    y = np.linspace(0, 1, 1024)
    xx, yy = np.meshgrid(x, y)
    img = np.stack([xx, yy, xx + yy], axis=0)
    
    tiles, shape = tile_image(img, tile_size=512, overlap=128)
    reconstructed = blend_tiles(tiles, shape, tile_size=512, overlap=128, device='cpu')
    
    l2_error = np.sqrt(np.mean((reconstructed - img) ** 2))
    relative_l2 = l2_error / (np.sqrt(np.mean(img ** 2)) + 1e-10)
    max_error = np.max(np.abs(reconstructed - img))
    
    print(f"  L2 ì˜¤ì°¨: {l2_error:.6e}")
    print(f"  ìƒëŒ€ L2 ì˜¤ì°¨: {relative_l2:.6e}")
    print(f"  ìµœëŒ€ ì ˆëŒ€ ì˜¤ì°¨: {max_error:.6e}")
    
    results["gradient_image"] = {
        "l2_error": float(l2_error),
        "relative_l2_error": float(relative_l2),
        "max_abs_error": float(max_error)
    }
    
    # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€: ëœë¤
    print("\ní…ŒìŠ¤íŠ¸ 3: ëœë¤ ì´ë¯¸ì§€")
    np.random.seed(42)
    img = np.random.rand(3, 1024, 1024)
    
    tiles, shape = tile_image(img, tile_size=512, overlap=128)
    reconstructed = blend_tiles(tiles, shape, tile_size=512, overlap=128, device='cpu')
    
    l2_error = np.sqrt(np.mean((reconstructed - img) ** 2))
    relative_l2 = l2_error / (np.sqrt(np.mean(img ** 2)) + 1e-10)
    max_error = np.max(np.abs(reconstructed - img))
    
    print(f"  L2 ì˜¤ì°¨: {l2_error:.6e}")
    print(f"  ìƒëŒ€ L2 ì˜¤ì°¨: {relative_l2:.6e}")
    print(f"  ìµœëŒ€ ì ˆëŒ€ ì˜¤ì°¨: {max_error:.6e}")
    
    results["random_image"] = {
        "l2_error": float(l2_error),
        "relative_l2_error": float(relative_l2),
        "max_abs_error": float(max_error)
    }
    
    return results


def benchmark_memory_estimation():
    """ë©”ëª¨ë¦¬ ì¶”ì • ì •í™•ë„"""
    print("\n" + "="*70)
    print("ğŸ“Š ë²¤ì¹˜ë§ˆí¬ 3: ë©”ëª¨ë¦¬ ì¶”ì • vs ì‹¤ì œ ì‚¬ìš©ëŸ‰")
    print("="*70)
    
    results = {}
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    if device == 'cpu':
        print("CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤.")
        return {"skipped": True}
    
    model = SimpleUNet(in_channels=16, out_channels=1).to(device)
    model.eval()
    model_params = sum(p.numel() for p in model.parameters())
    
    test_cases = [
        (16, 512, 512),
        (16, 1024, 1024),
        (16, 2048, 2048),
    ]
    
    for channels, height, width in test_cases:
        label = f"{channels}Ã—{height}Ã—{width}"
        print(f"\ní…ŒìŠ¤íŠ¸: {label}")
        
        # ì¶”ì •
        estimated = estimate_memory_usage(
            (channels, height, width),
            tile_size=512,
            overlap=128,
            model_params=model_params,
            batch_size=1,
            dtype='float32'
        )
        
        # ì‹¤ì œ ì¸¡ì •
        img = np.random.rand(channels, height, width).astype(np.float32)
        reset_gpu_memory()
        
        _ = infer_with_tiling(
            img,
            model,
            tile_size=512,
            overlap=128,
            device=device,
            batch_size=1,
            verbose=False
        )
        
        actual_mb = get_gpu_memory_mb()
        
        print(f"  ì¶”ì • ë©”ëª¨ë¦¬: {estimated['total_mb']:.2f} MB")
        print(f"  ì‹¤ì œ ë©”ëª¨ë¦¬: {actual_mb:.2f} MB")
        print(f"  ì˜¤ì°¨: {abs(estimated['total_mb'] - actual_mb):.2f} MB ({abs(estimated['total_mb'] - actual_mb) / actual_mb * 100:.1f}%)")
        
        results[label] = {
            "estimated_mb": estimated['total_mb'],
            "actual_mb": float(actual_mb),
            "error_mb": abs(estimated['total_mb'] - actual_mb),
            "error_percent": abs(estimated['total_mb'] - actual_mb) / actual_mb * 100
        }
    
    return results


def benchmark_amp_training(device='cuda'):
    """AMP í›ˆë ¨ ì„±ëŠ¥ ë¹„êµ"""
    print("\n" + "="*70)
    print("ğŸ“Š ë²¤ì¹˜ë§ˆí¬ 4: AMP vs FP32 í›ˆë ¨ ì„±ëŠ¥")
    print("="*70)
    
    if device == 'cpu' or not torch.cuda.is_available():
        print("CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤.")
        return {"skipped": True}
    
    results = {}
    
    # ê°„ë‹¨í•œ í›ˆë ¨ ë£¨í”„
    def train_epochs(model, optimizer, use_amp, num_epochs=10):
        model.train()
        scaler = torch.cuda.amp.GradScaler() if use_amp else None
        
        # ë”ë¯¸ ë°ì´í„°
        data = torch.randn(4, 16, 256, 256, device=device)
        target = torch.randn(4, 1, 256, 256, device=device)
        
        reset_gpu_memory()
        start_time = time.time()
        
        for epoch in range(num_epochs):
            optimizer.zero_grad()
            
            if use_amp:
                with torch.cuda.amp.autocast():
                    output = model(data)
                    loss = nn.functional.mse_loss(output, target)
                scaler.scale(loss).backward()
                scaler.step(optimizer)
                scaler.update()
            else:
                output = model(data)
                loss = nn.functional.mse_loss(output, target)
                loss.backward()
                optimizer.step()
        
        torch.cuda.synchronize()
        elapsed = time.time() - start_time
        memory = get_gpu_memory_mb()
        
        return elapsed, memory
    
    # FP32 í›ˆë ¨
    print("\ní…ŒìŠ¤íŠ¸ 1: FP32 í›ˆë ¨")
    model_fp32 = SimpleUNet(in_channels=16, out_channels=1).to(device)
    optimizer_fp32 = torch.optim.Adam(model_fp32.parameters(), lr=1e-3)
    
    fp32_time, fp32_memory = train_epochs(model_fp32, optimizer_fp32, use_amp=False, num_epochs=20)
    print(f"  ì‹œê°„: {fp32_time:.3f}ì´ˆ")
    print(f"  ë©”ëª¨ë¦¬: {fp32_memory:.2f} MB")
    
    results["fp32"] = {
        "time_sec": fp32_time,
        "memory_mb": float(fp32_memory)
    }
    
    # AMP í›ˆë ¨
    print("\ní…ŒìŠ¤íŠ¸ 2: AMP (FP16) í›ˆë ¨")
    model_amp = SimpleUNet(in_channels=16, out_channels=1).to(device)
    optimizer_amp = torch.optim.Adam(model_amp.parameters(), lr=1e-3)
    
    amp_time, amp_memory = train_epochs(model_amp, optimizer_amp, use_amp=True, num_epochs=20)
    print(f"  ì‹œê°„: {amp_time:.3f}ì´ˆ")
    print(f"  ë©”ëª¨ë¦¬: {amp_memory:.2f} MB")
    
    results["amp"] = {
        "time_sec": amp_time,
        "memory_mb": float(amp_memory)
    }
    
    # ë¹„êµ
    speedup = fp32_time / amp_time
    memory_reduction = (1 - amp_memory / fp32_memory) * 100
    
    print(f"\nâœ… AMP ì„±ëŠ¥ í–¥ìƒ:")
    print(f"  ì†ë„: {speedup:.2f}ë°° ë¹ ë¦„")
    print(f"  ë©”ëª¨ë¦¬: {memory_reduction:.1f}% ì ˆê°")
    
    results["improvement"] = {
        "speedup": speedup,
        "memory_reduction_percent": memory_reduction
    }
    
    return results


def main():
    """ë©”ì¸ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
    print("\n" + "="*70)
    print("ğŸš€ v2-P1 ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘")
    print("="*70)
    
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"ë””ë°”ì´ìŠ¤: {device}")
    
    if device == 'cuda':
        print(f"GPU: {torch.cuda.get_device_name(0)}")
        print(f"CUDA ë²„ì „: {torch.version.cuda}")
    
    all_results = {}
    
    # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
    try:
        all_results["tiling_inference"] = benchmark_tiling_inference(device)
    except Exception as e:
        print(f"âŒ íƒ€ì¼ë§ ì¶”ë¡  ë²¤ì¹˜ë§ˆí¬ ì‹¤íŒ¨: {e}")
        all_results["tiling_inference"] = {"error": str(e)}
    
    try:
        all_results["reconstruction_accuracy"] = benchmark_reconstruction_accuracy()
    except Exception as e:
        print(f"âŒ ì¬êµ¬ì„± ì •í™•ë„ ë²¤ì¹˜ë§ˆí¬ ì‹¤íŒ¨: {e}")
        all_results["reconstruction_accuracy"] = {"error": str(e)}
    
    try:
        all_results["memory_estimation"] = benchmark_memory_estimation()
    except Exception as e:
        print(f"âŒ ë©”ëª¨ë¦¬ ì¶”ì • ë²¤ì¹˜ë§ˆí¬ ì‹¤íŒ¨: {e}")
        all_results["memory_estimation"] = {"error": str(e)}
    
    try:
        all_results["amp_training"] = benchmark_amp_training(device)
    except Exception as e:
        print(f"âŒ AMP í›ˆë ¨ ë²¤ì¹˜ë§ˆí¬ ì‹¤íŒ¨: {e}")
        all_results["amp_training"] = {"error": str(e)}
    
    # ê²°ê³¼ ì €ì¥
    output_dir = Path("outputs/benchmark_results")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    output_file = output_dir / "p1_benchmark_results.json"
    with open(output_file, 'w') as f:
        json.dump(all_results, f, indent=2)
    
    print("\n" + "="*70)
    print("âœ… ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ!")
    print(f"ê²°ê³¼ ì €ì¥: {output_file}")
    print("="*70)
    
    return all_results


if __name__ == '__main__':
    main()
