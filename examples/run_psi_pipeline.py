"""
ìœ„ìƒì²œì´ê°„ì„­ë²•(PSI) ê¸°ë°˜ 3D ë†’ì´ ì¬êµ¬ì„± íŒŒì´í”„ë¼ì¸

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” í•©ì„± bucket ì´ë¯¸ì§€ë¡œë¶€í„° 3D í‘œë©´ ë†’ì´ë¥¼ ì¬êµ¬ì„±í•˜ëŠ”
ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ì œê³µí•©ë‹ˆë‹¤.

ë‹¨ê³„:
    1. generate: í•©ì„± bucket ì´ë¯¸ì§€ ë°ì´í„° ìƒì„±
    2. pretrain: UNet ëª¨ë¸ ì‚¬ì „í•™ìŠµ
    3. finetune: PINN ëª¨ë¸ ë¯¸ì„¸ì¡°ì • (ì„ íƒì )
    4. inference: ìƒˆë¡œìš´ ë°ì´í„°ì— ëŒ€í•œ ì¶”ë¡ 
    5. test: ì„±ëŠ¥ í‰ê°€ ë° ì‹œê°í™”
    6. all: ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (1â†’2â†’4â†’5)

ì‚¬ìš©ë²•:
    # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    python examples/run_psi_pipeline.py all
    
    # ê°œë³„ ë‹¨ê³„ ì‹¤í–‰
    python examples/run_psi_pipeline.py generate
    python examples/run_psi_pipeline.py pretrain
    python examples/run_psi_pipeline.py inference
    python examples/run_psi_pipeline.py test
    
    # ì˜µì…˜ ì§€ì •
    python examples/run_psi_pipeline.py all --num-samples 50 --epochs 100 --device cuda
"""

import argparse
import os
import sys
import json
import torch
import torch.nn as nn
import numpy as np
from pathlib import Path
from PIL import Image
import matplotlib.pyplot as plt
from torch.utils.data import DataLoader
from tqdm import tqdm

# src ë””ë ‰í† ë¦¬ë¥¼ ê²½ë¡œì— ì¶”ê°€
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from src.models import UNet, Scaled_cPIKAN
from src.loss import UNetPhysicsLoss, PinnReconstructionLoss
from src.data_generator import generate_synthetic_data, DEFAULT_WAVELENGTHS
from src.data import WaferPatchDataset


class PSIPipeline:
    """ìœ„ìƒì²œì´ê°„ì„­ë²• ê¸°ë°˜ 3D ì¬êµ¬ì„± íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self, config):
        """
        Args:
            config: argparse Namespace ê°ì²´ (ì„¤ì • ê°’ë“¤)
        """
        self.config = config
        self.device = torch.device(config.device if torch.cuda.is_available() else 'cpu')
        
        # ê²½ë¡œ ì„¤ì •
        self.output_dir = Path(config.output_dir)
        self.data_dir = self.output_dir / 'synthetic_data'
        self.train_dir = self.data_dir / 'train'
        self.test_dir = self.data_dir / 'test'
        self.model_dir = self.output_dir / 'models'
        self.result_dir = self.output_dir / 'results'
        self.viz_dir = self.result_dir / 'visualizations'
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        for dir_path in [self.train_dir, self.test_dir, self.model_dir, 
                        self.result_dir, self.viz_dir]:
            dir_path.mkdir(parents=True, exist_ok=True)
        
        # PSI ì„¤ì •
        self.wavelengths = DEFAULT_WAVELENGTHS
        self.num_buckets = 4
        self.num_channels = len(self.wavelengths) * self.num_buckets  # 16
        
        print(f"ğŸ”§ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"   ë””ë°”ì´ìŠ¤: {self.device}")
        print(f"   ì¶œë ¥ ë””ë ‰í† ë¦¬: {self.output_dir}")
        print(f"   íŒŒì¥: {self.wavelengths}")
        print(f"   Bucket ìˆ˜: {self.num_buckets}")
        print(f"   ì…ë ¥ ì±„ë„: {self.num_channels}")
    
    def generate_data(self):
        """í•©ì„± bucket ì´ë¯¸ì§€ ë°ì´í„° ìƒì„±"""
        print("\n" + "="*70)
        print("ğŸ“Š 1ë‹¨ê³„: í•©ì„± ë°ì´í„° ìƒì„±")
        print("="*70)
        
        num_train = self.config.num_train_samples
        num_test = self.config.num_test_samples
        img_size = self.config.image_size
        
        print(f"í›ˆë ¨ ìƒ˜í”Œ: {num_train}, í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ: {num_test}")
        print(f"ì´ë¯¸ì§€ í¬ê¸°: {img_size}x{img_size}")
        
        # ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ê³ ì •
        np.random.seed(42)
        
        # í›ˆë ¨ ë°ì´í„° ìƒì„±
        print(f"\nğŸ“ í›ˆë ¨ ë°ì´í„° ìƒì„± ì¤‘... ({self.train_dir})")
        for i in tqdm(range(num_train), desc="í›ˆë ¨ ë°ì´í„°"):
            # ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ì„¤ì •
            np.random.seed(42 + i)
            
            sample_dir = self.train_dir / f"sample_{i:04d}"
            sample_dir.mkdir(exist_ok=True)
            
            height_map, buckets = generate_synthetic_data(
                shape=(img_size, img_size),
                wavelengths=self.wavelengths,
                num_buckets=self.num_buckets,
                save_path=None  # ìˆ˜ë™ ì €ì¥
            )
            
            # Ground truth ì €ì¥
            np.save(sample_dir / "ground_truth.npy", height_map)
            
            # Bucket ì´ë¯¸ì§€ ì €ì¥ (BMP í˜•ì‹)
            for laser_idx in range(len(self.wavelengths)):
                for bucket_idx in range(self.num_buckets):
                    channel_idx = laser_idx * self.num_buckets + bucket_idx
                    bucket_img = buckets[laser_idx, bucket_idx]
                    img = Image.fromarray(bucket_img.astype(np.uint8))
                    img.save(sample_dir / f"bucket_{channel_idx:02d}.bmp")
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
        print(f"\nğŸ“ í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± ì¤‘... ({self.test_dir})")
        for i in tqdm(range(num_test), desc="í…ŒìŠ¤íŠ¸ ë°ì´í„°"):
            # í…ŒìŠ¤íŠ¸ëŠ” ë‹¤ë¥¸ ì‹œë“œ ì‚¬ìš©
            np.random.seed(10000 + i)
            
            sample_dir = self.test_dir / f"sample_{i:04d}"
            sample_dir.mkdir(exist_ok=True)
            
            height_map, buckets = generate_synthetic_data(
                shape=(img_size, img_size),
                wavelengths=self.wavelengths,
                num_buckets=self.num_buckets,
                save_path=None  # ìˆ˜ë™ ì €ì¥
            )
            
            np.save(sample_dir / "ground_truth.npy", height_map)
            
            for laser_idx in range(len(self.wavelengths)):
                for bucket_idx in range(self.num_buckets):
                    channel_idx = laser_idx * self.num_buckets + bucket_idx
                    bucket_img = buckets[laser_idx, bucket_idx]
                    img = Image.fromarray(bucket_img.astype(np.uint8))
                    img.save(sample_dir / f"bucket_{channel_idx:02d}.bmp")
        
        print(f"\nâœ… ë°ì´í„° ìƒì„± ì™„ë£Œ!")
        print(f"   í›ˆë ¨: {num_train}ê°œ ìƒ˜í”Œ")
        print(f"   í…ŒìŠ¤íŠ¸: {num_test}ê°œ ìƒ˜í”Œ")
    
    def train_unet(self):
        """UNet ëª¨ë¸ ì‚¬ì „í•™ìŠµ"""
        print("\n" + "="*70)
        print("ğŸ“ 2ë‹¨ê³„: UNet ì‚¬ì „í•™ìŠµ")
        print("="*70)
        
        # ë°ì´í„°ì…‹ ë° ë°ì´í„°ë¡œë” ì¤€ë¹„
        train_dataset = WaferPatchDataset(
            data_dir=str(self.train_dir),
            patch_size=self.config.patch_size,
            num_channels=self.num_channels,
            output_format='bmp'
        )
        
        train_loader = DataLoader(
            train_dataset,
            batch_size=self.config.batch_size,
            shuffle=True,
            num_workers=0  # Windowsì—ì„œëŠ” 0 ê¶Œì¥
        )
        
        print(f"í›ˆë ¨ ë°ì´í„°: {len(train_dataset)}ê°œ íŒ¨ì¹˜")
        print(f"ë°°ì¹˜ í¬ê¸°: {self.config.batch_size}")
        
        # ëª¨ë¸ ìƒì„±
        model = UNet(
            n_channels=self.num_channels,
            n_classes=1,
            bilinear=True
        ).to(self.device)
        
        print(f"ëª¨ë¸ íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in model.parameters()):,}")
        
        # ì†ì‹¤ í•¨ìˆ˜ ë° ì˜µí‹°ë§ˆì´ì €
        criterion = UNetPhysicsLoss(
            wavelengths=self.wavelengths,
            num_buckets=self.num_buckets,
            smoothness_weight=self.config.smoothness_weight
        )
        
        optimizer = torch.optim.Adam(model.parameters(), lr=self.config.learning_rate)
        scheduler = torch.optim.lr_scheduler.StepLR(
            optimizer, 
            step_size=self.config.epochs // 3, 
            gamma=0.5
        )
        
        # í•™ìŠµ
        print(f"\nğŸš€ í•™ìŠµ ì‹œì‘ (ì—í¬í¬: {self.config.epochs})")
        best_loss = float('inf')
        history = {'epoch': [], 'loss': [], 'loss_data': [], 'loss_smoothness': []}
        
        for epoch in range(self.config.epochs):
            model.train()
            epoch_loss = 0.0
            epoch_metrics = {'loss_data': 0.0, 'loss_smoothness': 0.0}
            
            pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{self.config.epochs}")
            for batch_idx, (input_buckets, target_height) in enumerate(pbar):
                input_buckets = input_buckets.to(self.device)
                target_height = target_height.to(self.device)
                
                optimizer.zero_grad()
                
                # ìˆœì „íŒŒ
                predicted_height = model(input_buckets)
                loss = criterion(predicted_height, input_buckets)
                
                # ì—­ì „íŒŒ
                loss.backward()
                optimizer.step()
                
                # ë©”íŠ¸ë¦­ ëˆ„ì 
                epoch_loss += loss.item()
                for key in ['loss_data', 'loss_smoothness']:
                    epoch_metrics[key] += criterion.metrics[key]
                
                # ì§„í–‰ í‘œì‹œì¤„ ì—…ë°ì´íŠ¸
                pbar.set_postfix({
                    'loss': f"{loss.item():.4e}",
                    'data': f"{criterion.metrics['loss_data']:.4e}",
                    'smooth': f"{criterion.metrics['loss_smoothness']:.4e}"
                })
            
            # ì—í¬í¬ í‰ê· 
            avg_loss = epoch_loss / len(train_loader)
            for key in epoch_metrics:
                epoch_metrics[key] /= len(train_loader)
            
            # ê¸°ë¡
            history['epoch'].append(epoch)
            history['loss'].append(avg_loss)
            history['loss_data'].append(epoch_metrics['loss_data'])
            history['loss_smoothness'].append(epoch_metrics['loss_smoothness'])
            
            # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬
            scheduler.step()
            
            # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
            if avg_loss < best_loss:
                best_loss = avg_loss
                checkpoint = {
                    'epoch': epoch,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'loss': avg_loss,
                    'config': vars(self.config)
                }
                torch.save(checkpoint, self.model_dir / 'unet_best.pth')
                print(f"âœ¨ ìµœê³  ëª¨ë¸ ì €ì¥ (loss: {best_loss:.6e})")
        
        # ìµœì¢… ëª¨ë¸ ì €ì¥
        torch.save(model.state_dict(), self.model_dir / 'unet_final.pth')
        
        # í•™ìŠµ ì´ë ¥ ì €ì¥
        with open(self.result_dir / 'unet_history.json', 'w') as f:
            json.dump(history, f, indent=2)
        
        # í•™ìŠµ ê³¡ì„  ì‹œê°í™”
        self._plot_training_curve(history, 'UNet')
        
        print(f"\nâœ… UNet í•™ìŠµ ì™„ë£Œ!")
        print(f"   ìµœê³  ì†ì‹¤: {best_loss:.6e}")
        print(f"   ëª¨ë¸ ì €ì¥: {self.model_dir / 'unet_best.pth'}")
    
    def finetune_pinn(self):
        """PINN ëª¨ë¸ ë¯¸ì„¸ì¡°ì • (ì„ íƒì )"""
        print("\n" + "="*70)
        print("ğŸ”¬ 3ë‹¨ê³„: PINN ë¯¸ì„¸ì¡°ì • (ì„ íƒì )")
        print("="*70)
        print("âš ï¸  ì´ ê¸°ëŠ¥ì€ ê³ ê¸‰ ì‚¬ìš©ìë¥¼ ìœ„í•œ ê²ƒì…ë‹ˆë‹¤.")
        print("    ëŒ€ë¶€ë¶„ì˜ ê²½ìš° UNetë§Œìœ¼ë¡œ ì¶©ë¶„í•©ë‹ˆë‹¤.")
        print("    êµ¬í˜„ ì˜ˆì •...")
    
    def inference(self):
        """ì¶”ë¡  ì‹¤í–‰"""
        print("\n" + "="*70)
        print("ğŸ”® 4ë‹¨ê³„: ì¶”ë¡ ")
        print("="*70)
        
        # ëª¨ë¸ ë¡œë“œ
        model = UNet(
            n_channels=self.num_channels,
            n_classes=1,
            bilinear=True
        ).to(self.device)
        
        checkpoint_path = self.model_dir / 'unet_best.pth'
        if not checkpoint_path.exists():
            print(f"âŒ ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {checkpoint_path}")
            print("   ë¨¼ì € 'pretrain' ë‹¨ê³„ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
            return
        
        checkpoint = torch.load(checkpoint_path, map_location=self.device)
        model.load_state_dict(checkpoint['model_state_dict'])
        model.eval()
        
        print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {checkpoint_path}")
        print(f"   í•™ìŠµ ì—í¬í¬: {checkpoint['epoch']}")
        print(f"   í•™ìŠµ ì†ì‹¤: {checkpoint['loss']:.6e}")
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì¤€ë¹„
        test_dataset = WaferPatchDataset(
            data_dir=str(self.test_dir),
            patch_size=self.config.patch_size,
            num_channels=self.num_channels,
            output_format='bmp'
        )
        
        test_loader = DataLoader(
            test_dataset,
            batch_size=1,
            shuffle=False,
            num_workers=0
        )
        
        print(f"\nğŸ” ì¶”ë¡  ì‹¤í–‰ ì¤‘... ({len(test_dataset)}ê°œ ìƒ˜í”Œ)")
        
        # ì¶”ë¡  ê²°ê³¼ ì €ì¥
        predictions = []
        ground_truths = []
        
        with torch.no_grad():
            for idx, (input_buckets, target_height) in enumerate(tqdm(test_loader, desc="ì¶”ë¡ ")):
                input_buckets = input_buckets.to(self.device)
                
                # ì˜ˆì¸¡
                predicted_height = model(input_buckets)
                
                # CPUë¡œ ì´ë™ ë° ì €ì¥
                pred = predicted_height.cpu().numpy().squeeze()
                gt = target_height.numpy().squeeze()
                
                predictions.append(pred)
                ground_truths.append(gt)
                
                # ì²˜ìŒ ëª‡ ê°œë§Œ ì‹œê°í™” ì €ì¥
                if idx < self.config.num_visualize:
                    self._save_inference_visualization(
                        pred, gt, idx, 
                        input_buckets.cpu().numpy().squeeze()
                    )
        
        # ê²°ê³¼ ì €ì¥
        results = {
            'predictions': [p.tolist() for p in predictions],
            'ground_truths': [g.tolist() for g in ground_truths]
        }
        
        with open(self.result_dir / 'inference_results.json', 'w') as f:
            json.dump(results, f)
        
        print(f"\nâœ… ì¶”ë¡  ì™„ë£Œ!")
        print(f"   ê²°ê³¼ ì €ì¥: {self.result_dir / 'inference_results.json'}")
        print(f"   ì‹œê°í™”: {self.viz_dir} (ìƒìœ„ {self.config.num_visualize}ê°œ)")
    
    def test(self):
        """ì„±ëŠ¥ í‰ê°€"""
        print("\n" + "="*70)
        print("ğŸ“Š 5ë‹¨ê³„: ì„±ëŠ¥ í‰ê°€")
        print("="*70)
        
        # ì¶”ë¡  ê²°ê³¼ ë¡œë“œ
        results_path = self.result_dir / 'inference_results.json'
        if not results_path.exists():
            print(f"âŒ ì¶”ë¡  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤: {results_path}")
            print("   ë¨¼ì € 'inference' ë‹¨ê³„ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
            return
        
        with open(results_path, 'r') as f:
            results = json.load(f)
        
        predictions = [np.array(p) for p in results['predictions']]
        ground_truths = [np.array(g) for g in results['ground_truths']]
        
        print(f"í‰ê°€ ìƒ˜í”Œ ìˆ˜: {len(predictions)}")
        
        # ë©”íŠ¸ë¦­ ê³„ì‚°
        rmse_list = []
        mae_list = []
        mape_list = []
        
        for pred, gt in zip(predictions, ground_truths):
            # RMSE
            rmse = np.sqrt(np.mean((pred - gt) ** 2))
            rmse_list.append(rmse)
            
            # MAE
            mae = np.mean(np.abs(pred - gt))
            mae_list.append(mae)
            
            # MAPE (í‰ê·  ì ˆëŒ€ ë°±ë¶„ìœ¨ ì˜¤ì°¨)
            # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
            mask = np.abs(gt) > 1e-10
            if mask.any():
                mape = np.mean(np.abs((pred[mask] - gt[mask]) / gt[mask])) * 100
                mape_list.append(mape)
        
        # í†µê³„
        metrics = {
            'rmse_mean': float(np.mean(rmse_list)),
            'rmse_std': float(np.std(rmse_list)),
            'rmse_min': float(np.min(rmse_list)),
            'rmse_max': float(np.max(rmse_list)),
            'mae_mean': float(np.mean(mae_list)),
            'mae_std': float(np.std(mae_list)),
            'mape_mean': float(np.mean(mape_list)) if mape_list else None,
            'mape_std': float(np.std(mape_list)) if mape_list else None,
            'num_samples': len(predictions)
        }
        
        # ê²°ê³¼ ì¶œë ¥
        print("\nğŸ“ˆ í‰ê°€ ê²°ê³¼:")
        print(f"   RMSE: {metrics['rmse_mean']:.6e} Â± {metrics['rmse_std']:.6e}")
        print(f"         (min: {metrics['rmse_min']:.6e}, max: {metrics['rmse_max']:.6e})")
        print(f"   MAE:  {metrics['mae_mean']:.6e} Â± {metrics['mae_std']:.6e}")
        if metrics['mape_mean'] is not None:
            print(f"   MAPE: {metrics['mape_mean']:.2f}% Â± {metrics['mape_std']:.2f}%")
        
        # ë©”íŠ¸ë¦­ ì €ì¥
        with open(self.result_dir / 'metrics.json', 'w') as f:
            json.dump(metrics, f, indent=2)
        
        # ë©”íŠ¸ë¦­ ì‹œê°í™”
        self._plot_metrics(rmse_list, mae_list, mape_list)
        
        print(f"\nâœ… í‰ê°€ ì™„ë£Œ!")
        print(f"   ë©”íŠ¸ë¦­ ì €ì¥: {self.result_dir / 'metrics.json'}")
    
    def run_all(self):
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        print("\n" + "="*70)
        print("ğŸš€ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰")
        print("="*70)
        
        self.generate_data()
        self.train_unet()
        self.inference()
        self.test()
        
        print("\n" + "="*70)
        print("ğŸ‰ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
        print("="*70)
        print(f"ê²°ê³¼ ë””ë ‰í† ë¦¬: {self.output_dir}")
    
    def _plot_training_curve(self, history, model_name):
        """í•™ìŠµ ê³¡ì„  ì‹œê°í™”"""
        fig, axes = plt.subplots(1, 2, figsize=(12, 4))
        
        # ì´ ì†ì‹¤
        axes[0].plot(history['epoch'], history['loss'], 'b-', linewidth=2)
        axes[0].set_xlabel('Epoch')
        axes[0].set_ylabel('Total Loss')
        axes[0].set_title(f'{model_name} Training Loss')
        axes[0].grid(True, alpha=0.3)
        axes[0].set_yscale('log')
        
        # ì„¸ë¶€ ì†ì‹¤
        axes[1].plot(history['epoch'], history['loss_data'], 'r-', label='Data Loss', linewidth=2)
        axes[1].plot(history['epoch'], history['loss_smoothness'], 'g-', label='Smoothness Loss', linewidth=2)
        axes[1].set_xlabel('Epoch')
        axes[1].set_ylabel('Loss')
        axes[1].set_title(f'{model_name} Loss Components')
        axes[1].legend()
        axes[1].grid(True, alpha=0.3)
        axes[1].set_yscale('log')
        
        plt.tight_layout()
        plt.savefig(self.result_dir / f'{model_name.lower()}_training_curve.png', dpi=150, bbox_inches='tight')
        plt.close()
    
    def _save_inference_visualization(self, pred, gt, idx, buckets):
        """ì¶”ë¡  ê²°ê³¼ ì‹œê°í™”"""
        fig, axes = plt.subplots(2, 3, figsize=(15, 10))
        
        # ì˜ˆì¸¡ ë†’ì´
        im1 = axes[0, 0].imshow(pred, cmap='viridis')
        axes[0, 0].set_title('ì˜ˆì¸¡ ë†’ì´')
        axes[0, 0].axis('off')
        plt.colorbar(im1, ax=axes[0, 0])
        
        # Ground truth
        im2 = axes[0, 1].imshow(gt, cmap='viridis')
        axes[0, 1].set_title('Ground Truth')
        axes[0, 1].axis('off')
        plt.colorbar(im2, ax=axes[0, 1])
        
        # ì ˆëŒ€ ì˜¤ì°¨
        error = np.abs(pred - gt)
        im3 = axes[0, 2].imshow(error, cmap='hot')
        axes[0, 2].set_title('ì ˆëŒ€ ì˜¤ì°¨')
        axes[0, 2].axis('off')
        plt.colorbar(im3, ax=axes[0, 2])
        
        # Bucket ì´ë¯¸ì§€ ìƒ˜í”Œ (ì²˜ìŒ 3ê°œ)
        for i in range(3):
            if i < buckets.shape[0]:
                axes[1, i].imshow(buckets[i], cmap='gray')
                axes[1, i].set_title(f'Bucket {i}')
                axes[1, i].axis('off')
        
        plt.tight_layout()
        plt.savefig(self.viz_dir / f'inference_{idx:04d}.png', dpi=150, bbox_inches='tight')
        plt.close()
    
    def _plot_metrics(self, rmse_list, mae_list, mape_list):
        """ë©”íŠ¸ë¦­ ë¶„í¬ ì‹œê°í™”"""
        fig, axes = plt.subplots(1, 3, figsize=(15, 4))
        
        # RMSE íˆìŠ¤í† ê·¸ë¨
        axes[0].hist(rmse_list, bins=20, color='blue', alpha=0.7, edgecolor='black')
        axes[0].set_xlabel('RMSE')
        axes[0].set_ylabel('ë¹ˆë„')
        axes[0].set_title(f'RMSE ë¶„í¬\ní‰ê· : {np.mean(rmse_list):.6e}')
        axes[0].grid(True, alpha=0.3)
        
        # MAE íˆìŠ¤í† ê·¸ë¨
        axes[1].hist(mae_list, bins=20, color='green', alpha=0.7, edgecolor='black')
        axes[1].set_xlabel('MAE')
        axes[1].set_ylabel('ë¹ˆë„')
        axes[1].set_title(f'MAE ë¶„í¬\ní‰ê· : {np.mean(mae_list):.6e}')
        axes[1].grid(True, alpha=0.3)
        
        # MAPE íˆìŠ¤í† ê·¸ë¨
        if mape_list:
            axes[2].hist(mape_list, bins=20, color='red', alpha=0.7, edgecolor='black')
            axes[2].set_xlabel('MAPE (%)')
            axes[2].set_ylabel('ë¹ˆë„')
            axes[2].set_title(f'MAPE ë¶„í¬\ní‰ê· : {np.mean(mape_list):.2f}%')
            axes[2].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(self.result_dir / 'metrics_distribution.png', dpi=150, bbox_inches='tight')
        plt.close()


def parse_args():
    """ëª…ë ¹ì¤„ ì¸ì íŒŒì‹±"""
    parser = argparse.ArgumentParser(
        description='ìœ„ìƒì²œì´ê°„ì„­ë²•(PSI) ê¸°ë°˜ 3D ë†’ì´ ì¬êµ¬ì„± íŒŒì´í”„ë¼ì¸',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì œ:
  # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ê¸°ë³¸ ì„¤ì •)
  python %(prog)s all
  
  # ê°œë³„ ë‹¨ê³„ ì‹¤í–‰
  python %(prog)s generate
  python %(prog)s pretrain
  python %(prog)s inference
  python %(prog)s test
  
  # ì˜µì…˜ ì§€ì •
  python %(prog)s all --num-train-samples 100 --epochs 50 --device cuda
  python %(prog)s pretrain --batch-size 8 --learning-rate 0.001
        """
    )
    
    # í•„ìˆ˜ ì¸ì: ëª…ë ¹ì–´
    parser.add_argument(
        'command',
        choices=['generate', 'pretrain', 'finetune', 'inference', 'test', 'all'],
        help='ì‹¤í–‰í•  ë‹¨ê³„ ì„ íƒ'
    )
    
    # ê²½ë¡œ ì„¤ì •
    parser.add_argument(
        '--output-dir',
        type=str,
        default='outputs',
        help='ì¶œë ¥ ë””ë ‰í† ë¦¬ ê²½ë¡œ (ê¸°ë³¸ê°’: outputs)'
    )
    
    # ë°ì´í„° ìƒì„± ì˜µì…˜
    parser.add_argument(
        '--num-train-samples',
        type=int,
        default=20,
        help='í›ˆë ¨ ìƒ˜í”Œ ìˆ˜ (ê¸°ë³¸ê°’: 20)'
    )
    parser.add_argument(
        '--num-test-samples',
        type=int,
        default=5,
        help='í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ìˆ˜ (ê¸°ë³¸ê°’: 5)'
    )
    parser.add_argument(
        '--image-size',
        type=int,
        default=256,
        help='ìƒì„±í•  ì´ë¯¸ì§€ í¬ê¸° (ê¸°ë³¸ê°’: 256)'
    )
    
    # í•™ìŠµ ì˜µì…˜
    parser.add_argument(
        '--epochs',
        type=int,
        default=50,
        help='í•™ìŠµ ì—í¬í¬ ìˆ˜ (ê¸°ë³¸ê°’: 50)'
    )
    parser.add_argument(
        '--batch-size',
        type=int,
        default=4,
        help='ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ê°’: 4)'
    )
    parser.add_argument(
        '--learning-rate',
        type=float,
        default=1e-3,
        help='í•™ìŠµë¥  (ê¸°ë³¸ê°’: 0.001)'
    )
    parser.add_argument(
        '--patch-size',
        type=int,
        default=256,
        help='íŒ¨ì¹˜ í¬ê¸° (ê¸°ë³¸ê°’: 256)'
    )
    parser.add_argument(
        '--smoothness-weight',
        type=float,
        default=1e-4,
        help='í‰í™œë„ ì†ì‹¤ ê°€ì¤‘ì¹˜ (ê¸°ë³¸ê°’: 1e-4)'
    )
    
    # ì¶”ë¡ /í‰ê°€ ì˜µì…˜
    parser.add_argument(
        '--num-visualize',
        type=int,
        default=5,
        help='ì‹œê°í™”í•  ê²°ê³¼ ìˆ˜ (ê¸°ë³¸ê°’: 5)'
    )
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    parser.add_argument(
        '--device',
        type=str,
        default='cuda',
        choices=['cuda', 'cpu'],
        help='ì—°ì‚° ë””ë°”ì´ìŠ¤ (ê¸°ë³¸ê°’: cuda)'
    )
    
    return parser.parse_args()


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    args = parse_args()
    
    # íŒŒì´í”„ë¼ì¸ ìƒì„±
    pipeline = PSIPipeline(args)
    
    # ëª…ë ¹ì–´ì— ë”°ë¼ ì‹¤í–‰
    if args.command == 'generate':
        pipeline.generate_data()
    elif args.command == 'pretrain':
        pipeline.train_unet()
    elif args.command == 'finetune':
        pipeline.finetune_pinn()
    elif args.command == 'inference':
        pipeline.inference()
    elif args.command == 'test':
        pipeline.test()
    elif args.command == 'all':
        pipeline.run_all()


if __name__ == '__main__':
    main()
