"""
ê³ ê¸‰ ê¸°ëŠ¥ì„ ì‚¬ìš©í•œ 1D Poisson ë°©ì •ì‹ í•´ê²°

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” Scaled-cPIKANì˜ ê³ ê¸‰ ê¸°ëŠ¥ë“¤ì„ ì‹œì—°í•©ë‹ˆë‹¤:
1. ë™ì  ì†ì‹¤ ê°€ì¤‘ì¹˜ (DynamicWeightedLoss)
2. ì ì‘í˜• ì½œë¡œì¼€ì´ì…˜ ìƒ˜í”Œë§ (AdaptiveResidualSampler)

ë¬¸ì œ ì„¤ì •:
    u''(x) = -Ï€Â²sin(Ï€x),  x âˆˆ [0,1]
    u(0) = 0,  u(1) = 0
    
ë¶„ì„í•´:
    u(x) = sin(Ï€x)

ì‚¬ìš©ë²•:
    # ê¸°ë³¸ ì‹¤í–‰ (ëª¨ë“  ê³ ê¸‰ ê¸°ëŠ¥ í™œì„±í™”)
    python examples/solve_poisson_1d_advanced.py
    
    # ë™ì  ê°€ì¤‘ì¹˜ë§Œ ì‚¬ìš©
    python examples/solve_poisson_1d_advanced.py --use-dynamic-weights --no-adaptive-sampling
    
    # ì ì‘í˜• ìƒ˜í”Œë§ë§Œ ì‚¬ìš©
    python examples/solve_poisson_1d_advanced.py --no-dynamic-weights --use-adaptive-sampling
    
    # ëª¨ë“  ê³ ê¸‰ ê¸°ëŠ¥ ë¹„í™œì„±í™” (ê¸°ì¡´ ë°©ì‹)
    python examples/solve_poisson_1d_advanced.py --no-dynamic-weights --no-adaptive-sampling
"""

import argparse
import torch
import numpy as np
import matplotlib.pyplot as plt
import sys
import os
from pathlib import Path

# src ë””ë ‰í† ë¦¬ ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from src.models import Scaled_cPIKAN
from src.loss import PhysicsInformedLoss, DynamicWeightedLoss
from src.data import LatinHypercubeSampler, AdaptiveResidualSampler


def analytical_solution(x):
    """ë¶„ì„í•´: u(x) = sin(Ï€x)"""
    return torch.sin(np.pi * x)


def create_pde_residual_fn():
    """PDE ì”ì°¨ í•¨ìˆ˜ ìƒì„±"""
    def pde_residual_fn(model, points):
        """
        1D Poisson: u''(x) = -Ï€Â²sin(Ï€x)
        ì”ì°¨: u'' + Ï€Â²sin(Ï€x) = 0
        """
        points.requires_grad_(True)
        u = model(points)
        
        # 1ì°¨ ë„í•¨ìˆ˜
        u_x = torch.autograd.grad(u.sum(), points, create_graph=True)[0]
        # 2ì°¨ ë„í•¨ìˆ˜
        u_xx = torch.autograd.grad(u_x.sum(), points, create_graph=True)[0]
        
        # ì†ŒìŠ¤ í•­
        source = np.pi**2 * torch.sin(np.pi * points)
        
        # ì”ì°¨
        residual = u_xx + source
        return residual
    
    return pde_residual_fn


def create_bc_fn():
    """ê²½ê³„ ì¡°ê±´ í•¨ìˆ˜ ìƒì„±"""
    def bc_fn(model, points):
        """u(0) = 0, u(1) = 0"""
        u = model(points)
        return u  # ê²½ê³„ì—ì„œ 0ì´ì–´ì•¼ í•¨
    
    return bc_fn


def compute_l2_error(model, device, domain_min=0.0, domain_max=1.0, n_test=1000):
    """ìƒëŒ€ L2 ì˜¤ì°¨ ê³„ì‚°"""
    model.eval()
    with torch.no_grad():
        x_test = torch.linspace(domain_min, domain_max, n_test, device=device).reshape(-1, 1)
        u_pred = model(x_test)
        u_exact = analytical_solution(x_test)
        
        l2_error = torch.sqrt(torch.mean((u_pred - u_exact)**2))
        l2_norm = torch.sqrt(torch.mean(u_exact**2))
        relative_error = (l2_error / l2_norm).item()
    
    return relative_error


def train(model, loss_fn, sampler, args, device):
    """ëª¨ë¸ í›ˆë ¨"""
    print(f"\n{'='*70}")
    print(f"ğŸš€ í›ˆë ¨ ì‹œì‘")
    print(f"{'='*70}")
    print(f"ì—í¬í¬: {args.epochs}")
    print(f"ë™ì  ê°€ì¤‘ì¹˜: {'âœ…' if args.use_dynamic_weights else 'âŒ'}")
    print(f"ì ì‘í˜• ìƒ˜í”Œë§: {'âœ…' if args.use_adaptive_sampling else 'âŒ'}")
    
    # ì˜µí‹°ë§ˆì´ì €
    optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)
    
    # ê²½ê³„ í¬ì¸íŠ¸ (ê³ ì •)
    bc_points_0 = torch.tensor([[args.domain_min]], device=device)
    bc_points_1 = torch.tensor([[args.domain_max]], device=device)
    bc_points_dicts = [
        {'points': bc_points_0},
        {'points': bc_points_1}
    ]
    
    # ë©”íŠ¸ë¦­ ê¸°ë¡
    history = {
        'epoch': [],
        'loss': [],
        'error': [],
        'n_points': []
    }
    
    # ê°€ì¤‘ì¹˜ ê¸°ë¡ (ë™ì  ê°€ì¤‘ì¹˜ ì‚¬ìš© ì‹œ)
    if args.use_dynamic_weights:
        history['weight_pde'] = []
        history['weight_bc'] = []
    
    # ì ì‘í˜• ìƒ˜í”Œë§ ì„¤ì •
    is_adaptive = args.use_adaptive_sampling
    refinement_interval = args.refinement_interval if is_adaptive else None
    
    model.train()
    
    for epoch in range(args.epochs):
        # ìƒ˜í”Œ í¬ì¸íŠ¸ ê°€ì ¸ì˜¤ê¸°
        if is_adaptive:
            pde_points = sampler.get_current_points()
        else:
            pde_points = sampler.sample()
        
        # ìˆœì „íŒŒ
        optimizer.zero_grad()
        total_loss, loss_dict = loss_fn(model, pde_points, bc_points_dicts)
        
        # ì—­ì „íŒŒ
        total_loss.backward()
        optimizer.step()
        
        # ì ì‘í˜• ìƒ˜í”Œë§: ì”ì°¨ ì—…ë°ì´íŠ¸ ë° ì •ì œ
        if is_adaptive and (epoch + 1) % refinement_interval == 0:
            # ì™„ì „íˆ ë…ë¦½ì ì¸ forward passë¡œ ì”ì°¨ ê³„ì‚°
            model.eval()
            # í˜„ì¬ í¬ì¸íŠ¸ì˜ ìƒˆë¡œìš´ ë³µì‚¬ë³¸ ìƒì„±
            with torch.enable_grad():  # gradient ê³„ì‚° í™œì„±í™”
                pde_points_copy = sampler.get_current_points().clone().detach().requires_grad_(True)
                pde_residual_fn = create_pde_residual_fn()
                residuals = pde_residual_fn(model, pde_points_copy)
                # ê³„ì‚° ê·¸ë˜í”„ì™€ ë¶„ë¦¬
                sampler.update_residuals(residuals.detach())
            
            refined = sampler.refine()
            if refined:
                n_points = sampler.get_current_points().shape[0]
                print(f"  [Epoch {epoch+1}] ğŸ“ˆ ì •ì œ ì™„ë£Œ â†’ {n_points}ê°œ í¬ì¸íŠ¸")
            
            model.train()
        
        # ë©”íŠ¸ë¦­ ê¸°ë¡ (ë§¤ log_interval ë§ˆë‹¤)
        if (epoch + 1) % args.log_interval == 0 or epoch == 0:
            error = compute_l2_error(model, device, args.domain_min, args.domain_max)
            n_points = pde_points.shape[0]
            
            history['epoch'].append(epoch + 1)
            history['loss'].append(total_loss.item())
            history['error'].append(error)
            history['n_points'].append(n_points)
            
            # ë™ì  ê°€ì¤‘ì¹˜ ê¸°ë¡
            if args.use_dynamic_weights and 'weights' in loss_dict:
                weights = loss_dict['weights']
                history['weight_pde'].append(weights.get('loss_pde', 1.0))
                history['weight_bc'].append(weights.get('loss_bc', 1.0))
            
            # ì¶œë ¥
            log_str = f"  Epoch {epoch+1:5d} | Loss: {total_loss.item():.4e} | Error: {error:.4e} | Points: {n_points}"
            
            if args.use_dynamic_weights and 'weights' in loss_dict:
                weights = loss_dict['weights']
                log_str += f" | w_pde: {weights['loss_pde']:.3f} | w_bc: {weights['loss_bc']:.3f}"
            
            print(log_str)
    
    print(f"\nâœ… í›ˆë ¨ ì™„ë£Œ!")
    
    return history


def visualize_results(model, history, args, device):
    """ê²°ê³¼ ì‹œê°í™”"""
    print(f"\n{'='*70}")
    print(f"ğŸ“Š ê²°ê³¼ ì‹œê°í™”")
    print(f"{'='*70}")
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # 1. ì˜ˆì¸¡ vs ë¶„ì„í•´
    model.eval()
    with torch.no_grad():
        x_test = torch.linspace(args.domain_min, args.domain_max, 1000, device=device).reshape(-1, 1)
        u_pred = model(x_test).cpu().numpy()
        u_exact = analytical_solution(x_test).cpu().numpy()
        x_test_np = x_test.cpu().numpy()
    
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
    
    # (a) ì˜ˆì¸¡ vs ë¶„ì„í•´
    axes[0, 0].plot(x_test_np, u_exact, 'b-', linewidth=2, label='ë¶„ì„í•´')
    axes[0, 0].plot(x_test_np, u_pred, 'r--', linewidth=2, label='ì˜ˆì¸¡')
    axes[0, 0].set_xlabel('x')
    axes[0, 0].set_ylabel('u(x)')
    axes[0, 0].set_title('ì˜ˆì¸¡ vs ë¶„ì„í•´')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)
    
    # (b) ì ˆëŒ€ ì˜¤ì°¨
    error = np.abs(u_pred - u_exact)
    axes[0, 1].plot(x_test_np, error, 'g-', linewidth=2)
    axes[0, 1].set_xlabel('x')
    axes[0, 1].set_ylabel('|u_pred - u_exact|')
    axes[0, 1].set_title('ì ˆëŒ€ ì˜¤ì°¨')
    axes[0, 1].set_yscale('log')
    axes[0, 1].grid(True, alpha=0.3)
    
    # (c) ì†ì‹¤ ë° ì˜¤ì°¨ íˆìŠ¤í† ë¦¬
    axes[1, 0].plot(history['epoch'], history['loss'], 'b-', linewidth=2, label='Loss')
    axes[1, 0].set_xlabel('Epoch')
    axes[1, 0].set_ylabel('Loss')
    axes[1, 0].set_title('í›ˆë ¨ ì†ì‹¤')
    axes[1, 0].set_yscale('log')
    axes[1, 0].grid(True, alpha=0.3)
    axes[1, 0].legend()
    
    ax2 = axes[1, 0].twinx()
    ax2.plot(history['epoch'], history['error'], 'r--', linewidth=2, label='Error')
    ax2.set_ylabel('Relative L2 Error', color='r')
    ax2.tick_params(axis='y', labelcolor='r')
    ax2.set_yscale('log')
    
    # (d) ì½œë¡œì¼€ì´ì…˜ í¬ì¸íŠ¸ ìˆ˜
    if args.use_adaptive_sampling:
        axes[1, 1].plot(history['epoch'], history['n_points'], 'g-', linewidth=2)
        axes[1, 1].set_xlabel('Epoch')
        axes[1, 1].set_ylabel('ì½œë¡œì¼€ì´ì…˜ í¬ì¸íŠ¸ ìˆ˜')
        axes[1, 1].set_title('ì ì‘í˜• ìƒ˜í”Œë§ ì§„í–‰')
        axes[1, 1].grid(True, alpha=0.3)
    else:
        axes[1, 1].text(0.5, 0.5, 'ì ì‘í˜• ìƒ˜í”Œë§ ë¯¸ì‚¬ìš©', 
                       ha='center', va='center', fontsize=12, 
                       transform=axes[1, 1].transAxes)
        axes[1, 1].axis('off')
    
    # ê°€ì¤‘ì¹˜ íˆìŠ¤í† ë¦¬ (ë™ì  ê°€ì¤‘ì¹˜ ì‚¬ìš© ì‹œ)
    if args.use_dynamic_weights and 'weight_pde' in history:
        # ìƒˆ figure ìƒì„±
        fig2, ax = plt.subplots(figsize=(10, 6))
        ax.plot(history['epoch'], history['weight_pde'], 'b-', linewidth=2, label='w_pde')
        ax.plot(history['epoch'], history['weight_bc'], 'r-', linewidth=2, label='w_bc')
        ax.set_xlabel('Epoch')
        ax.set_ylabel('ê°€ì¤‘ì¹˜')
        ax.set_title('ë™ì  ì†ì‹¤ ê°€ì¤‘ì¹˜ ë³€í™”')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        fig2.savefig(output_dir / 'weights_history.png', dpi=150, bbox_inches='tight')
        plt.close(fig2)
        print(f"  ê°€ì¤‘ì¹˜ íˆìŠ¤í† ë¦¬ ì €ì¥: {output_dir / 'weights_history.png'}")
    
    plt.tight_layout()
    fig.savefig(output_dir / 'results.png', dpi=150, bbox_inches='tight')
    plt.close(fig)
    
    print(f"  ê²°ê³¼ ì‹œê°í™” ì €ì¥: {output_dir / 'results.png'}")
    
    # ìµœì¢… ë©”íŠ¸ë¦­ ì¶œë ¥
    final_error = history['error'][-1]
    print(f"\nğŸ“ˆ ìµœì¢… ë©”íŠ¸ë¦­:")
    print(f"  ìƒëŒ€ L2 ì˜¤ì°¨: {final_error:.6e}")
    print(f"  ì½œë¡œì¼€ì´ì…˜ í¬ì¸íŠ¸: {history['n_points'][-1]}ê°œ")


def parse_args():
    """ëª…ë ¹ì¤„ ì¸ì íŒŒì‹±"""
    parser = argparse.ArgumentParser(
        description='ê³ ê¸‰ ê¸°ëŠ¥ì„ ì‚¬ìš©í•œ 1D Poisson ë°©ì •ì‹ í•´ê²°',
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    
    # ê³ ê¸‰ ê¸°ëŠ¥ í”Œë˜ê·¸
    parser.add_argument(
        '--use-dynamic-weights',
        action='store_true',
        default=True,
        help='ë™ì  ì†ì‹¤ ê°€ì¤‘ì¹˜ ì‚¬ìš© (ê¸°ë³¸: True)'
    )
    parser.add_argument(
        '--no-dynamic-weights',
        action='store_false',
        dest='use_dynamic_weights',
        help='ë™ì  ì†ì‹¤ ê°€ì¤‘ì¹˜ ë¹„í™œì„±í™”'
    )
    parser.add_argument(
        '--use-adaptive-sampling',
        action='store_true',
        default=True,
        help='ì ì‘í˜• ì½œë¡œì¼€ì´ì…˜ ìƒ˜í”Œë§ ì‚¬ìš© (ê¸°ë³¸: True)'
    )
    parser.add_argument(
        '--no-adaptive-sampling',
        action='store_false',
        dest='use_adaptive_sampling',
        help='ì ì‘í˜• ìƒ˜í”Œë§ ë¹„í™œì„±í™”'
    )
    
    # ë„ë©”ì¸ ì„¤ì •
    parser.add_argument(
        '--domain-min',
        type=float,
        default=0.0,
        help='ë„ë©”ì¸ ìµœì†Ÿê°’ (ê¸°ë³¸: 0.0)'
    )
    parser.add_argument(
        '--domain-max',
        type=float,
        default=1.0,
        help='ë„ë©”ì¸ ìµœëŒ“ê°’ (ê¸°ë³¸: 1.0)'
    )
    
    # ëª¨ë¸ ì„¤ì •
    parser.add_argument(
        '--hidden-dims',
        type=int,
        nargs='+',
        default=[32, 32],
        help='ì€ë‹‰ì¸µ ì°¨ì› (ê¸°ë³¸: [32, 32])'
    )
    parser.add_argument(
        '--cheby-order',
        type=int,
        default=4,
        help='Chebyshev ë‹¤í•­ì‹ ì°¨ìˆ˜ (ê¸°ë³¸: 4)'
    )
    
    # ìƒ˜í”Œë§ ì„¤ì •
    parser.add_argument(
        '--n-initial-points',
        type=int,
        default=50,
        help='ì´ˆê¸° ì½œë¡œì¼€ì´ì…˜ í¬ì¸íŠ¸ ìˆ˜ (ê¸°ë³¸: 50)'
    )
    parser.add_argument(
        '--n-max-points',
        type=int,
        default=200,
        help='ìµœëŒ€ ì½œë¡œì¼€ì´ì…˜ í¬ì¸íŠ¸ ìˆ˜ (ì ì‘í˜• ìƒ˜í”Œë§ ì‹œ, ê¸°ë³¸: 200)'
    )
    parser.add_argument(
        '--refinement-interval',
        type=int,
        default=500,
        help='ì ì‘í˜• ì •ì œ ì£¼ê¸° (ì—í¬í¬, ê¸°ë³¸: 500)'
    )
    
    # ì†ì‹¤ ê°€ì¤‘ì¹˜ ì„¤ì •
    parser.add_argument(
        '--pde-weight',
        type=float,
        default=1.0,
        help='PDE ì”ì°¨ ì†ì‹¤ ì´ˆê¸° ê°€ì¤‘ì¹˜ (ê¸°ë³¸: 1.0)'
    )
    parser.add_argument(
        '--bc-weight',
        type=float,
        default=10.0,
        help='ê²½ê³„ ì¡°ê±´ ì†ì‹¤ ì´ˆê¸° ê°€ì¤‘ì¹˜ (ê¸°ë³¸: 10.0)'
    )
    parser.add_argument(
        '--gradnorm-alpha',
        type=float,
        default=1.5,
        help='GradNorm alpha íŒŒë¼ë¯¸í„° (ê¸°ë³¸: 1.5)'
    )
    parser.add_argument(
        '--gradnorm-lr',
        type=float,
        default=0.025,
        help='GradNorm ê°€ì¤‘ì¹˜ í•™ìŠµë¥  (ê¸°ë³¸: 0.025)'
    )
    
    # í›ˆë ¨ ì„¤ì •
    parser.add_argument(
        '--epochs',
        type=int,
        default=5000,
        help='í›ˆë ¨ ì—í¬í¬ ìˆ˜ (ê¸°ë³¸: 5000)'
    )
    parser.add_argument(
        '--learning-rate',
        type=float,
        default=1e-3,
        help='í•™ìŠµë¥  (ê¸°ë³¸: 0.001)'
    )
    parser.add_argument(
        '--log-interval',
        type=int,
        default=100,
        help='ë¡œê·¸ ì¶œë ¥ ê°„ê²© (ê¸°ë³¸: 100)'
    )
    
    # ì¶œë ¥ ì„¤ì •
    parser.add_argument(
        '--output-dir',
        type=str,
        default='outputs/poisson_1d_advanced',
        help='ì¶œë ¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸: outputs/poisson_1d_advanced)'
    )
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    parser.add_argument(
        '--device',
        type=str,
        default='cuda' if torch.cuda.is_available() else 'cpu',
        choices=['cuda', 'cpu'],
        help='ì—°ì‚° ë””ë°”ì´ìŠ¤ (ê¸°ë³¸: cuda if available)'
    )
    
    return parser.parse_args()


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    args = parse_args()
    
    print(f"\n{'='*70}")
    print(f"ğŸš€ ê³ ê¸‰ 1D Poisson í•´ë²•")
    print(f"{'='*70}")
    print(f"ë””ë°”ì´ìŠ¤: {args.device}")
    print(f"ë™ì  ê°€ì¤‘ì¹˜: {'âœ…' if args.use_dynamic_weights else 'âŒ'}")
    print(f"ì ì‘í˜• ìƒ˜í”Œë§: {'âœ…' if args.use_adaptive_sampling else 'âŒ'}")
    
    device = torch.device(args.device)
    
    # ëª¨ë¸ ìƒì„±
    layers_dims = [1] + args.hidden_dims + [1]
    model = Scaled_cPIKAN(
        layers_dims=layers_dims,
        cheby_order=args.cheby_order,
        domain_min=torch.tensor([args.domain_min]),
        domain_max=torch.tensor([args.domain_max])
    ).to(device)
    
    print(f"\nëª¨ë¸ êµ¬ì¡°:")
    print(f"  ë ˆì´ì–´: {layers_dims}")
    print(f"  Chebyshev ì°¨ìˆ˜: {args.cheby_order}")
    print(f"  íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in model.parameters()):,}")
    
    # ê¸°ë³¸ ì†ì‹¤ í•¨ìˆ˜
    base_loss_fn = PhysicsInformedLoss(
        pde_residual_fn=create_pde_residual_fn(),
        bc_fns=[create_bc_fn()],
        loss_weights={'pde': args.pde_weight, 'bc': args.bc_weight}
    )
    
    # ë™ì  ê°€ì¤‘ì¹˜ ì ìš© ì—¬ë¶€
    if args.use_dynamic_weights:
        loss_fn = DynamicWeightedLoss(
            base_loss_fn=base_loss_fn,
            loss_names=['loss_pde', 'loss_bc'],
            alpha=args.gradnorm_alpha,
            learning_rate=args.gradnorm_lr
        )
        print(f"\nâœ… ë™ì  ê°€ì¤‘ì¹˜ í™œì„±í™”")
        print(f"  Alpha: {args.gradnorm_alpha}")
        print(f"  í•™ìŠµë¥ : {args.gradnorm_lr}")
    else:
        loss_fn = base_loss_fn
        print(f"\nâŒ ê³ ì • ê°€ì¤‘ì¹˜ ì‚¬ìš©")
        print(f"  PDE ê°€ì¤‘ì¹˜: {args.pde_weight}")
        print(f"  BC ê°€ì¤‘ì¹˜: {args.bc_weight}")
    
    # ìƒ˜í”ŒëŸ¬ ì„ íƒ
    if args.use_adaptive_sampling:
        sampler = AdaptiveResidualSampler(
            n_initial_points=args.n_initial_points,
            n_max_points=args.n_max_points,
            domain_min=[args.domain_min],
            domain_max=[args.domain_max],
            refinement_ratio=0.2,
            residual_threshold_percentile=75.0,
            device=device
        )
        print(f"\nâœ… ì ì‘í˜• ìƒ˜í”Œë§ í™œì„±í™”")
        print(f"  ì´ˆê¸° í¬ì¸íŠ¸: {args.n_initial_points}")
        print(f"  ìµœëŒ€ í¬ì¸íŠ¸: {args.n_max_points}")
        print(f"  ì •ì œ ì£¼ê¸°: {args.refinement_interval} ì—í¬í¬")
    else:
        sampler = LatinHypercubeSampler(
            n_points=args.n_initial_points,
            domain_min=[args.domain_min],
            domain_max=[args.domain_max],
            device=device
        )
        print(f"\nâŒ ê³ ì • ìƒ˜í”Œë§ ì‚¬ìš©")
        print(f"  í¬ì¸íŠ¸ ìˆ˜: {args.n_initial_points}")
    
    # í›ˆë ¨
    history = train(model, loss_fn, sampler, args, device)
    
    # ê²°ê³¼ ì‹œê°í™”
    visualize_results(model, history, args, device)
    
    print(f"\n{'='*70}")
    print(f"ğŸ‰ ì™„ë£Œ!")
    print(f"{'='*70}")
    print(f"ê²°ê³¼ ë””ë ‰í† ë¦¬: {args.output_dir}")


if __name__ == '__main__':
    main()
