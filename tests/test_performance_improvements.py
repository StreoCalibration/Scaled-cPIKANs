"""
ì„±ëŠ¥ ê°œì„  ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ë‹¤ìŒ ê¸°ëŠ¥ë“¤ì˜ ì„±ëŠ¥ì„ ì •ëŸ‰ì ìœ¼ë¡œ ë¹„êµí•©ë‹ˆë‹¤:
1. ê¸°ì¡´ ë°©ì‹: PhysicsInformedLoss + LatinHypercubeSampler
2. ë™ì  ê°€ì¤‘ì¹˜: DynamicWeightedLoss + LatinHypercubeSampler
3. ì ì‘í˜• ìƒ˜í”Œë§: PhysicsInformedLoss + AdaptiveResidualSampler
4. í†µí•© ë°©ì‹: DynamicWeightedLoss + AdaptiveResidualSampler â­

ì¸¡ì • ì§€í‘œ:
- ìˆ˜ë ´ ì†ë„ (ëª©í‘œ ì˜¤ì°¨ ë„ë‹¬ ì‹œê°„)
- ìµœì¢… ì˜¤ì°¨ (L2 relative error)
- í•„ìš”í•œ ì½œë¡œì¼€ì´ì…˜ í¬ì¸íŠ¸ ìˆ˜
- í›ˆë ¨ ì‹œê°„

í…ŒìŠ¤íŠ¸ ë¬¸ì œ: 1D Poisson ë°©ì •ì‹
    u''(x) = -Ï€Â²sin(Ï€x),  x âˆˆ [0,1]
    u(0) = 0,  u(1) = 0
    ë¶„ì„í•´: u(x) = sin(Ï€x)
"""

import unittest
import torch
import torch.nn as nn
import numpy as np
import time
import json
from pathlib import Path
import sys
import os

# src ë””ë ‰í† ë¦¬ ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from src.models import Scaled_cPIKAN
from src.loss import PhysicsInformedLoss, DynamicWeightedLoss
from src.data import LatinHypercubeSampler, AdaptiveResidualSampler


class PerformanceBenchmark(unittest.TestCase):
    """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤"""
    
    @classmethod
    def setUpClass(cls):
        """í…ŒìŠ¤íŠ¸ í™˜ê²½ ì´ˆê¸°í™”"""
        cls.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"\n{'='*70}")
        print(f"ğŸš€ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        print(f"{'='*70}")
        print(f"ë””ë°”ì´ìŠ¤: {cls.device}")
        print(f"PyTorch ë²„ì „: {torch.__version__}")
        
        # ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
        cls.output_dir = Path('outputs/benchmark_results')
        cls.output_dir.mkdir(parents=True, exist_ok=True)
        
        # 1D Poisson ë¬¸ì œ ì„¤ì •
        cls.domain_min = 0.0
        cls.domain_max = 1.0
        
        # í›ˆë ¨ ì„¤ì •
        cls.n_initial_points = 50
        cls.n_max_points = 200
        cls.adam_epochs = 1000  # ë” ì§§ê²Œ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©)
        cls.target_error = 1e-3  # ëª©í‘œ ìƒëŒ€ ì˜¤ì°¨
        
        # ëª¨ë¸ êµ¬ì¡° (ëª¨ë“  í…ŒìŠ¤íŠ¸ì—ì„œ ë™ì¼)
        cls.layers_dims = [1, 32, 32, 1]
        cls.cheby_order = 4
        
        # ê²°ê³¼ ì €ì¥ìš©
        cls.results = {}
    
    def analytical_solution(self, x):
        """ë¶„ì„í•´: u(x) = sin(Ï€x)"""
        return torch.sin(np.pi * x)
    
    def create_pde_residual_fn(self):
        """PDE ì”ì°¨ í•¨ìˆ˜ ìƒì„±"""
        def pde_residual_fn(model, points):
            """
            1D Poisson: u''(x) = -Ï€Â²sin(Ï€x)
            ì”ì°¨: u'' + Ï€Â²sin(Ï€x) = 0
            """
            points.requires_grad_(True)
            u = model(points)
            
            # 1ì°¨ ë„í•¨ìˆ˜
            u_x = torch.autograd.grad(u.sum(), points, create_graph=True)[0]
            # 2ì°¨ ë„í•¨ìˆ˜
            u_xx = torch.autograd.grad(u_x.sum(), points, create_graph=True)[0]
            
            # ì†ŒìŠ¤ í•­
            source = np.pi**2 * torch.sin(np.pi * points)
            
            # ì”ì°¨
            residual = u_xx + source
            return residual
        
        return pde_residual_fn
    
    def create_bc_fn(self):
        """ê²½ê³„ ì¡°ê±´ í•¨ìˆ˜ ìƒì„±"""
        def bc_fn(model, points):
            """u(0) = 0, u(1) = 0"""
            u = model(points)
            return u  # ê²½ê³„ì—ì„œ 0ì´ì–´ì•¼ í•¨
        
        return bc_fn
    
    def compute_l2_error(self, model, n_test=1000):
        """
        ìƒëŒ€ L2 ì˜¤ì°¨ ê³„ì‚°
        
        Returns:
            float: ìƒëŒ€ L2 ì˜¤ì°¨
        """
        model.eval()
        with torch.no_grad():
            x_test = torch.linspace(
                self.domain_min, self.domain_max, n_test, 
                device=self.device
            ).reshape(-1, 1)
            
            u_pred = model(x_test)
            u_exact = self.analytical_solution(x_test)
            
            l2_error = torch.sqrt(torch.mean((u_pred - u_exact)**2))
            l2_norm = torch.sqrt(torch.mean(u_exact**2))
            
            relative_error = (l2_error / l2_norm).item()
        
        return relative_error
    
    def train_model(self, model, loss_fn, sampler, method_name):
        """
        ëª¨ë¸ í›ˆë ¨ ë° ì„±ëŠ¥ ì¸¡ì •
        
        Args:
            model: PINN ëª¨ë¸
            loss_fn: ì†ì‹¤ í•¨ìˆ˜
            sampler: ìƒ˜í”ŒëŸ¬ (LatinHypercubeSampler ë˜ëŠ” AdaptiveResidualSampler)
            method_name: ë°©ë²• ì´ë¦„
            
        Returns:
            dict: ì„±ëŠ¥ ë©”íŠ¸ë¦­
        """
        print(f"\n{'='*70}")
        print(f"ğŸ“Š ë°©ë²•: {method_name}")
        print(f"{'='*70}")
        
        # ì˜µí‹°ë§ˆì´ì €
        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
        
        # ë©”íŠ¸ë¦­ ê¸°ë¡
        history = {
            'epoch': [],
            'loss': [],
            'error': [],
            'n_points': []
        }
        
        # ê²½ê³„ í¬ì¸íŠ¸ (ê³ ì •)
        bc_points_0 = torch.tensor([[self.domain_min]], device=self.device)
        bc_points_1 = torch.tensor([[self.domain_max]], device=self.device)
        bc_points_dicts = [
            {'points': bc_points_0},
            {'points': bc_points_1}
        ]
        
        # í›ˆë ¨ ì‹œì‘ ì‹œê°„
        start_time = time.time()
        
        # ëª©í‘œ ì˜¤ì°¨ ë„ë‹¬ ì—í¬í¬
        convergence_epoch = None
        
        # AdaptiveResidualSamplerì¸ ê²½ìš° ì •ì œ ì£¼ê¸° ì„¤ì •
        is_adaptive = isinstance(sampler, AdaptiveResidualSampler)
        refinement_interval = 500 if is_adaptive else None
        
        model.train()
        
        for epoch in range(self.adam_epochs):
            # ìƒ˜í”Œ í¬ì¸íŠ¸ ê°€ì ¸ì˜¤ê¸°
            if is_adaptive:
                pde_points = sampler.get_current_points()
            else:
                pde_points = sampler.sample()
            
            # ìˆœì „íŒŒ
            optimizer.zero_grad()
            total_loss, loss_dict = loss_fn(
                model, pde_points, bc_points_dicts
            )
            
            # ì—­ì „íŒŒ
            total_loss.backward()
            optimizer.step()
            
            # ì ì‘í˜• ìƒ˜í”Œë§: ì”ì°¨ ì—…ë°ì´íŠ¸ ë° ì •ì œ
            if is_adaptive and (epoch + 1) % refinement_interval == 0:
                # ì™„ì „íˆ ë…ë¦½ì ì¸ forward passë¡œ ì”ì°¨ ê³„ì‚°
                model.eval()
                # í˜„ì¬ í¬ì¸íŠ¸ì˜ ìƒˆë¡œìš´ ë³µì‚¬ë³¸ ìƒì„±
                with torch.enable_grad():  # gradient ê³„ì‚° í™œì„±í™”
                    pde_points_copy = sampler.get_current_points().clone().detach().requires_grad_(True)
                    pde_residual_fn = self.create_pde_residual_fn()
                    residuals = pde_residual_fn(model, pde_points_copy)
                    # ê³„ì‚° ê·¸ë˜í”„ì™€ ë¶„ë¦¬
                    sampler.update_residuals(residuals.detach())
                
                refined = sampler.refine()
                if refined:
                    n_points = sampler.get_current_points().shape[0]
                    print(f"  [Epoch {epoch+1}] ì •ì œ ì™„ë£Œ â†’ {n_points}ê°œ í¬ì¸íŠ¸")
                
                model.train()
            
            # ì˜¤ì°¨ ê³„ì‚° (ë§¤ 100 ì—í¬í¬ë§ˆë‹¤)
            if (epoch + 1) % 100 == 0:
                error = self.compute_l2_error(model)
                n_points = pde_points.shape[0]
                
                history['epoch'].append(epoch + 1)
                history['loss'].append(total_loss.item())
                history['error'].append(error)
                history['n_points'].append(n_points)
                
                print(f"  Epoch {epoch+1:5d} | Loss: {total_loss.item():.4e} | "
                      f"Error: {error:.4e} | Points: {n_points}")
                
                # ëª©í‘œ ì˜¤ì°¨ ë„ë‹¬ ì—¬ë¶€ í™•ì¸
                if convergence_epoch is None and error < self.target_error:
                    convergence_epoch = epoch + 1
                    print(f"  âœ… ëª©í‘œ ì˜¤ì°¨ ë„ë‹¬! (Epoch {convergence_epoch})")
        
        # í›ˆë ¨ ì¢…ë£Œ ì‹œê°„
        end_time = time.time()
        training_time = end_time - start_time
        
        # ìµœì¢… ì˜¤ì°¨
        final_error = self.compute_l2_error(model)
        final_n_points = pde_points.shape[0] if is_adaptive else self.n_initial_points
        
        # ê²°ê³¼ ì •ë¦¬
        result = {
            'method': method_name,
            'convergence_epoch': convergence_epoch if convergence_epoch else 'Not converged',
            'final_error': final_error,
            'final_n_points': final_n_points,
            'training_time_seconds': training_time,
            'history': history
        }
        
        print(f"\nğŸ“ˆ ê²°ê³¼ ìš”ì•½:")
        print(f"  ìˆ˜ë ´ ì—í¬í¬: {result['convergence_epoch']}")
        print(f"  ìµœì¢… ì˜¤ì°¨: {result['final_error']:.6e}")
        print(f"  ìµœì¢… í¬ì¸íŠ¸ ìˆ˜: {result['final_n_points']}")
        print(f"  í›ˆë ¨ ì‹œê°„: {training_time:.2f}ì´ˆ")
        
        return result
    
    def test_1_baseline(self):
        """ê¸°ì¡´ ë°©ì‹: PhysicsInformedLoss + LatinHypercubeSampler"""
        print("\n" + "="*70)
        print("ğŸ”µ í…ŒìŠ¤íŠ¸ 1: ê¸°ì¡´ ë°©ì‹ (Baseline)")
        print("="*70)
        
        # ëª¨ë¸ ìƒì„±
        model = Scaled_cPIKAN(
            layers_dims=self.layers_dims,
            cheby_order=self.cheby_order,
            domain_min=torch.tensor([self.domain_min]),
            domain_max=torch.tensor([self.domain_max])
        ).to(self.device)
        
        # ì†ì‹¤ í•¨ìˆ˜
        base_loss_fn = PhysicsInformedLoss(
            pde_residual_fn=self.create_pde_residual_fn(),
            bc_fns=[self.create_bc_fn()],
            loss_weights={'pde': 1.0, 'bc': 10.0}
        )
        
        # ìƒ˜í”ŒëŸ¬
        sampler = LatinHypercubeSampler(
            n_points=self.n_initial_points,
            domain_min=[self.domain_min],
            domain_max=[self.domain_max],
            device=self.device
        )
        
        # í›ˆë ¨ ë° ì¸¡ì •
        result = self.train_model(model, base_loss_fn, sampler, 'Baseline')
        self.results['baseline'] = result
    
    def test_2_dynamic_weights(self):
        """ë™ì  ê°€ì¤‘ì¹˜: DynamicWeightedLoss + LatinHypercubeSampler"""
        print("\n" + "="*70)
        print("ğŸŸ¢ í…ŒìŠ¤íŠ¸ 2: ë™ì  ê°€ì¤‘ì¹˜")
        print("="*70)
        
        # ëª¨ë¸ ìƒì„±
        model = Scaled_cPIKAN(
            layers_dims=self.layers_dims,
            cheby_order=self.cheby_order,
            domain_min=torch.tensor([self.domain_min]),
            domain_max=torch.tensor([self.domain_max])
        ).to(self.device)
        
        # ê¸°ë³¸ ì†ì‹¤ í•¨ìˆ˜
        base_loss_fn = PhysicsInformedLoss(
            pde_residual_fn=self.create_pde_residual_fn(),
            bc_fns=[self.create_bc_fn()],
            loss_weights={'pde': 1.0, 'bc': 10.0}
        )
        
        # ë™ì  ê°€ì¤‘ì¹˜ ì†ì‹¤ í•¨ìˆ˜
        dynamic_loss_fn = DynamicWeightedLoss(
            base_loss_fn=base_loss_fn,
            loss_names=['loss_pde', 'loss_bc'],
            alpha=1.5,
            learning_rate=0.025
        )
        
        # ìƒ˜í”ŒëŸ¬
        sampler = LatinHypercubeSampler(
            n_points=self.n_initial_points,
            domain_min=[self.domain_min],
            domain_max=[self.domain_max],
            device=self.device
        )
        
        # í›ˆë ¨ ë° ì¸¡ì •
        result = self.train_model(model, dynamic_loss_fn, sampler, 'Dynamic Weights')
        self.results['dynamic_weights'] = result
    
    def test_3_adaptive_sampling(self):
        """ì ì‘í˜• ìƒ˜í”Œë§: PhysicsInformedLoss + AdaptiveResidualSampler"""
        print("\n" + "="*70)
        print("ğŸŸ¡ í…ŒìŠ¤íŠ¸ 3: ì ì‘í˜• ìƒ˜í”Œë§")
        print("="*70)
        
        # ëª¨ë¸ ìƒì„±
        model = Scaled_cPIKAN(
            layers_dims=self.layers_dims,
            cheby_order=self.cheby_order,
            domain_min=torch.tensor([self.domain_min]),
            domain_max=torch.tensor([self.domain_max])
        ).to(self.device)
        
        # ì†ì‹¤ í•¨ìˆ˜
        base_loss_fn = PhysicsInformedLoss(
            pde_residual_fn=self.create_pde_residual_fn(),
            bc_fns=[self.create_bc_fn()],
            loss_weights={'pde': 1.0, 'bc': 10.0}
        )
        
        # ì ì‘í˜• ìƒ˜í”ŒëŸ¬
        sampler = AdaptiveResidualSampler(
            n_initial_points=self.n_initial_points,
            n_max_points=self.n_max_points,
            domain_min=[self.domain_min],
            domain_max=[self.domain_max],
            refinement_ratio=0.2,
            residual_threshold_percentile=75.0,
            device=self.device
        )
        
        # í›ˆë ¨ ë° ì¸¡ì •
        result = self.train_model(model, base_loss_fn, sampler, 'Adaptive Sampling')
        self.results['adaptive_sampling'] = result
    
    def test_4_combined(self):
        """í†µí•© ë°©ì‹: DynamicWeightedLoss + AdaptiveResidualSampler â­"""
        print("\n" + "="*70)
        print("ğŸ”´ í…ŒìŠ¤íŠ¸ 4: í†µí•© ë°©ì‹ (Dynamic + Adaptive) â­")
        print("="*70)
        
        # ëª¨ë¸ ìƒì„±
        model = Scaled_cPIKAN(
            layers_dims=self.layers_dims,
            cheby_order=self.cheby_order,
            domain_min=torch.tensor([self.domain_min]),
            domain_max=torch.tensor([self.domain_max])
        ).to(self.device)
        
        # ê¸°ë³¸ ì†ì‹¤ í•¨ìˆ˜
        base_loss_fn = PhysicsInformedLoss(
            pde_residual_fn=self.create_pde_residual_fn(),
            bc_fns=[self.create_bc_fn()],
            loss_weights={'pde': 1.0, 'bc': 10.0}
        )
        
        # ë™ì  ê°€ì¤‘ì¹˜ ì†ì‹¤ í•¨ìˆ˜
        dynamic_loss_fn = DynamicWeightedLoss(
            base_loss_fn=base_loss_fn,
            loss_names=['loss_pde', 'loss_bc'],
            alpha=1.5,
            learning_rate=0.025
        )
        
        # ì ì‘í˜• ìƒ˜í”ŒëŸ¬
        sampler = AdaptiveResidualSampler(
            n_initial_points=self.n_initial_points,
            n_max_points=self.n_max_points,
            domain_min=[self.domain_min],
            domain_max=[self.domain_max],
            refinement_ratio=0.2,
            residual_threshold_percentile=75.0,
            device=self.device
        )
        
        # í›ˆë ¨ ë° ì¸¡ì •
        result = self.train_model(model, dynamic_loss_fn, sampler, 'Combined (Dynamic + Adaptive)')
        self.results['combined'] = result
    
    @classmethod
    def tearDownClass(cls):
        """ê²°ê³¼ ì €ì¥ ë° ìš”ì•½"""
        print("\n" + "="*70)
        print("ğŸ“Š ìµœì¢… ì„±ëŠ¥ ë¹„êµ")
        print("="*70)
        
        # ê²°ê³¼ í…Œì´ë¸” ì¶œë ¥
        print("\n| ë°©ë²• | ìˆ˜ë ´ ì—í¬í¬ | ìµœì¢… ì˜¤ì°¨ | í¬ì¸íŠ¸ ìˆ˜ | í›ˆë ¨ ì‹œê°„(ì´ˆ) |")
        print("|------|-------------|-----------|-----------|---------------|")
        
        for key, result in cls.results.items():
            method = result['method']
            conv_epoch = result['convergence_epoch']
            if conv_epoch == 'Not converged':
                conv_epoch_str = "ë¯¸ìˆ˜ë ´"
            else:
                conv_epoch_str = f"{conv_epoch}"
            
            error = result['final_error']
            n_points = result['final_n_points']
            time_sec = result['training_time_seconds']
            
            print(f"| {method:30s} | {conv_epoch_str:11s} | {error:.6e} | "
                  f"{n_points:9d} | {time_sec:13.2f} |")
        
        # JSON ì €ì¥
        output_path = cls.output_dir / 'benchmark_results.json'
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(cls.results, f, indent=2, ensure_ascii=False)
        
        print(f"\nâœ… ê²°ê³¼ ì €ì¥: {output_path}")
        
        # ê°œì„  ë¹„ìœ¨ ê³„ì‚°
        if 'baseline' in cls.results and 'combined' in cls.results:
            baseline = cls.results['baseline']
            combined = cls.results['combined']
            
            print("\n" + "="*70)
            print("ğŸ“ˆ í†µí•© ë°©ì‹ vs ê¸°ì¡´ ë°©ì‹ ê°œì„  ë¹„ìœ¨")
            print("="*70)
            
            # ìˆ˜ë ´ ì†ë„ ê°œì„ 
            if baseline['convergence_epoch'] != 'Not converged' and \
               combined['convergence_epoch'] != 'Not converged':
                speedup = baseline['convergence_epoch'] / combined['convergence_epoch']
                print(f"ìˆ˜ë ´ ì†ë„: {speedup:.2f}ë°° í–¥ìƒ")
            
            # ì˜¤ì°¨ ê°œì„ 
            error_improvement = (baseline['final_error'] - combined['final_error']) / baseline['final_error'] * 100
            print(f"ìµœì¢… ì˜¤ì°¨: {error_improvement:.1f}% ê°œì„ ")
            
            # ì‹œê°„ ë¹„êµ
            time_ratio = combined['training_time_seconds'] / baseline['training_time_seconds']
            print(f"í›ˆë ¨ ì‹œê°„: {time_ratio:.2f}ë°°")
        
        print("\n" + "="*70)
        print("ğŸ‰ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        print("="*70)


if __name__ == '__main__':
    # unittest ì‹¤í–‰
    unittest.main(verbosity=2)
